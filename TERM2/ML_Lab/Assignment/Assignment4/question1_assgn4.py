# -*- coding: utf-8 -*-
"""question1_assgn4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TERM2/ML_Lab/Assignment/Assignment4/question1_assgn4.ipynb

Soumya Mukherjee | CH24M571 | Assignment 4 | Question 1
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split                                #For splitting the data into train and test
from sklearn.ensemble import RandomForestClassifier                                 #For Random Forest Classifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix #For model evaluation
from sklearn.metrics import classification_report                                   #For classification report

from sklearn.feature_selection import SelectFromModel

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import GridSearchCV , cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer

dataset = "./data/problem1/assignment4.csv"

# submission

# dataset = "./assignment4.csv"
# #
# #local

df = pd.read_csv(dataset)

df.isnull().sum()   # checking for any null columns

# breaking into features and targets

X= df.drop(['target'],axis=1)
y = df['target']

X.shape , y.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40) # 80:20 split

X_train.shape , X_test.shape , y_train.shape , y_test.shape

# pre-processing scaling data

sclr = StandardScaler()
X_train_scaled = sclr.fit_transform(X_train)
X_test_scaled = sclr.transform(X_test)

"""Random Forest Classifier"""

# parameter grids

param_grid_rf = {
    'n_estimators': [100, 150,200 , 300],             # Number of trees in the forest
    'max_features': ['sqrt', 'log2'],           # The number of features to consider when looking for the best split
    'max_depth': [20, 30 , 40],                  # The maximum depth of the tree (to find which one works best for the dataset) - since 50 feat , kept deep
    'min_samples_split': [2, 5, 10],            # Minimum samples required to split a node
    'criterion': ['gini', 'entropy']            # The function to measure the quality of a split (to find which one works best for the dataset)
}

param_grid_aboost = {
    'n_estimators': [50, 100, 200 ,300 , 500],             # Number of trees in the forest                   # The base estimator from which the boosted ensemble is built
    'learning_rate': [0.005,0.01, 0.05, 0.1],  # Learning rate shrinks the contribution of each classifier
}

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],             # Number of neighbors to use
    'weights': ['uniform', 'distance'],       # Weight function used in prediction
    'metric': ['euclidean', 'manhattan']      # Distance metric to use for the tree
}

# initializing classifiers

rf = RandomForestClassifier(random_state=40)
ab = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=40),random_state=40)
knn = KNeighborsClassifier()

# performing gridsearchcv for 3 models

grid_rf = GridSearchCV(estimator=rf,param_grid=param_grid_rf,cv=5,scoring='accuracy',n_jobs=-1)
grid_ab = GridSearchCV(estimator=ab,param_grid=param_grid_aboost,cv=5,scoring='accuracy',n_jobs=-1)
grid_knn = GridSearchCV(estimator=knn,param_grid=param_grid_knn,cv=5,scoring='accuracy',n_jobs=-1)

grid_rf.fit(X_train_scaled,y_train)

grid_ab.fit(X_train_scaled,y_train)

grid_knn.fit(X_train_scaled,y_train)

# Best estimators in models

best_rf = grid_rf.best_estimator_
best_rf.fit(X_train_scaled, y_train)

best_ab = grid_ab.best_estimator_
best_ab.fit(X_train_scaled, y_train)

best_knn = grid_knn.best_estimator_
best_knn.fit(X_train_scaled, y_train)

print(f"Best estimator Random Forest:{best_rf}")
print(f"Best estimator adaboost: {best_ab}")
print(f"Best estimator KNN: {best_knn}")



def evaluate_model(model , X_test_scaled , y_test):
    y_pred = model.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)

    print(f'Accuracy of {model} is : {accuracy}')

    # Classification report
    print(classification_report(y_test, y_pred))
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    return accuracy, precision, recall, f1

metrics_rf = evaluate_model(best_rf , X_test_scaled , y_test)
metrics_ab = evaluate_model(best_ab , X_test_scaled , y_test)
metrics_knn = evaluate_model(best_knn , X_test_scaled , y_test)

# performing K-Cross validation

cv_rf = np.mean(cross_val_score(best_rf, X_train, y_train, cv=5))
cv_ab = np.mean(cross_val_score(best_ab, X_train, y_train, cv=5))
cv_knn = np.mean(cross_val_score(best_knn, X_train, y_train, cv=5))
print("Cross-validation Scores:", cv_rf, cv_ab, cv_knn)

# Plotting the confusion matrices

# Plot confusion matrices
def plot_confusion_matrix(model, X_test, y_test, title):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(title)
    plt.show()

plot_confusion_matrix(best_rf, X_test, y_test, "Random Forest Confusion Matrix")
plot_confusion_matrix(best_ab, X_test, y_test, "AdaBoost Confusion Matrix")
plot_confusion_matrix(best_knn, X_test, y_test, "KNN Confusion Matrix")

"""Redundant Code"""

# model = RandomForestClassifier(random_state=40)

# param_grid = {
#     'n_estimators': [100, 150,200 , 300],             # Number of trees in the forest
#     'max_features': ['sqrt', 'log2'],           # The number of features to consider when looking for the best split
#     'max_depth': [20, 30 , 40],                  # The maximum depth of the tree (to find which one works best for the dataset) - since 50 feat , kept deep
#     'min_samples_split': [2, 5, 10],            # Minimum samples required to split a node
#     'criterion': ['gini', 'entropy']            # The function to measure the quality of a split (to find which one works best for the dataset)
# }

# # Grid search
# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
# grid_search.fit(X_train_scaled, y_train)

# # Best parameters
# print(f"Best Hyperparameters: {grid_search.best_params_}")   # Prints the best hyperparameter combination
# print(f"Best Score: {grid_search.best_score_}")              # best cross-validation accuracy score
# print(f"Best Estimator: {grid_search.best_estimator_}")      # Retrieves the best-tuned Random Forest model

# # Best estimator
# best_model_randForest = grid_search.best_estimator_

# # Model evaluation
# y_pred = best_model_randForest.predict(X_test_scaled)
# accuracy = accuracy_score(y_test, y_pred)
# print(f'Accuracy: {accuracy}')

# # Classification report
# print(classification_report(y_test, y_pred))

# # Building model

# # model2 = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=20, min_samples_split=5, random_state=42)
# best_model_randForest.fit(X_train, y_train)


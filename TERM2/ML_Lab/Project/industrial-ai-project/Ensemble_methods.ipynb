{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TERM2/ML_Lab/Project/industrial-ai-project/Ensemble_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9c94d84a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c94d84a",
        "outputId": "b5692519-c849-4dda-f1f4-5c473cd8abb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install xgboost lightgbm catboost optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "42da91ed",
      "metadata": {
        "id": "42da91ed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import KNNImputer ,SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "418ec25d",
      "metadata": {
        "id": "418ec25d"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b7f12e86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7f12e86",
        "outputId": "76801b88-404f-44b0-b6c7-5ffe391d190c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (15000, 20)\n",
            "Test shape: (10000, 19)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eea00597",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eea00597",
        "outputId": "1bbfbe10-b67b-4d5f-84e5-8c3cafa28526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target variable distribution:\n",
            "Status\n",
            "C     67.340000\n",
            "D     30.246667\n",
            "CL     2.413333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Check for the target variable distribution\n",
        "print(\"\\nTarget variable distribution:\")\n",
        "print(train_df['Status'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2c274bdc",
      "metadata": {
        "id": "2c274bdc"
      },
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis\n",
        "def analyze_missing_values(df, name):\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = missing / len(df) * 100\n",
        "    print(f\"\\nMissing values in {name} dataset:\")\n",
        "    for col, pct in zip(missing.index, missing_pct):\n",
        "        if pct > 0:\n",
        "            print(f\"{col}: {pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44eb6a7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44eb6a7b",
        "outputId": "4691eb14-12fe-4a55-f1a7-55ceaf7eaf13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values in train dataset:\n",
            "Drug: 43.67%\n",
            "Ascites: 43.61%\n",
            "Hepatomegaly: 43.67%\n",
            "Spiders: 43.69%\n",
            "Cholesterol: 55.58%\n",
            "Copper: 44.27%\n",
            "Alk_Phos: 43.70%\n",
            "SGOT: 43.71%\n",
            "Tryglicerides: 55.90%\n",
            "Platelets: 3.85%\n",
            "Prothrombin: 0.12%\n",
            "\n",
            "Missing values in test dataset:\n",
            "Drug: 42.84%\n",
            "Ascites: 42.82%\n",
            "Hepatomegaly: 42.87%\n",
            "Spiders: 42.89%\n",
            "Cholesterol: 55.47%\n",
            "Copper: 43.58%\n",
            "Alk_Phos: 42.91%\n",
            "SGOT: 42.92%\n",
            "Tryglicerides: 55.81%\n",
            "Platelets: 3.63%\n",
            "Prothrombin: 0.16%\n"
          ]
        }
      ],
      "source": [
        "analyze_missing_values(train_df, 'train')\n",
        "analyze_missing_values(test_df, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "52d1b7da",
      "metadata": {
        "id": "52d1b7da"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X_train = train_df.drop('Status', axis=1).copy()\n",
        "y_train = train_df['Status'].copy()\n",
        "X_test = test_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f44b0d8b",
      "metadata": {
        "id": "f44b0d8b"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(X_train, y_train, X_test):\n",
        "    # Encode categorical target\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y_train)\n",
        "    print(f\"\\nEncoded target classes: {label_encoder.classes_}\")\n",
        "\n",
        "    # Keep track of original indices\n",
        "    X_train['original_index'] = X_train.index\n",
        "    X_test['original_index'] = X_test.index\n",
        "\n",
        "    # Identify data types\n",
        "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "    numeric_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols.remove('original_index')  # Remove the index column we added\n",
        "\n",
        "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
        "    print(f\"Numeric columns: {numeric_cols}\")\n",
        "\n",
        "    # Strategy for handling missing values:\n",
        "    # 1. For categorical: impute with most frequent value\n",
        "    # 2. For numerical: use KNN imputation\n",
        "\n",
        "    # Create pipeline for categorical features\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Use most frequent value for categorical imputation\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Create pipeline for numerical features\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=5)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Combine transformers\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', categorical_transformer, categorical_cols),\n",
        "            ('num', numeric_transformer, numeric_cols)\n",
        "        ])\n",
        "\n",
        "    # Apply preprocessing\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Get feature names after preprocessing\n",
        "    categorical_feature_names = []\n",
        "    if categorical_cols:\n",
        "        ohe = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
        "        categorical_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
        "\n",
        "    numeric_feature_names = numeric_cols\n",
        "    all_feature_names = categorical_feature_names + numeric_feature_names\n",
        "\n",
        "    print(f\"Processed feature count: {len(all_feature_names)}\")\n",
        "\n",
        "    return X_train_processed, y_encoded, X_test_processed, label_encoder, all_feature_names, X_train['original_index'], X_test['original_index']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "845dd9f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "845dd9f7",
        "outputId": "dd62a1d8-7f26-405f-bbfa-e74e023ca93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded target classes: ['C' 'CL' 'D']\n",
            "\n",
            "Categorical columns: ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
            "Numeric columns: ['id', 'N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n",
            "\n",
            "Preprocessing data...\n",
            "Processed feature count: 27\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "X_train_processed, y_encoded, X_test_processed, label_encoder, feature_names, train_indices, test_indices = preprocess_data(X_train, y_train, X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0dd14ecd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dd14ecd",
        "outputId": "ed9190b8-860e-4848-a5f5-86deff915f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training data shape: (12000, 27)\n",
            "Validation data shape: (3000, 27)\n",
            "Test data shape: (10000, 27)\n"
          ]
        }
      ],
      "source": [
        "# Create validation set\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_processed, y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train_final.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "print(f\"Test data shape: {X_test_processed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e98ed061",
      "metadata": {
        "id": "e98ed061"
      },
      "outputs": [],
      "source": [
        "# Create and train models\n",
        "def train_xgboost(X_train, y_train, X_val, y_val):\n",
        "    print(\"\\nTraining XGBoost model...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    if num_classes == 2:\n",
        "        objective = 'binary:logistic'\n",
        "        eval_metric = 'logloss'\n",
        "    else:\n",
        "        objective = 'multi:softprob'\n",
        "        eval_metric = 'mlogloss'\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        min_child_weight=1,\n",
        "        gamma=0,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=objective,\n",
        "        eval_metric=eval_metric,\n",
        "        random_state=42,\n",
        "        early_stopping_rounds=20,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "\n",
        "    # Get validation score\n",
        "    if num_classes == 2:\n",
        "        y_pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred_val)\n",
        "        y_pred_val_tensor = np.array(y_pred_val).reshape(-1, 1)\n",
        "        y_pred_val_complement = 1 - y_pred_val_tensor\n",
        "        y_pred_val_probs = np.hstack((y_pred_val_complement, y_pred_val_tensor))\n",
        "        loss = log_loss(y_val, y_pred_val_probs)\n",
        "    else:\n",
        "        y_pred_val = model.predict_proba(X_val)\n",
        "        auc = roc_auc_score(y_val, y_pred_val, multi_class='ovr')\n",
        "        loss = log_loss(y_val, y_pred_val)\n",
        "\n",
        "    print(f\"XGBoost - Validation AUC: {auc:.4f}, Log Loss: {loss:.4f}\")\n",
        "    return model, auc, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c8c3e197",
      "metadata": {
        "id": "c8c3e197"
      },
      "outputs": [],
      "source": [
        "# LBGM\n",
        "def train_lightgbm(X_train, y_train, X_val, y_val):\n",
        "    print(\"\\nTraining LightGBM model...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    if num_classes == 2:\n",
        "        objective = 'binary'\n",
        "        metric = 'binary_logloss'\n",
        "    else:\n",
        "        objective = 'multiclass'\n",
        "        metric = 'multi_logloss'\n",
        "\n",
        "    model = lgb.LGBMClassifier(\n",
        "        boosting_type='gbdt',\n",
        "        num_leaves=31,\n",
        "        max_depth=-1,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=500,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=objective,\n",
        "        random_state=42,\n",
        "        metric=metric,\n",
        "        num_class=num_classes if num_classes > 2 else 1\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[lgb.early_stopping(20, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # Get validation score\n",
        "    if num_classes == 2:\n",
        "        y_pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred_val)\n",
        "        loss = log_loss(y_val, model.predict_proba(X_val))\n",
        "    else:\n",
        "        y_pred_val = model.predict_proba(X_val)\n",
        "        auc = roc_auc_score(y_val, y_pred_val, multi_class='ovr')\n",
        "        loss = log_loss(y_val, y_pred_val)\n",
        "\n",
        "    print(f\"LightGBM - Validation AUC: {auc:.4f}, Log Loss: {loss:.4f}\")\n",
        "    return model, auc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "387e49f8",
      "metadata": {
        "id": "387e49f8"
      },
      "outputs": [],
      "source": [
        "def train_catboost(X_train, y_train, X_val, y_val):\n",
        "    print(\"\\nTraining CatBoost model...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    if num_classes == 2:\n",
        "        loss_function = 'Logloss'\n",
        "    else:\n",
        "        loss_function = 'MultiClass'\n",
        "\n",
        "    model = cb.CatBoostClassifier(\n",
        "        iterations=500,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        l2_leaf_reg=3,\n",
        "        loss_function=loss_function,\n",
        "        eval_metric='AUC',\n",
        "        random_seed=42,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        early_stopping_rounds=20,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Get validation score\n",
        "    if num_classes == 2:\n",
        "        y_pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred_val)\n",
        "        loss = log_loss(y_val, model.predict_proba(X_val))\n",
        "    else:\n",
        "        y_pred_val = model.predict_proba(X_val)\n",
        "        auc = roc_auc_score(y_val, y_pred_val, multi_class='ovr')\n",
        "        loss = log_loss(y_val, y_pred_val)\n",
        "\n",
        "    print(f\"CatBoost - Validation AUC: {auc:.4f}, Log Loss: {loss:.4f}\")\n",
        "    return model, auc, loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using optuna for tuning\n",
        "def objective(trial):\n",
        "  # XGB\n",
        "  xgb_params = {\n",
        "      \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
        "      \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 1e-3, 1e-1, log=True),\n",
        "      \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10)\n",
        "      }\n",
        "\n",
        "  lgb_params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"lgb_learning_rate\", 1e-3, 1e-1, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"lgb_num_leaves\", 20, 50),\n",
        "        # ... other LightGBM hyperparameters ...\n",
        "    }\n",
        "\n",
        "    # CatBoost hyperparameters\n",
        "  cb_params = {\n",
        "        \"iterations\": trial.suggest_int(\"cb_iterations\", 100, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"cb_learning_rate\", 1e-3, 1e-1, log=True),\n",
        "        \"depth\": trial.suggest_int(\"cb_depth\", 4, 10),\n",
        "        # ... other CatBoost hyperparameters ...\n",
        "    }\n",
        "# Train models with the suggested hyperparameters\n",
        "  xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "  xgb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "  lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
        "  lgb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)])\n",
        "\n",
        "  cb_model = cb.CatBoostClassifier(**cb_params, verbose=False)\n",
        "  cb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "  # Ensemble predictions (weighted based on inverse log loss)\n",
        "  num_classes = len(label_encoder.classes_)\n",
        "  if num_classes == 2:\n",
        "      val_pred_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
        "      val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
        "      val_pred_cb = cb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "  else:\n",
        "      val_pred_xgb = xgb_model.predict_proba(X_val)\n",
        "      val_pred_lgb = lgb_model.predict_proba(X_val)\n",
        "      val_pred_cb = cb_model.predict_proba(X_val)\n",
        "\n",
        "  # Calculate individual model losses\n",
        "  xgb_loss = log_loss(y_val, xgb_model.predict_proba(X_val))\n",
        "  lgb_loss = log_loss(y_val, lgb_model.predict_proba(X_val))\n",
        "  cb_loss = log_loss(y_val, cb_model.predict_proba(X_val))\n",
        "\n",
        "  # Calculate ensemble weights (inverse of log loss)\n",
        "  weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "  weights = weights / weights.sum()  # Normalize to sum to 1\n",
        "\n",
        "  # Ensemble predictions (weighted)\n",
        "  val_pred_ensemble = (\n",
        "      weights[0] * val_pred_xgb +\n",
        "      weights[1] * val_pred_lgb +\n",
        "      weights[2] * val_pred_cb\n",
        "  )\n",
        "\n",
        "  # Evaluate ensemble\n",
        "  ensemble_loss = log_loss(y_val, val_pred_ensemble)\n",
        "\n",
        "  return ensemble_loss\n"
      ],
      "metadata": {
        "id": "g5pgzK0-u1n4"
      },
      "id": "g5pgzK0-u1n4",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study and optimize\n",
        "study = optuna.create_study(direction=\"minimize\")  # Minimize log loss\n",
        "study.optimize(objective, n_trials=100)  # Number of trials to run\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYJqtbaRwYRj",
        "outputId": "ce94b2d4-5d52-490b-f5d1-f792090603d1"
      },
      "id": "bYJqtbaRwYRj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:52:37,432] A new study created in memory with name: no-name-8fc370cc-0efa-4bb2-a1d2-95504e23e94b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002185 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:53:18,476] Trial 0 finished with value: 0.4355711066576625 and parameters: {'xgb_n_estimators': 603, 'xgb_learning_rate': 0.0034344319812771996, 'xgb_max_depth': 8, 'lgb_n_estimators': 888, 'lgb_learning_rate': 0.004698995645825962, 'lgb_num_leaves': 20, 'cb_iterations': 547, 'cb_learning_rate': 0.0023068615739709787, 'cb_depth': 5}. Best is trial 0 with value: 0.4355711066576625.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002176 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:54:34,126] Trial 1 finished with value: 0.4145549160762878 and parameters: {'xgb_n_estimators': 861, 'xgb_learning_rate': 0.001150974551853228, 'xgb_max_depth': 10, 'lgb_n_estimators': 206, 'lgb_learning_rate': 0.015947602942199417, 'lgb_num_leaves': 38, 'cb_iterations': 733, 'cb_learning_rate': 0.036127040618607546, 'cb_depth': 6}. Best is trial 1 with value: 0.4145549160762878.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:55:00,175] Trial 2 finished with value: 0.3850547037835639 and parameters: {'xgb_n_estimators': 234, 'xgb_learning_rate': 0.026061240818389862, 'xgb_max_depth': 6, 'lgb_n_estimators': 853, 'lgb_learning_rate': 0.004329843191490662, 'lgb_num_leaves': 44, 'cb_iterations': 809, 'cb_learning_rate': 0.014379090212599989, 'cb_depth': 4}. Best is trial 2 with value: 0.3850547037835639.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:56:50,783] Trial 3 finished with value: 0.38107856782275257 and parameters: {'xgb_n_estimators': 905, 'xgb_learning_rate': 0.028848304026138954, 'xgb_max_depth': 9, 'lgb_n_estimators': 564, 'lgb_learning_rate': 0.009999226204775013, 'lgb_num_leaves': 30, 'cb_iterations': 935, 'cb_learning_rate': 0.06449262779152708, 'cb_depth': 9}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:58:56,255] Trial 4 finished with value: 0.45317433473168506 and parameters: {'xgb_n_estimators': 887, 'xgb_learning_rate': 0.0023192137420303682, 'xgb_max_depth': 4, 'lgb_n_estimators': 842, 'lgb_learning_rate': 0.0016265378522614788, 'lgb_num_leaves': 42, 'cb_iterations': 655, 'cb_learning_rate': 0.0026133426332417857, 'cb_depth': 10}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:59:38,614] Trial 5 finished with value: 0.4435272379473396 and parameters: {'xgb_n_estimators': 371, 'xgb_learning_rate': 0.0052391609269047505, 'xgb_max_depth': 8, 'lgb_n_estimators': 934, 'lgb_learning_rate': 0.002861456034093806, 'lgb_num_leaves': 42, 'cb_iterations': 214, 'cb_learning_rate': 0.0057278464824572425, 'cb_depth': 4}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003241 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:00:04,018] Trial 6 finished with value: 0.38304159194792986 and parameters: {'xgb_n_estimators': 581, 'xgb_learning_rate': 0.05248595878177405, 'xgb_max_depth': 3, 'lgb_n_estimators': 254, 'lgb_learning_rate': 0.009926227310672919, 'lgb_num_leaves': 23, 'cb_iterations': 209, 'cb_learning_rate': 0.0434358596126868, 'cb_depth': 9}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:01:09,384] Trial 7 finished with value: 0.41707646603184273 and parameters: {'xgb_n_estimators': 767, 'xgb_learning_rate': 0.008469343866647005, 'xgb_max_depth': 10, 'lgb_n_estimators': 999, 'lgb_learning_rate': 0.01638553125842893, 'lgb_num_leaves': 37, 'cb_iterations': 359, 'cb_learning_rate': 0.0010936871531018956, 'cb_depth': 6}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:02:47,113] Trial 8 finished with value: 0.3850205802396837 and parameters: {'xgb_n_estimators': 328, 'xgb_learning_rate': 0.02988881459336587, 'xgb_max_depth': 9, 'lgb_n_estimators': 758, 'lgb_learning_rate': 0.09263427379367854, 'lgb_num_leaves': 48, 'cb_iterations': 485, 'cb_learning_rate': 0.06806422163774686, 'cb_depth': 10}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:03:36,282] Trial 9 finished with value: 0.3816303043607921 and parameters: {'xgb_n_estimators': 766, 'xgb_learning_rate': 0.007869433877824607, 'xgb_max_depth': 9, 'lgb_n_estimators': 414, 'lgb_learning_rate': 0.015718281092564207, 'lgb_num_leaves': 34, 'cb_iterations': 578, 'cb_learning_rate': 0.015202029740152544, 'cb_depth': 6}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:04:43,868] Trial 10 finished with value: 0.3903533208999072 and parameters: {'xgb_n_estimators': 997, 'xgb_learning_rate': 0.08832818136422711, 'xgb_max_depth': 6, 'lgb_n_estimators': 631, 'lgb_learning_rate': 0.05265539894443213, 'lgb_num_leaves': 29, 'cb_iterations': 989, 'cb_learning_rate': 0.09469890741065366, 'cb_depth': 8}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002044 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:05:37,302] Trial 11 finished with value: 0.3797947310772851 and parameters: {'xgb_n_estimators': 727, 'xgb_learning_rate': 0.014209551309026056, 'xgb_max_depth': 8, 'lgb_n_estimators': 415, 'lgb_learning_rate': 0.030092958804615287, 'lgb_num_leaves': 29, 'cb_iterations': 1000, 'cb_learning_rate': 0.017220910115701985, 'cb_depth': 7}. Best is trial 11 with value: 0.3797947310772851.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:06:41,589] Trial 12 finished with value: 0.37937655573390944 and parameters: {'xgb_n_estimators': 697, 'xgb_learning_rate': 0.01801741981842991, 'xgb_max_depth': 7, 'lgb_n_estimators': 464, 'lgb_learning_rate': 0.038215994963948084, 'lgb_num_leaves': 29, 'cb_iterations': 959, 'cb_learning_rate': 0.02604564547008563, 'cb_depth': 8}. Best is trial 12 with value: 0.37937655573390944.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002056 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:07:24,614] Trial 13 finished with value: 0.3780440386449938 and parameters: {'xgb_n_estimators': 688, 'xgb_learning_rate': 0.015426415296303665, 'xgb_max_depth': 7, 'lgb_n_estimators': 393, 'lgb_learning_rate': 0.03796312148453056, 'lgb_num_leaves': 27, 'cb_iterations': 860, 'cb_learning_rate': 0.023878438340859492, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002332 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:08:13,522] Trial 14 finished with value: 0.3789421721729586 and parameters: {'xgb_n_estimators': 469, 'xgb_learning_rate': 0.015576533697852458, 'xgb_max_depth': 5, 'lgb_n_estimators': 382, 'lgb_learning_rate': 0.040253921014511, 'lgb_num_leaves': 24, 'cb_iterations': 839, 'cb_learning_rate': 0.026135825571810902, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:08:43,419] Trial 15 finished with value: 0.3848001534215458 and parameters: {'xgb_n_estimators': 432, 'xgb_learning_rate': 0.013214755842671516, 'xgb_max_depth': 5, 'lgb_n_estimators': 106, 'lgb_learning_rate': 0.0751404035906124, 'lgb_num_leaves': 24, 'cb_iterations': 819, 'cb_learning_rate': 0.007625405037143, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:09:24,459] Trial 16 finished with value: 0.38035527327545493 and parameters: {'xgb_n_estimators': 105, 'xgb_learning_rate': 0.055254441633802796, 'xgb_max_depth': 5, 'lgb_n_estimators': 287, 'lgb_learning_rate': 0.029222968704265877, 'lgb_num_leaves': 24, 'cb_iterations': 816, 'cb_learning_rate': 0.02859372303033039, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002331 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:09:54,962] Trial 17 finished with value: 0.3942664284228105 and parameters: {'xgb_n_estimators': 506, 'xgb_learning_rate': 0.005983697480744163, 'xgb_max_depth': 5, 'lgb_n_estimators': 358, 'lgb_learning_rate': 0.055731862785389705, 'lgb_num_leaves': 33, 'cb_iterations': 713, 'cb_learning_rate': 0.004955038490838827, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002541 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:11:18,645] Trial 18 finished with value: 0.3817798029493973 and parameters: {'xgb_n_estimators': 483, 'xgb_learning_rate': 0.018878301782366404, 'xgb_max_depth': 3, 'lgb_n_estimators': 649, 'lgb_learning_rate': 0.02592688538657333, 'lgb_num_leaves': 26, 'cb_iterations': 873, 'cb_learning_rate': 0.01112294496497207, 'cb_depth': 9}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002088 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:11:56,858] Trial 19 finished with value: 0.4001236092382909 and parameters: {'xgb_n_estimators': 656, 'xgb_learning_rate': 0.04557096477399767, 'xgb_max_depth': 7, 'lgb_n_estimators': 523, 'lgb_learning_rate': 0.0010229760052662337, 'lgb_num_leaves': 20, 'cb_iterations': 401, 'cb_learning_rate': 0.021359234553712848, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002186 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:12:09,461] Trial 20 finished with value: 0.4135493734939326 and parameters: {'xgb_n_estimators': 257, 'xgb_learning_rate': 0.003681177606226899, 'xgb_max_depth': 4, 'lgb_n_estimators': 134, 'lgb_learning_rate': 0.046999742184124046, 'lgb_num_leaves': 27, 'cb_iterations': 652, 'cb_learning_rate': 0.04351236389829653, 'cb_depth': 5}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002054 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:13:17,932] Trial 21 finished with value: 0.37944491699384053 and parameters: {'xgb_n_estimators': 658, 'xgb_learning_rate': 0.01269592104892469, 'xgb_max_depth': 7, 'lgb_n_estimators': 474, 'lgb_learning_rate': 0.03170654748462342, 'lgb_num_leaves': 31, 'cb_iterations': 918, 'cb_learning_rate': 0.025639819302499013, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002187 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:14:14,308] Trial 22 finished with value: 0.37970032597880105 and parameters: {'xgb_n_estimators': 693, 'xgb_learning_rate': 0.020232398867499245, 'xgb_max_depth': 6, 'lgb_n_estimators': 348, 'lgb_learning_rate': 0.04085472248118742, 'lgb_num_leaves': 26, 'cb_iterations': 857, 'cb_learning_rate': 0.00912588847033772, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003616 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:15:33,838] Trial 23 finished with value: 0.3798030817633836 and parameters: {'xgb_n_estimators': 551, 'xgb_learning_rate': 0.010279999125194172, 'xgb_max_depth': 7, 'lgb_n_estimators': 465, 'lgb_learning_rate': 0.02149065668103151, 'lgb_num_leaves': 22, 'cb_iterations': 736, 'cb_learning_rate': 0.02875652931408289, 'cb_depth': 9}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002172 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:16:18,264] Trial 24 finished with value: 0.3806069896065277 and parameters: {'xgb_n_estimators': 788, 'xgb_learning_rate': 0.018382050078209602, 'xgb_max_depth': 6, 'lgb_n_estimators': 578, 'lgb_learning_rate': 0.06803025056595664, 'lgb_num_leaves': 32, 'cb_iterations': 915, 'cb_learning_rate': 0.054414568601477725, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:16:52,674] Trial 25 finished with value: 0.3811682218259039 and parameters: {'xgb_n_estimators': 431, 'xgb_learning_rate': 0.04155333271946115, 'xgb_max_depth': 7, 'lgb_n_estimators': 309, 'lgb_learning_rate': 0.09821879487855945, 'lgb_num_leaves': 27, 'cb_iterations': 780, 'cb_learning_rate': 0.020640655641883283, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002130 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:17:07,120] Trial 26 finished with value: 0.39159339181588765 and parameters: {'xgb_n_estimators': 622, 'xgb_learning_rate': 0.09019228945597384, 'xgb_max_depth': 4, 'lgb_n_estimators': 209, 'lgb_learning_rate': 0.007613012001155725, 'lgb_num_leaves': 36, 'cb_iterations': 104, 'cb_learning_rate': 0.03352886963353835, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002181 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:17:39,831] Trial 27 finished with value: 0.378785571539275 and parameters: {'xgb_n_estimators': 501, 'xgb_learning_rate': 0.02280628556091962, 'xgb_max_depth': 5, 'lgb_n_estimators': 715, 'lgb_learning_rate': 0.039562548327798985, 'lgb_num_leaves': 28, 'cb_iterations': 964, 'cb_learning_rate': 0.012493898944028586, 'cb_depth': 6}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002136 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:18:06,012] Trial 28 finished with value: 0.3786894138824488 and parameters: {'xgb_n_estimators': 504, 'xgb_learning_rate': 0.03694993228444818, 'xgb_max_depth': 5, 'lgb_n_estimators': 743, 'lgb_learning_rate': 0.018958325718875547, 'lgb_num_leaves': 22, 'cb_iterations': 874, 'cb_learning_rate': 0.011731712148583612, 'cb_depth': 5}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:18:25,443] Trial 29 finished with value: 0.3847864168106292 and parameters: {'xgb_n_estimators': 549, 'xgb_learning_rate': 0.03685882069773132, 'xgb_max_depth': 4, 'lgb_n_estimators': 742, 'lgb_learning_rate': 0.01961922880993991, 'lgb_num_leaves': 20, 'cb_iterations': 649, 'cb_learning_rate': 0.004271161120013331, 'cb_depth': 5}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002107 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:18:49,712] Trial 30 finished with value: 0.37803885826854866 and parameters: {'xgb_n_estimators': 370, 'xgb_learning_rate': 0.0627301174860321, 'xgb_max_depth': 5, 'lgb_n_estimators': 748, 'lgb_learning_rate': 0.012929391104217194, 'lgb_num_leaves': 22, 'cb_iterations': 901, 'cb_learning_rate': 0.01072039597701798, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002186 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:19:14,078] Trial 31 finished with value: 0.3791305434060899 and parameters: {'xgb_n_estimators': 354, 'xgb_learning_rate': 0.07414656946579096, 'xgb_max_depth': 5, 'lgb_n_estimators': 734, 'lgb_learning_rate': 0.005748161766473985, 'lgb_num_leaves': 22, 'cb_iterations': 894, 'cb_learning_rate': 0.012605376496932955, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002106 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:19:39,224] Trial 32 finished with value: 0.37966364000781866 and parameters: {'xgb_n_estimators': 295, 'xgb_learning_rate': 0.06343727889106505, 'xgb_max_depth': 6, 'lgb_n_estimators': 669, 'lgb_learning_rate': 0.014899831595074298, 'lgb_num_leaves': 20, 'cb_iterations': 742, 'cb_learning_rate': 0.007921179493093105, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003712 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:20:00,373] Trial 33 finished with value: 0.38710993281799705 and parameters: {'xgb_n_estimators': 170, 'xgb_learning_rate': 0.026807241429246276, 'xgb_max_depth': 5, 'lgb_n_estimators': 803, 'lgb_learning_rate': 0.013007889281442177, 'lgb_num_leaves': 26, 'cb_iterations': 955, 'cb_learning_rate': 0.006291424858341247, 'cb_depth': 4}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:20:19,928] Trial 34 finished with value: 0.37983234451214803 and parameters: {'xgb_n_estimators': 400, 'xgb_learning_rate': 0.03516990307668143, 'xgb_max_depth': 4, 'lgb_n_estimators': 701, 'lgb_learning_rate': 0.02272095888201094, 'lgb_num_leaves': 22, 'cb_iterations': 772, 'cb_learning_rate': 0.011158238109095343, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:20:54,957] Trial 35 finished with value: 0.38788325178162564 and parameters: {'xgb_n_estimators': 526, 'xgb_learning_rate': 0.022678813415178797, 'xgb_max_depth': 6, 'lgb_n_estimators': 907, 'lgb_learning_rate': 0.00799077907644142, 'lgb_num_leaves': 25, 'cb_iterations': 879, 'cb_learning_rate': 0.0028479380891427144, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002131 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:21:39,020] Trial 36 finished with value: 0.4302580757827124 and parameters: {'xgb_n_estimators': 599, 'xgb_learning_rate': 0.0011023409508839914, 'xgb_max_depth': 8, 'lgb_n_estimators': 808, 'lgb_learning_rate': 0.013020440112547845, 'lgb_num_leaves': 28, 'cb_iterations': 998, 'cb_learning_rate': 0.0038726745778761055, 'cb_depth': 4}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:22:00,466] Trial 37 finished with value: 0.42569404476096073 and parameters: {'xgb_n_estimators': 436, 'xgb_learning_rate': 0.0018374585598513876, 'xgb_max_depth': 4, 'lgb_n_estimators': 603, 'lgb_learning_rate': 0.004981334611699405, 'lgb_num_leaves': 39, 'cb_iterations': 688, 'cb_learning_rate': 0.018290757675341118, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002188 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:22:28,661] Trial 38 finished with value: 0.38335324705916796 and parameters: {'xgb_n_estimators': 226, 'xgb_learning_rate': 0.031561152125672044, 'xgb_max_depth': 3, 'lgb_n_estimators': 841, 'lgb_learning_rate': 0.011405399226394698, 'lgb_num_leaves': 22, 'cb_iterations': 947, 'cb_learning_rate': 0.013436546650729757, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:22:47,660] Trial 39 finished with value: 0.43316389410181055 and parameters: {'xgb_n_estimators': 382, 'xgb_learning_rate': 0.0487959815822201, 'xgb_max_depth': 5, 'lgb_n_estimators': 533, 'lgb_learning_rate': 0.002720924390605375, 'lgb_num_leaves': 30, 'cb_iterations': 586, 'cb_learning_rate': 0.0010467804390599195, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003381 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:23:27,683] Trial 40 finished with value: 0.38573834400816953 and parameters: {'xgb_n_estimators': 835, 'xgb_learning_rate': 0.06927184467442633, 'xgb_max_depth': 8, 'lgb_n_estimators': 784, 'lgb_learning_rate': 0.018274388078597718, 'lgb_num_leaves': 48, 'cb_iterations': 767, 'cb_learning_rate': 0.009085440514807162, 'cb_depth': 4}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001987 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:24:00,129] Trial 41 finished with value: 0.3787634320215678 and parameters: {'xgb_n_estimators': 509, 'xgb_learning_rate': 0.024231267596230205, 'xgb_max_depth': 5, 'lgb_n_estimators': 947, 'lgb_learning_rate': 0.037052039353037865, 'lgb_num_leaves': 24, 'cb_iterations': 840, 'cb_learning_rate': 0.01569274479261167, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003321 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:24:33,338] Trial 42 finished with value: 0.38094325283938985 and parameters: {'xgb_n_estimators': 576, 'xgb_learning_rate': 0.02412335875321516, 'xgb_max_depth': 6, 'lgb_n_estimators': 966, 'lgb_learning_rate': 0.06519841782590836, 'lgb_num_leaves': 24, 'cb_iterations': 863, 'cb_learning_rate': 0.013540216346074035, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002167 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:25:05,534] Trial 43 finished with value: 0.3843347513302958 and parameters: {'xgb_n_estimators': 476, 'xgb_learning_rate': 0.010863585039271283, 'xgb_max_depth': 5, 'lgb_n_estimators': 857, 'lgb_learning_rate': 0.03469870288734212, 'lgb_num_leaves': 21, 'cb_iterations': 918, 'cb_learning_rate': 0.007007873893752029, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:25:29,687] Trial 44 finished with value: 0.37903696112937063 and parameters: {'xgb_n_estimators': 629, 'xgb_learning_rate': 0.028447319630584274, 'xgb_max_depth': 4, 'lgb_n_estimators': 701, 'lgb_learning_rate': 0.022348708575605638, 'lgb_num_leaves': 27, 'cb_iterations': 827, 'cb_learning_rate': 0.02130064358676788, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:26:09,836] Trial 45 finished with value: 0.3796465398300253 and parameters: {'xgb_n_estimators': 326, 'xgb_learning_rate': 0.03972173536140765, 'xgb_max_depth': 6, 'lgb_n_estimators': 900, 'lgb_learning_rate': 0.026680612833231074, 'lgb_num_leaves': 23, 'cb_iterations': 966, 'cb_learning_rate': 0.015789711344214738, 'cb_depth': 7}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003388 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:26:36,064] Trial 46 finished with value: 0.4217005573447645 and parameters: {'xgb_n_estimators': 509, 'xgb_learning_rate': 0.007356569495706425, 'xgb_max_depth': 5, 'lgb_n_estimators': 943, 'lgb_learning_rate': 0.048102354048564264, 'lgb_num_leaves': 23, 'cb_iterations': 501, 'cb_learning_rate': 0.0015291454894599881, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:27:08,798] Trial 47 finished with value: 0.3799695317024703 and parameters: {'xgb_n_estimators': 433, 'xgb_learning_rate': 0.0564507637277974, 'xgb_max_depth': 6, 'lgb_n_estimators': 992, 'lgb_learning_rate': 0.009974827150058544, 'lgb_num_leaves': 25, 'cb_iterations': 791, 'cb_learning_rate': 0.009927923585498686, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:28:08,572] Trial 48 finished with value: 0.38274648341886763 and parameters: {'xgb_n_estimators': 943, 'xgb_learning_rate': 0.015596533721546743, 'xgb_max_depth': 10, 'lgb_n_estimators': 692, 'lgb_learning_rate': 0.03720926725449429, 'lgb_num_leaves': 30, 'cb_iterations': 897, 'cb_learning_rate': 0.03803392003456988, 'cb_depth': 5}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:28:49,923] Trial 49 finished with value: 0.3791666144750513 and parameters: {'xgb_n_estimators': 528, 'xgb_learning_rate': 0.022764439625819838, 'xgb_max_depth': 5, 'lgb_n_estimators': 866, 'lgb_learning_rate': 0.08004111028708905, 'lgb_num_leaves': 28, 'cb_iterations': 834, 'cb_learning_rate': 0.016425168574292368, 'cb_depth': 7}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:29:09,760] Trial 50 finished with value: 0.383800112644148 and parameters: {'xgb_n_estimators': 567, 'xgb_learning_rate': 0.029022313370743933, 'xgb_max_depth': 3, 'lgb_n_estimators': 620, 'lgb_learning_rate': 0.017185383041903958, 'lgb_num_leaves': 39, 'cb_iterations': 699, 'cb_learning_rate': 0.005967944734947195, 'cb_depth': 4}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:29:35,804] Trial 51 finished with value: 0.3793743830764638 and parameters: {'xgb_n_estimators': 470, 'xgb_learning_rate': 0.013566136293561246, 'xgb_max_depth': 5, 'lgb_n_estimators': 410, 'lgb_learning_rate': 0.04227431975589527, 'lgb_num_leaves': 25, 'cb_iterations': 858, 'cb_learning_rate': 0.023785103179695833, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:30:14,020] Trial 52 finished with value: 0.3820862574989001 and parameters: {'xgb_n_estimators': 403, 'xgb_learning_rate': 0.017647860479722898, 'xgb_max_depth': 4, 'lgb_n_estimators': 510, 'lgb_learning_rate': 0.05510082281342155, 'lgb_num_leaves': 46, 'cb_iterations': 937, 'cb_learning_rate': 0.011494080797084624, 'cb_depth': 7}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002124 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:30:39,659] Trial 53 finished with value: 0.3841653753943815 and parameters: {'xgb_n_estimators': 467, 'xgb_learning_rate': 0.00884027771771618, 'xgb_max_depth': 5, 'lgb_n_estimators': 363, 'lgb_learning_rate': 0.02596782574489929, 'lgb_num_leaves': 24, 'cb_iterations': 835, 'cb_learning_rate': 0.01779299240598373, 'cb_depth': 6}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002150 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:33:28,304] Trial 54 finished with value: 0.3824749184617966 and parameters: {'xgb_n_estimators': 347, 'xgb_learning_rate': 0.01194366378233605, 'xgb_max_depth': 6, 'lgb_n_estimators': 234, 'lgb_learning_rate': 0.03257775869289211, 'lgb_num_leaves': 21, 'cb_iterations': 804, 'cb_learning_rate': 0.033924581921397204, 'cb_depth': 10}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002088 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:34:06,581] Trial 55 finished with value: 0.37987133026867725 and parameters: {'xgb_n_estimators': 716, 'xgb_learning_rate': 0.015942811706453976, 'xgb_max_depth': 7, 'lgb_n_estimators': 578, 'lgb_learning_rate': 0.061653349731690066, 'lgb_num_leaves': 23, 'cb_iterations': 363, 'cb_learning_rate': 0.014828745905440402, 'cb_depth': 7}. Best is trial 30 with value: 0.37803885826854866.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002065 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "86cf0f6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86cf0f6e",
        "outputId": "65cb3f67-926e-4c8f-8711-3fbad011cbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training XGBoost model...\n",
            "XGBoost - Validation AUC: 0.9041, Log Loss: 0.3744\n",
            "\n",
            "Training LightGBM model...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n",
            "LightGBM - Validation AUC: 0.9005, Log Loss: 0.3802\n",
            "\n",
            "Training CatBoost model...\n",
            "CatBoost - Validation AUC: 0.8962, Log Loss: 0.3872\n"
          ]
        }
      ],
      "source": [
        "# Train all models\n",
        "xgb_model, xgb_auc, xgb_loss = train_xgboost(X_train_final, y_train_final, X_val, y_val)\n",
        "lgb_model, lgb_auc, lgb_loss = train_lightgbm(X_train_final, y_train_final, X_val, y_val)\n",
        "cb_model, cb_auc, cb_loss = train_catboost(X_train_final, y_train_final, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fe6ea73c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe6ea73c",
        "outputId": "fcb34972-4b41-4462-afb2-129ac2762e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble weights: XGBoost=0.339, LightGBM=0.334, CatBoost=0.328\n"
          ]
        }
      ],
      "source": [
        "# Create ensemble weights based on validation performance\n",
        "# We use inverse of log loss as weight (lower loss = higher weight)\n",
        "weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "weights = weights / weights.sum()  # Normalize to sum to 1\n",
        "print(f\"\\nEnsemble weights: XGBoost={weights[0]:.3f}, LightGBM={weights[1]:.3f}, CatBoost={weights[2]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best params\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Extract hyperparameters for each model\n",
        "best_xgb_params = {k: v for k, v in best_params.items() if k.startswith(\"xgb_\")}\n",
        "best_lgb_params = {k: v for k, v in best_params.items() if k.startswith(\"lgb_\")}\n",
        "best_cb_params = {k: v for k, v in best_params.items() if k.startswith(\"cb_\")}\n",
        "\n",
        "# Retrain models with best hyperparameters\n",
        "xgb_model = xgb.XGBClassifier(**best_xgb_params)\n",
        "xgb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)], verbose=False)  # Train on full training data\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(**best_lgb_params)\n",
        "lgb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)])  # Train on full training data\n",
        "\n",
        "cb_model = cb.CatBoostClassifier(**best_cb_params, verbose=False)\n",
        "cb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)], verbose=False)  # Train on full training data\n",
        "\n",
        "# Calculate individual model losses (for weights)\n",
        "xgb_loss = log_loss(y_val, xgb_model.predict_proba(X_val))\n",
        "lgb_loss = log_loss(y_val, lgb_model.predict_proba(X_val))\n",
        "cb_loss = log_loss(y_val, cb_model.predict_proba(X_val))\n",
        "\n",
        "# Calculate ensemble weights\n",
        "weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "weights = weights / weights.sum()  # Normalize"
      ],
      "metadata": {
        "id": "lcwDocao6aWC"
      },
      "id": "lcwDocao6aWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test set\n",
        "num_classes = len(label_encoder.classes_)\n",
        "if num_classes == 2:\n",
        "    test_pred_xgb = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_pred_lgb = lgb_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_pred_cb = cb_model.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "    test_pred_ensemble = (\n",
        "        weights[0] * test_pred_xgb +\n",
        "        weights[1] * test_pred_lgb +\n",
        "        weights[2] * test_pred_cb\n",
        "    )\n",
        "\n",
        "    # Convert to class probabilities\n",
        "    test_pred_probs = np.column_stack((1 - test_pred_ensemble, test_pred_ensemble))\n",
        "else:\n",
        "    test_pred_xgb = xgb_model.predict_proba(X_test_processed)\n",
        "    test_pred_lgb = lgb_model.predict_proba(X_test_processed)\n",
        "    test_pred_cb = cb_model.predict_proba(X_test_processed)\n",
        "\n",
        "    test_pred_ensemble = (\n",
        "        weights[0] * test_pred_xgb +\n",
        "        weights[1] * test_pred_lgb +\n",
        "        weights[2] * test_pred_cb\n",
        "    )\n",
        "\n",
        "    test_pred_probs = test_pred_ensemble"
      ],
      "metadata": {
        "id": "jxae2YqJ6wMa"
      },
      "id": "jxae2YqJ6wMa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the submission DataFrame matching the required format\n",
        "results_df = pd.DataFrame({\n",
        "    'id': test_indices  # Use the original indices from the test data\n",
        "})\n",
        "\n",
        "# Add probability columns for each class with the exact column names from the submission format\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    results_df[f'Status_{class_name}'] = test_pred_probs[:, i]\n",
        "\n",
        "# Sort by original index to maintain original order\n",
        "results_df = results_df.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "# Save predictions to CSV\n",
        "results_df.to_csv('ensemble_predictions_v3.csv', index=False)\n",
        "print(\"\\nPredictions saved to 'ensemble_predictions_v3.csv'\")"
      ],
      "metadata": {
        "id": "6fhdz8tl6-cp"
      },
      "id": "6fhdz8tl6-cp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7fdacf3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fdacf3e",
        "outputId": "43afb6fa-1f6a-4813-c3ea-876c4e71abf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble - Validation AUC: 0.9049, Log Loss: 0.3757\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "if num_classes == 2:\n",
        "    val_pred_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
        "    val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
        "    val_pred_cb = cb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    val_pred_ensemble = (\n",
        "        weights[0] * val_pred_xgb +\n",
        "        weights[1] * val_pred_lgb +\n",
        "        weights[2] * val_pred_cb\n",
        "    )\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_auc = roc_auc_score(y_val, val_pred_ensemble)\n",
        "    # For log loss we need probabilities for both classes\n",
        "    ensemble_probs = np.column_stack((1 - val_pred_ensemble, val_pred_ensemble))\n",
        "    ensemble_loss = log_loss(y_val, ensemble_probs)\n",
        "else:\n",
        "    val_pred_xgb = xgb_model.predict_proba(X_val)\n",
        "    val_pred_lgb = lgb_model.predict_proba(X_val)\n",
        "    val_pred_cb = cb_model.predict_proba(X_val)\n",
        "\n",
        "    val_pred_ensemble = (\n",
        "        weights[0] * val_pred_xgb +\n",
        "        weights[1] * val_pred_lgb +\n",
        "        weights[2] * val_pred_cb\n",
        "    )\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_auc = roc_auc_score(y_val, val_pred_ensemble, multi_class='ovr')\n",
        "    ensemble_loss = log_loss(y_val, val_pred_ensemble)\n",
        "\n",
        "print(f\"\\nEnsemble - Validation AUC: {ensemble_auc:.4f}, Log Loss: {ensemble_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "993bf213",
      "metadata": {
        "id": "993bf213"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "if num_classes == 2:\n",
        "    test_pred_xgb = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_pred_lgb = lgb_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_pred_cb = cb_model.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "    test_pred_ensemble = (\n",
        "        weights[0] * test_pred_xgb +\n",
        "        weights[1] * test_pred_lgb +\n",
        "        weights[2] * test_pred_cb\n",
        "    )\n",
        "\n",
        "    # Convert to class probabilities\n",
        "    test_pred_probs = np.column_stack((1 - test_pred_ensemble, test_pred_ensemble))\n",
        "else:\n",
        "    test_pred_xgb = xgb_model.predict_proba(X_test_processed)\n",
        "    test_pred_lgb = lgb_model.predict_proba(X_test_processed)\n",
        "    test_pred_cb = cb_model.predict_proba(X_test_processed)\n",
        "\n",
        "    test_pred_ensemble = (\n",
        "        weights[0] * test_pred_xgb +\n",
        "        weights[1] * test_pred_lgb +\n",
        "        weights[2] * test_pred_cb\n",
        "    )\n",
        "\n",
        "    test_pred_probs = test_pred_ensemble\n",
        "\n",
        "# Create the submission DataFrame matching the required format\n",
        "results_df = pd.DataFrame({\n",
        "    'id': test_indices\n",
        "})\n",
        "\n",
        "# Add probability columns for each class with the exact column names from the submission format\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    results_df[f'Status_{class_name}'] = test_pred_probs[:, i]\n",
        "\n",
        "# Sort by original index to maintain original order\n",
        "results_df = results_df.sort_values('id').reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({\n",
        "    'id': range(15000, 25000) # IDs from 15000 to 24999\n",
        "})\n",
        "\n",
        "# Add probability columns for each class with the exact column names from the submission format\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    results_df[f'Status_{class_name}'] = test_pred_probs[:, i]"
      ],
      "metadata": {
        "id": "QBO515JusWqM"
      },
      "id": "QBO515JusWqM",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e9bf5318",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9bf5318",
        "outputId": "6123ac7b-0087-4f64-fe27-39bc1eead3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few predictions:\n",
            "      id  Status_C  Status_CL  Status_D\n",
            "0  15000  0.834856   0.013206  0.151938\n",
            "1  15001  0.672774   0.011871  0.315355\n",
            "2  15002  0.945995   0.027247  0.026758\n",
            "3  15003  0.352355   0.579074  0.068571\n",
            "4  15004  0.204372   0.007669  0.787960\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFirst few predictions:\")\n",
        "print(results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "43c39a75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43c39a75",
        "outputId": "a3e4879a-5c15-4914-9bc2-414da9590404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions saved to 'ensemble_predictions.csv' with columns: ['id', 'Status_C', 'Status_CL', 'Status_D']\n"
          ]
        }
      ],
      "source": [
        "# Save predictions to CSV\n",
        "results_df.to_csv('ensemble_predictions.csv', index=False)\n",
        "print(\"\\nPredictions saved to 'ensemble_predictions.csv' with columns:\", results_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2354fad",
      "metadata": {
        "id": "b2354fad"
      },
      "outputs": [],
      "source": [
        "# Create feature importance visualization for the base models\n",
        "def print_feature_importance(model, model_name, feature_names):\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        print(f\"\\nTop 10 features for {model_name}:\")\n",
        "        for i in range(min(10, len(feature_names))):\n",
        "            idx = indices[i]\n",
        "            if idx < len(feature_names):\n",
        "                print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print_feature_importance(xgb_model, \"XGBoost\", feature_names)\n",
        "print_feature_importance(lgb_model, \"LightGBM\", feature_names)\n",
        "print_feature_importance(cb_model, \"CatBoost\", feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19887403",
      "metadata": {
        "id": "19887403"
      },
      "outputs": [],
      "source": [
        "# Cross-validation for more robust evaluation\n",
        "def cross_validate_ensemble(X, y, n_splits=5):\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    cv_aucs = []\n",
        "    cv_losses = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"Fold {fold+1}/{n_splits}\")\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Train models\n",
        "        xgb_model, xgb_auc, xgb_loss = train_xgboost(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "        lgb_model, lgb_auc, lgb_loss = train_lightgbm(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "        cb_model, cb_auc, cb_loss = train_catboost(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "\n",
        "        # Weight models\n",
        "        weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "        weights = weights / weights.sum()\n",
        "\n",
        "        # Make ensemble prediction\n",
        "        num_classes = len(np.unique(y))\n",
        "        if num_classes == 2:\n",
        "            val_pred_xgb = xgb_model.predict_proba(X_val_fold)[:, 1]\n",
        "            val_pred_lgb = lgb_model.predict_proba(X_val_fold)[:, 1]\n",
        "            val_pred_cb = cb_model.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "            val_pred_ensemble = (\n",
        "                weights[0] * val_pred_xgb +\n",
        "                weights[1] * val_pred_lgb +\n",
        "                weights[2] * val_pred_cb\n",
        "            )\n",
        "\n",
        "            # Evaluate ensemble\n",
        "            fold_auc = roc_auc_score(y_val_fold, val_pred_ensemble)\n",
        "            # For log loss we need probabilities for both classes\n",
        "            ensemble_probs = np.column_stack((1 - val_pred_ensemble, val_pred_ensemble))\n",
        "            fold_loss = log_loss(y_val_fold, ensemble_probs)\n",
        "        else:\n",
        "            val_pred_xgb = xgb_model.predict_proba(X_val_fold)\n",
        "            val_pred_lgb = lgb_model.predict_proba(X_val_fold)\n",
        "            val_pred_cb = cb_model.predict_proba(X_val_fold)\n",
        "\n",
        "            val_pred_ensemble = (\n",
        "                weights[0] * val_pred_xgb +\n",
        "                weights[1] * val_pred_lgb +\n",
        "                weights[2] * val_pred_cb\n",
        "            )\n",
        "\n",
        "            # Evaluate ensemble\n",
        "            fold_auc = roc_auc_score(y_val_fold, val_pred_ensemble, multi_class='ovr')\n",
        "            fold_loss = log_loss(y_val_fold, val_pred_ensemble)\n",
        "\n",
        "        cv_aucs.append(fold_auc)\n",
        "        cv_losses.append(fold_loss)\n",
        "        print(f\"Fold {fold+1} - AUC: {fold_auc:.4f}, Log Loss: {fold_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\nCross-validation results:\")\n",
        "    print(f\"Mean AUC: {np.mean(cv_aucs):.4f}  {np.std(cv_aucs):.4f}\")\n",
        "    print(f\"Mean Log Loss: {np.mean(cv_losses):.4f}  {np.std(cv_losses):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df241ba8",
      "metadata": {
        "id": "df241ba8"
      },
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "cross_validate_ensemble(X_train_processed, y_encoded)\n",
        "\n",
        "print(\"\\nModel training and evaluation complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
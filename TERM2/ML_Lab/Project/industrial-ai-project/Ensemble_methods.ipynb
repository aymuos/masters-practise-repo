{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TERM2/ML_Lab/Project/industrial-ai-project/Ensemble_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9c94d84a",
      "metadata": {
        "id": "9c94d84a",
        "outputId": "b5692519-c849-4dda-f1f4-5c473cd8abb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install xgboost lightgbm catboost optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "42da91ed",
      "metadata": {
        "id": "42da91ed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import KNNImputer ,SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "418ec25d",
      "metadata": {
        "id": "418ec25d"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b7f12e86",
      "metadata": {
        "id": "b7f12e86",
        "outputId": "76801b88-404f-44b0-b6c7-5ffe391d190c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (15000, 20)\n",
            "Test shape: (10000, 19)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "eea00597",
      "metadata": {
        "id": "eea00597",
        "outputId": "1bbfbe10-b67b-4d5f-84e5-8c3cafa28526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target variable distribution:\n",
            "Status\n",
            "C     67.340000\n",
            "D     30.246667\n",
            "CL     2.413333\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Check for the target variable distribution\n",
        "print(\"\\nTarget variable distribution:\")\n",
        "print(train_df['Status'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2c274bdc",
      "metadata": {
        "id": "2c274bdc"
      },
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis\n",
        "def analyze_missing_values(df, name):\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = missing / len(df) * 100\n",
        "    print(f\"\\nMissing values in {name} dataset:\")\n",
        "    for col, pct in zip(missing.index, missing_pct):\n",
        "        if pct > 0:\n",
        "            print(f\"{col}: {pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44eb6a7b",
      "metadata": {
        "id": "44eb6a7b",
        "outputId": "4691eb14-12fe-4a55-f1a7-55ceaf7eaf13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values in train dataset:\n",
            "Drug: 43.67%\n",
            "Ascites: 43.61%\n",
            "Hepatomegaly: 43.67%\n",
            "Spiders: 43.69%\n",
            "Cholesterol: 55.58%\n",
            "Copper: 44.27%\n",
            "Alk_Phos: 43.70%\n",
            "SGOT: 43.71%\n",
            "Tryglicerides: 55.90%\n",
            "Platelets: 3.85%\n",
            "Prothrombin: 0.12%\n",
            "\n",
            "Missing values in test dataset:\n",
            "Drug: 42.84%\n",
            "Ascites: 42.82%\n",
            "Hepatomegaly: 42.87%\n",
            "Spiders: 42.89%\n",
            "Cholesterol: 55.47%\n",
            "Copper: 43.58%\n",
            "Alk_Phos: 42.91%\n",
            "SGOT: 42.92%\n",
            "Tryglicerides: 55.81%\n",
            "Platelets: 3.63%\n",
            "Prothrombin: 0.16%\n"
          ]
        }
      ],
      "source": [
        "analyze_missing_values(train_df, 'train')\n",
        "analyze_missing_values(test_df, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "52d1b7da",
      "metadata": {
        "id": "52d1b7da"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X_train = train_df.drop('Status', axis=1).copy()\n",
        "y_train = train_df['Status'].copy()\n",
        "X_test = test_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f44b0d8b",
      "metadata": {
        "id": "f44b0d8b"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(X_train, y_train, X_test):\n",
        "    # Encode categorical target\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y_train)\n",
        "    print(f\"\\nEncoded target classes: {label_encoder.classes_}\")\n",
        "\n",
        "    # Keep track of original indices\n",
        "    X_train['original_index'] = X_train.index\n",
        "    X_test['original_index'] = X_test.index\n",
        "\n",
        "    # Identify data types\n",
        "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "    numeric_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
        "    numeric_cols.remove('original_index')  # Remove the index column we added\n",
        "\n",
        "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
        "    print(f\"Numeric columns: {numeric_cols}\")\n",
        "\n",
        "    # Strategy for handling missing values:\n",
        "    # 1. For categorical: impute with most frequent value\n",
        "    # 2. For numerical: use KNN imputation\n",
        "\n",
        "    # Create pipeline for categorical features\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Use most frequent value for categorical imputation\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Create pipeline for numerical features\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=5)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Combine transformers\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', categorical_transformer, categorical_cols),\n",
        "            ('num', numeric_transformer, numeric_cols)\n",
        "        ])\n",
        "\n",
        "    # Apply preprocessing\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Get feature names after preprocessing\n",
        "    categorical_feature_names = []\n",
        "    if categorical_cols:\n",
        "        ohe = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
        "        categorical_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
        "\n",
        "    numeric_feature_names = numeric_cols\n",
        "    all_feature_names = categorical_feature_names + numeric_feature_names\n",
        "\n",
        "    print(f\"Processed feature count: {len(all_feature_names)}\")\n",
        "\n",
        "    return X_train_processed, y_encoded, X_test_processed, label_encoder, all_feature_names, X_train['original_index'], X_test['original_index']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "845dd9f7",
      "metadata": {
        "id": "845dd9f7",
        "outputId": "dd62a1d8-7f26-405f-bbfa-e74e023ca93f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded target classes: ['C' 'CL' 'D']\n",
            "\n",
            "Categorical columns: ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
            "Numeric columns: ['id', 'N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n",
            "\n",
            "Preprocessing data...\n",
            "Processed feature count: 27\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "X_train_processed, y_encoded, X_test_processed, label_encoder, feature_names, train_indices, test_indices = preprocess_data(X_train, y_train, X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0dd14ecd",
      "metadata": {
        "id": "0dd14ecd",
        "outputId": "ed9190b8-860e-4848-a5f5-86deff915f84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training data shape: (12000, 27)\n",
            "Validation data shape: (3000, 27)\n",
            "Test data shape: (10000, 27)\n"
          ]
        }
      ],
      "source": [
        "# Create validation set\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_processed, y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train_final.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "print(f\"Test data shape: {X_test_processed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e98ed061",
      "metadata": {
        "id": "e98ed061"
      },
      "outputs": [],
      "source": [
        "# Create and train models\n",
        "def train_xgboost(X_train, y_train, X_val, y_val):\n",
        "    print(\"\\nTraining XGBoost model...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    if num_classes == 2:\n",
        "        objective = 'binary:logistic'\n",
        "        eval_metric = 'logloss'\n",
        "    else:\n",
        "        objective = 'multi:softprob'\n",
        "        eval_metric = 'mlogloss'\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        min_child_weight=1,\n",
        "        gamma=0,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=objective,\n",
        "        eval_metric=eval_metric,\n",
        "        random_state=42,\n",
        "        early_stopping_rounds=20,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "\n",
        "    # Get validation score\n",
        "    if num_classes == 2:\n",
        "        y_pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred_val)\n",
        "        y_pred_val_tensor = np.array(y_pred_val).reshape(-1, 1)\n",
        "        y_pred_val_complement = 1 - y_pred_val_tensor\n",
        "        y_pred_val_probs = np.hstack((y_pred_val_complement, y_pred_val_tensor))\n",
        "        loss = log_loss(y_val, y_pred_val_probs)\n",
        "    else:\n",
        "        y_pred_val = model.predict_proba(X_val)\n",
        "        auc = roc_auc_score(y_val, y_pred_val, multi_class='ovr')\n",
        "        loss = log_loss(y_val, y_pred_val)\n",
        "\n",
        "    print(f\"XGBoost - Validation AUC: {auc:.4f}, Log Loss: {loss:.4f}\")\n",
        "    return model, auc, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c8c3e197",
      "metadata": {
        "id": "c8c3e197"
      },
      "outputs": [],
      "source": [
        "# LBGM\n",
        "def train_lightgbm(X_train, y_train, X_val, y_val):\n",
        "    print(\"\\nTraining LightGBM model...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    if num_classes == 2:\n",
        "        objective = 'binary'\n",
        "        metric = 'binary_logloss'\n",
        "    else:\n",
        "        objective = 'multiclass'\n",
        "        metric = 'multi_logloss'\n",
        "\n",
        "    model = lgb.LGBMClassifier(\n",
        "        boosting_type='gbdt',\n",
        "        num_leaves=31,\n",
        "        max_depth=-1,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=500,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=objective,\n",
        "        random_state=42,\n",
        "        metric=metric,\n",
        "        num_class=num_classes if num_classes > 2 else 1\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[lgb.early_stopping(20, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # Get validation score\n",
        "    if num_classes == 2:\n",
        "        y_pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred_val)\n",
        "        loss = log_loss(y_val, model.predict_proba(X_val))\n",
        "    else:\n",
        "        y_pred_val = model.predict_proba(X_val)\n",
        "        auc = roc_auc_score(y_val, y_pred_val, multi_class='ovr')\n",
        "        loss = log_loss(y_val, y_pred_val)\n",
        "\n",
        "    print(f\"LightGBM - Validation AUC: {auc:.4f}, Log Loss: {loss:.4f}\")\n",
        "    return model, auc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "387e49f8",
      "metadata": {
        "id": "387e49f8"
      },
      "outputs": [],
      "source": [
        "def train_catboost(X_train, y_train, X_val, y_val):\n",
        "    print(\"\\nTraining CatBoost model...\")\n",
        "    num_classes = len(np.unique(y_train))\n",
        "\n",
        "    if num_classes == 2:\n",
        "        loss_function = 'Logloss'\n",
        "    else:\n",
        "        loss_function = 'MultiClass'\n",
        "\n",
        "    model = cb.CatBoostClassifier(\n",
        "        iterations=500,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        l2_leaf_reg=3,\n",
        "        loss_function=loss_function,\n",
        "        eval_metric='AUC',\n",
        "        random_seed=42,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        early_stopping_rounds=20,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Get validation score\n",
        "    if num_classes == 2:\n",
        "        y_pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred_val)\n",
        "        loss = log_loss(y_val, model.predict_proba(X_val))\n",
        "    else:\n",
        "        y_pred_val = model.predict_proba(X_val)\n",
        "        auc = roc_auc_score(y_val, y_pred_val, multi_class='ovr')\n",
        "        loss = log_loss(y_val, y_pred_val)\n",
        "\n",
        "    print(f\"CatBoost - Validation AUC: {auc:.4f}, Log Loss: {loss:.4f}\")\n",
        "    return model, auc, loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using optuna for tuning\n",
        "def objective(trial):\n",
        "  # XGB\n",
        "  xgb_params = {\n",
        "      \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
        "      \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 1e-3, 1e-1, log=True),\n",
        "      \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10)\n",
        "      }\n",
        "\n",
        "  lgb_params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"lgb_learning_rate\", 1e-3, 1e-1, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"lgb_num_leaves\", 20, 50),\n",
        "        # ... other LightGBM hyperparameters ...\n",
        "    }\n",
        "\n",
        "    # CatBoost hyperparameters\n",
        "  cb_params = {\n",
        "        \"iterations\": trial.suggest_int(\"cb_iterations\", 100, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"cb_learning_rate\", 1e-3, 1e-1, log=True),\n",
        "        \"depth\": trial.suggest_int(\"cb_depth\", 4, 10),\n",
        "        # ... other CatBoost hyperparameters ...\n",
        "    }\n",
        "# Train models with the suggested hyperparameters\n",
        "  xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "  xgb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "  lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
        "  lgb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)])\n",
        "\n",
        "  cb_model = cb.CatBoostClassifier(**cb_params, verbose=False)\n",
        "  cb_model.fit(X_train_final, y_train_final, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "  # Ensemble predictions (weighted based on inverse log loss)\n",
        "  num_classes = len(label_encoder.classes_)\n",
        "  if num_classes == 2:\n",
        "      val_pred_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
        "      val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
        "      val_pred_cb = cb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "  else:\n",
        "      val_pred_xgb = xgb_model.predict_proba(X_val)\n",
        "      val_pred_lgb = lgb_model.predict_proba(X_val)\n",
        "      val_pred_cb = cb_model.predict_proba(X_val)\n",
        "\n",
        "  # Calculate individual model losses\n",
        "  xgb_loss = log_loss(y_val, xgb_model.predict_proba(X_val))\n",
        "  lgb_loss = log_loss(y_val, lgb_model.predict_proba(X_val))\n",
        "  cb_loss = log_loss(y_val, cb_model.predict_proba(X_val))\n",
        "\n",
        "  # Calculate ensemble weights (inverse of log loss)\n",
        "  weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "  weights = weights / weights.sum()  # Normalize to sum to 1\n",
        "\n",
        "  # Ensemble predictions (weighted)\n",
        "  val_pred_ensemble = (\n",
        "      weights[0] * val_pred_xgb +\n",
        "      weights[1] * val_pred_lgb +\n",
        "      weights[2] * val_pred_cb\n",
        "  )\n",
        "\n",
        "  # Evaluate ensemble\n",
        "  ensemble_loss = log_loss(y_val, val_pred_ensemble)\n",
        "\n",
        "  return ensemble_loss\n"
      ],
      "metadata": {
        "id": "g5pgzK0-u1n4"
      },
      "id": "g5pgzK0-u1n4",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study and optimize\n",
        "study = optuna.create_study(direction=\"minimize\")  # Minimize log loss\n",
        "study.optimize(objective, n_trials=100)  # Number of trials to run\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = study.best_params"
      ],
      "metadata": {
        "id": "bYJqtbaRwYRj",
        "outputId": "ce94b2d4-5d52-490b-f5d1-f792090603d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bYJqtbaRwYRj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:52:37,432] A new study created in memory with name: no-name-8fc370cc-0efa-4bb2-a1d2-95504e23e94b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002185 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:53:18,476] Trial 0 finished with value: 0.4355711066576625 and parameters: {'xgb_n_estimators': 603, 'xgb_learning_rate': 0.0034344319812771996, 'xgb_max_depth': 8, 'lgb_n_estimators': 888, 'lgb_learning_rate': 0.004698995645825962, 'lgb_num_leaves': 20, 'cb_iterations': 547, 'cb_learning_rate': 0.0023068615739709787, 'cb_depth': 5}. Best is trial 0 with value: 0.4355711066576625.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002176 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:54:34,126] Trial 1 finished with value: 0.4145549160762878 and parameters: {'xgb_n_estimators': 861, 'xgb_learning_rate': 0.001150974551853228, 'xgb_max_depth': 10, 'lgb_n_estimators': 206, 'lgb_learning_rate': 0.015947602942199417, 'lgb_num_leaves': 38, 'cb_iterations': 733, 'cb_learning_rate': 0.036127040618607546, 'cb_depth': 6}. Best is trial 1 with value: 0.4145549160762878.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:55:00,175] Trial 2 finished with value: 0.3850547037835639 and parameters: {'xgb_n_estimators': 234, 'xgb_learning_rate': 0.026061240818389862, 'xgb_max_depth': 6, 'lgb_n_estimators': 853, 'lgb_learning_rate': 0.004329843191490662, 'lgb_num_leaves': 44, 'cb_iterations': 809, 'cb_learning_rate': 0.014379090212599989, 'cb_depth': 4}. Best is trial 2 with value: 0.3850547037835639.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:56:50,783] Trial 3 finished with value: 0.38107856782275257 and parameters: {'xgb_n_estimators': 905, 'xgb_learning_rate': 0.028848304026138954, 'xgb_max_depth': 9, 'lgb_n_estimators': 564, 'lgb_learning_rate': 0.009999226204775013, 'lgb_num_leaves': 30, 'cb_iterations': 935, 'cb_learning_rate': 0.06449262779152708, 'cb_depth': 9}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:58:56,255] Trial 4 finished with value: 0.45317433473168506 and parameters: {'xgb_n_estimators': 887, 'xgb_learning_rate': 0.0023192137420303682, 'xgb_max_depth': 4, 'lgb_n_estimators': 842, 'lgb_learning_rate': 0.0016265378522614788, 'lgb_num_leaves': 42, 'cb_iterations': 655, 'cb_learning_rate': 0.0026133426332417857, 'cb_depth': 10}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 13:59:38,614] Trial 5 finished with value: 0.4435272379473396 and parameters: {'xgb_n_estimators': 371, 'xgb_learning_rate': 0.0052391609269047505, 'xgb_max_depth': 8, 'lgb_n_estimators': 934, 'lgb_learning_rate': 0.002861456034093806, 'lgb_num_leaves': 42, 'cb_iterations': 214, 'cb_learning_rate': 0.0057278464824572425, 'cb_depth': 4}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003241 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:00:04,018] Trial 6 finished with value: 0.38304159194792986 and parameters: {'xgb_n_estimators': 581, 'xgb_learning_rate': 0.05248595878177405, 'xgb_max_depth': 3, 'lgb_n_estimators': 254, 'lgb_learning_rate': 0.009926227310672919, 'lgb_num_leaves': 23, 'cb_iterations': 209, 'cb_learning_rate': 0.0434358596126868, 'cb_depth': 9}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:01:09,384] Trial 7 finished with value: 0.41707646603184273 and parameters: {'xgb_n_estimators': 767, 'xgb_learning_rate': 0.008469343866647005, 'xgb_max_depth': 10, 'lgb_n_estimators': 999, 'lgb_learning_rate': 0.01638553125842893, 'lgb_num_leaves': 37, 'cb_iterations': 359, 'cb_learning_rate': 0.0010936871531018956, 'cb_depth': 6}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:02:47,113] Trial 8 finished with value: 0.3850205802396837 and parameters: {'xgb_n_estimators': 328, 'xgb_learning_rate': 0.02988881459336587, 'xgb_max_depth': 9, 'lgb_n_estimators': 758, 'lgb_learning_rate': 0.09263427379367854, 'lgb_num_leaves': 48, 'cb_iterations': 485, 'cb_learning_rate': 0.06806422163774686, 'cb_depth': 10}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:03:36,282] Trial 9 finished with value: 0.3816303043607921 and parameters: {'xgb_n_estimators': 766, 'xgb_learning_rate': 0.007869433877824607, 'xgb_max_depth': 9, 'lgb_n_estimators': 414, 'lgb_learning_rate': 0.015718281092564207, 'lgb_num_leaves': 34, 'cb_iterations': 578, 'cb_learning_rate': 0.015202029740152544, 'cb_depth': 6}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:04:43,868] Trial 10 finished with value: 0.3903533208999072 and parameters: {'xgb_n_estimators': 997, 'xgb_learning_rate': 0.08832818136422711, 'xgb_max_depth': 6, 'lgb_n_estimators': 631, 'lgb_learning_rate': 0.05265539894443213, 'lgb_num_leaves': 29, 'cb_iterations': 989, 'cb_learning_rate': 0.09469890741065366, 'cb_depth': 8}. Best is trial 3 with value: 0.38107856782275257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002044 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:05:37,302] Trial 11 finished with value: 0.3797947310772851 and parameters: {'xgb_n_estimators': 727, 'xgb_learning_rate': 0.014209551309026056, 'xgb_max_depth': 8, 'lgb_n_estimators': 415, 'lgb_learning_rate': 0.030092958804615287, 'lgb_num_leaves': 29, 'cb_iterations': 1000, 'cb_learning_rate': 0.017220910115701985, 'cb_depth': 7}. Best is trial 11 with value: 0.3797947310772851.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:06:41,589] Trial 12 finished with value: 0.37937655573390944 and parameters: {'xgb_n_estimators': 697, 'xgb_learning_rate': 0.01801741981842991, 'xgb_max_depth': 7, 'lgb_n_estimators': 464, 'lgb_learning_rate': 0.038215994963948084, 'lgb_num_leaves': 29, 'cb_iterations': 959, 'cb_learning_rate': 0.02604564547008563, 'cb_depth': 8}. Best is trial 12 with value: 0.37937655573390944.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002056 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:07:24,614] Trial 13 finished with value: 0.3780440386449938 and parameters: {'xgb_n_estimators': 688, 'xgb_learning_rate': 0.015426415296303665, 'xgb_max_depth': 7, 'lgb_n_estimators': 393, 'lgb_learning_rate': 0.03796312148453056, 'lgb_num_leaves': 27, 'cb_iterations': 860, 'cb_learning_rate': 0.023878438340859492, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002332 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:08:13,522] Trial 14 finished with value: 0.3789421721729586 and parameters: {'xgb_n_estimators': 469, 'xgb_learning_rate': 0.015576533697852458, 'xgb_max_depth': 5, 'lgb_n_estimators': 382, 'lgb_learning_rate': 0.040253921014511, 'lgb_num_leaves': 24, 'cb_iterations': 839, 'cb_learning_rate': 0.026135825571810902, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:08:43,419] Trial 15 finished with value: 0.3848001534215458 and parameters: {'xgb_n_estimators': 432, 'xgb_learning_rate': 0.013214755842671516, 'xgb_max_depth': 5, 'lgb_n_estimators': 106, 'lgb_learning_rate': 0.0751404035906124, 'lgb_num_leaves': 24, 'cb_iterations': 819, 'cb_learning_rate': 0.007625405037143, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:09:24,459] Trial 16 finished with value: 0.38035527327545493 and parameters: {'xgb_n_estimators': 105, 'xgb_learning_rate': 0.055254441633802796, 'xgb_max_depth': 5, 'lgb_n_estimators': 287, 'lgb_learning_rate': 0.029222968704265877, 'lgb_num_leaves': 24, 'cb_iterations': 816, 'cb_learning_rate': 0.02859372303033039, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002331 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:09:54,962] Trial 17 finished with value: 0.3942664284228105 and parameters: {'xgb_n_estimators': 506, 'xgb_learning_rate': 0.005983697480744163, 'xgb_max_depth': 5, 'lgb_n_estimators': 358, 'lgb_learning_rate': 0.055731862785389705, 'lgb_num_leaves': 33, 'cb_iterations': 713, 'cb_learning_rate': 0.004955038490838827, 'cb_depth': 7}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002541 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:11:18,645] Trial 18 finished with value: 0.3817798029493973 and parameters: {'xgb_n_estimators': 483, 'xgb_learning_rate': 0.018878301782366404, 'xgb_max_depth': 3, 'lgb_n_estimators': 649, 'lgb_learning_rate': 0.02592688538657333, 'lgb_num_leaves': 26, 'cb_iterations': 873, 'cb_learning_rate': 0.01112294496497207, 'cb_depth': 9}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002088 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:11:56,858] Trial 19 finished with value: 0.4001236092382909 and parameters: {'xgb_n_estimators': 656, 'xgb_learning_rate': 0.04557096477399767, 'xgb_max_depth': 7, 'lgb_n_estimators': 523, 'lgb_learning_rate': 0.0010229760052662337, 'lgb_num_leaves': 20, 'cb_iterations': 401, 'cb_learning_rate': 0.021359234553712848, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002186 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:12:09,461] Trial 20 finished with value: 0.4135493734939326 and parameters: {'xgb_n_estimators': 257, 'xgb_learning_rate': 0.003681177606226899, 'xgb_max_depth': 4, 'lgb_n_estimators': 134, 'lgb_learning_rate': 0.046999742184124046, 'lgb_num_leaves': 27, 'cb_iterations': 652, 'cb_learning_rate': 0.04351236389829653, 'cb_depth': 5}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002054 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-22 14:13:17,932] Trial 21 finished with value: 0.37944491699384053 and parameters: {'xgb_n_estimators': 658, 'xgb_learning_rate': 0.01269592104892469, 'xgb_max_depth': 7, 'lgb_n_estimators': 474, 'lgb_learning_rate': 0.03170654748462342, 'lgb_num_leaves': 31, 'cb_iterations': 918, 'cb_learning_rate': 0.025639819302499013, 'cb_depth': 8}. Best is trial 13 with value: 0.3780440386449938.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "86cf0f6e",
      "metadata": {
        "id": "86cf0f6e",
        "outputId": "65cb3f67-926e-4c8f-8711-3fbad011cbfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training XGBoost model...\n",
            "XGBoost - Validation AUC: 0.9041, Log Loss: 0.3744\n",
            "\n",
            "Training LightGBM model...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2598\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score -0.395391\n",
            "[LightGBM] [Info] Start training from score -3.722781\n",
            "[LightGBM] [Info] Start training from score -1.195950\n",
            "LightGBM - Validation AUC: 0.9005, Log Loss: 0.3802\n",
            "\n",
            "Training CatBoost model...\n",
            "CatBoost - Validation AUC: 0.8962, Log Loss: 0.3872\n"
          ]
        }
      ],
      "source": [
        "# Train all models\n",
        "xgb_model, xgb_auc, xgb_loss = train_xgboost(X_train_final, y_train_final, X_val, y_val)\n",
        "lgb_model, lgb_auc, lgb_loss = train_lightgbm(X_train_final, y_train_final, X_val, y_val)\n",
        "cb_model, cb_auc, cb_loss = train_catboost(X_train_final, y_train_final, X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fe6ea73c",
      "metadata": {
        "id": "fe6ea73c",
        "outputId": "fcb34972-4b41-4462-afb2-129ac2762e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble weights: XGBoost=0.339, LightGBM=0.334, CatBoost=0.328\n"
          ]
        }
      ],
      "source": [
        "# Create ensemble weights based on validation performance\n",
        "# We use inverse of log loss as weight (lower loss = higher weight)\n",
        "weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "weights = weights / weights.sum()  # Normalize to sum to 1\n",
        "print(f\"\\nEnsemble weights: XGBoost={weights[0]:.3f}, LightGBM={weights[1]:.3f}, CatBoost={weights[2]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7fdacf3e",
      "metadata": {
        "id": "7fdacf3e",
        "outputId": "43afb6fa-1f6a-4813-c3ea-876c4e71abf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble - Validation AUC: 0.9049, Log Loss: 0.3757\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "if num_classes == 2:\n",
        "    val_pred_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
        "    val_pred_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
        "    val_pred_cb = cb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    val_pred_ensemble = (\n",
        "        weights[0] * val_pred_xgb +\n",
        "        weights[1] * val_pred_lgb +\n",
        "        weights[2] * val_pred_cb\n",
        "    )\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_auc = roc_auc_score(y_val, val_pred_ensemble)\n",
        "    # For log loss we need probabilities for both classes\n",
        "    ensemble_probs = np.column_stack((1 - val_pred_ensemble, val_pred_ensemble))\n",
        "    ensemble_loss = log_loss(y_val, ensemble_probs)\n",
        "else:\n",
        "    val_pred_xgb = xgb_model.predict_proba(X_val)\n",
        "    val_pred_lgb = lgb_model.predict_proba(X_val)\n",
        "    val_pred_cb = cb_model.predict_proba(X_val)\n",
        "\n",
        "    val_pred_ensemble = (\n",
        "        weights[0] * val_pred_xgb +\n",
        "        weights[1] * val_pred_lgb +\n",
        "        weights[2] * val_pred_cb\n",
        "    )\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_auc = roc_auc_score(y_val, val_pred_ensemble, multi_class='ovr')\n",
        "    ensemble_loss = log_loss(y_val, val_pred_ensemble)\n",
        "\n",
        "print(f\"\\nEnsemble - Validation AUC: {ensemble_auc:.4f}, Log Loss: {ensemble_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "993bf213",
      "metadata": {
        "id": "993bf213"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "if num_classes == 2:\n",
        "    test_pred_xgb = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_pred_lgb = lgb_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_pred_cb = cb_model.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "    test_pred_ensemble = (\n",
        "        weights[0] * test_pred_xgb +\n",
        "        weights[1] * test_pred_lgb +\n",
        "        weights[2] * test_pred_cb\n",
        "    )\n",
        "\n",
        "    # Convert to class probabilities\n",
        "    test_pred_probs = np.column_stack((1 - test_pred_ensemble, test_pred_ensemble))\n",
        "else:\n",
        "    test_pred_xgb = xgb_model.predict_proba(X_test_processed)\n",
        "    test_pred_lgb = lgb_model.predict_proba(X_test_processed)\n",
        "    test_pred_cb = cb_model.predict_proba(X_test_processed)\n",
        "\n",
        "    test_pred_ensemble = (\n",
        "        weights[0] * test_pred_xgb +\n",
        "        weights[1] * test_pred_lgb +\n",
        "        weights[2] * test_pred_cb\n",
        "    )\n",
        "\n",
        "    test_pred_probs = test_pred_ensemble\n",
        "\n",
        "# Create the submission DataFrame matching the required format\n",
        "results_df = pd.DataFrame({\n",
        "    'id': test_indices\n",
        "})\n",
        "\n",
        "# Add probability columns for each class with the exact column names from the submission format\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    results_df[f'Status_{class_name}'] = test_pred_probs[:, i]\n",
        "\n",
        "# Sort by original index to maintain original order\n",
        "results_df = results_df.sort_values('id').reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({\n",
        "    'id': range(15000, 25000) # IDs from 15000 to 24999\n",
        "})\n",
        "\n",
        "# Add probability columns for each class with the exact column names from the submission format\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    results_df[f'Status_{class_name}'] = test_pred_probs[:, i]"
      ],
      "metadata": {
        "id": "QBO515JusWqM"
      },
      "id": "QBO515JusWqM",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e9bf5318",
      "metadata": {
        "id": "e9bf5318",
        "outputId": "6123ac7b-0087-4f64-fe27-39bc1eead3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few predictions:\n",
            "      id  Status_C  Status_CL  Status_D\n",
            "0  15000  0.834856   0.013206  0.151938\n",
            "1  15001  0.672774   0.011871  0.315355\n",
            "2  15002  0.945995   0.027247  0.026758\n",
            "3  15003  0.352355   0.579074  0.068571\n",
            "4  15004  0.204372   0.007669  0.787960\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFirst few predictions:\")\n",
        "print(results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "43c39a75",
      "metadata": {
        "id": "43c39a75",
        "outputId": "a3e4879a-5c15-4914-9bc2-414da9590404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions saved to 'ensemble_predictions.csv' with columns: ['id', 'Status_C', 'Status_CL', 'Status_D']\n"
          ]
        }
      ],
      "source": [
        "# Save predictions to CSV\n",
        "results_df.to_csv('ensemble_predictions.csv', index=False)\n",
        "print(\"\\nPredictions saved to 'ensemble_predictions.csv' with columns:\", results_df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2354fad",
      "metadata": {
        "id": "b2354fad"
      },
      "outputs": [],
      "source": [
        "# Create feature importance visualization for the base models\n",
        "def print_feature_importance(model, model_name, feature_names):\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        print(f\"\\nTop 10 features for {model_name}:\")\n",
        "        for i in range(min(10, len(feature_names))):\n",
        "            idx = indices[i]\n",
        "            if idx < len(feature_names):\n",
        "                print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print_feature_importance(xgb_model, \"XGBoost\", feature_names)\n",
        "print_feature_importance(lgb_model, \"LightGBM\", feature_names)\n",
        "print_feature_importance(cb_model, \"CatBoost\", feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19887403",
      "metadata": {
        "id": "19887403"
      },
      "outputs": [],
      "source": [
        "# Cross-validation for more robust evaluation\n",
        "def cross_validate_ensemble(X, y, n_splits=5):\n",
        "    print(\"\\nPerforming cross-validation...\")\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    cv_aucs = []\n",
        "    cv_losses = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"Fold {fold+1}/{n_splits}\")\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Train models\n",
        "        xgb_model, xgb_auc, xgb_loss = train_xgboost(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "        lgb_model, lgb_auc, lgb_loss = train_lightgbm(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "        cb_model, cb_auc, cb_loss = train_catboost(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "\n",
        "        # Weight models\n",
        "        weights = np.array([1/xgb_loss, 1/lgb_loss, 1/cb_loss])\n",
        "        weights = weights / weights.sum()\n",
        "\n",
        "        # Make ensemble prediction\n",
        "        num_classes = len(np.unique(y))\n",
        "        if num_classes == 2:\n",
        "            val_pred_xgb = xgb_model.predict_proba(X_val_fold)[:, 1]\n",
        "            val_pred_lgb = lgb_model.predict_proba(X_val_fold)[:, 1]\n",
        "            val_pred_cb = cb_model.predict_proba(X_val_fold)[:, 1]\n",
        "\n",
        "            val_pred_ensemble = (\n",
        "                weights[0] * val_pred_xgb +\n",
        "                weights[1] * val_pred_lgb +\n",
        "                weights[2] * val_pred_cb\n",
        "            )\n",
        "\n",
        "            # Evaluate ensemble\n",
        "            fold_auc = roc_auc_score(y_val_fold, val_pred_ensemble)\n",
        "            # For log loss we need probabilities for both classes\n",
        "            ensemble_probs = np.column_stack((1 - val_pred_ensemble, val_pred_ensemble))\n",
        "            fold_loss = log_loss(y_val_fold, ensemble_probs)\n",
        "        else:\n",
        "            val_pred_xgb = xgb_model.predict_proba(X_val_fold)\n",
        "            val_pred_lgb = lgb_model.predict_proba(X_val_fold)\n",
        "            val_pred_cb = cb_model.predict_proba(X_val_fold)\n",
        "\n",
        "            val_pred_ensemble = (\n",
        "                weights[0] * val_pred_xgb +\n",
        "                weights[1] * val_pred_lgb +\n",
        "                weights[2] * val_pred_cb\n",
        "            )\n",
        "\n",
        "            # Evaluate ensemble\n",
        "            fold_auc = roc_auc_score(y_val_fold, val_pred_ensemble, multi_class='ovr')\n",
        "            fold_loss = log_loss(y_val_fold, val_pred_ensemble)\n",
        "\n",
        "        cv_aucs.append(fold_auc)\n",
        "        cv_losses.append(fold_loss)\n",
        "        print(f\"Fold {fold+1} - AUC: {fold_auc:.4f}, Log Loss: {fold_loss:.4f}\")\n",
        "\n",
        "    print(f\"\\nCross-validation results:\")\n",
        "    print(f\"Mean AUC: {np.mean(cv_aucs):.4f}  {np.std(cv_aucs):.4f}\")\n",
        "    print(f\"Mean Log Loss: {np.mean(cv_losses):.4f}  {np.std(cv_losses):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df241ba8",
      "metadata": {
        "id": "df241ba8"
      },
      "outputs": [],
      "source": [
        "# Perform cross-validation\n",
        "cross_validate_ensemble(X_train_processed, y_encoded)\n",
        "\n",
        "print(\"\\nModel training and evaluation complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Instructions to students:\n",
    "\n",
    "1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.\n",
    "    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)\n",
    "    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)\n",
    "    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)\n",
    "    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)\n",
    "    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)\n",
    "    \n",
    "2. You are not allowed to insert new cells in the submitted notebook.\n",
    "\n",
    "3. You are not allowed to import any extra packages.\n",
    "\n",
    "4. The code is to be written in Python 3.6 syntax. Latest versions of other packages maybe assumed.\n",
    "\n",
    "5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. \n",
    "\n",
    "6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.\n",
    "\n",
    "7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with \"run all\" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.\n",
    "\n",
    "8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks. \n",
    "\n",
    "9. You may discuss broad ideas with friends, but all code must be written by yourself.\n",
    "\n",
    "9. All datasets will be given as .npz files, and will contain data in 4 numpy arrays :\"X_train, Y_train, X_test, Y_test\". In that order. The meaning of the 4 arrays can be easily inferred from their names. (The Lasso regression problem requires you to split the data yourself)\n",
    "    1. Problem 1 uses datasets 1_1, 1_2 and 1_3. (Classification)\n",
    "    2. Problem 2 uses datasets 2_1 and 2_2. (Classification)\n",
    "    3. Problem 3 uses datasets 3_1. (Regression)\n",
    "    4. Problem 4 uses a small dataset described within the problem itself. (Regression)\n",
    "    5. Problems 5,6,7,8 uses classification datasets A,B,C,D  (Classification)\n",
    "    6. Problem 9 uses dataset LassoReg_data (Regression)\n",
    "\n",
    "10. All plots must be labelled properly, all tables must have rows and columns named properly.\n",
    "\n",
    "11. Plotting the data and prediction is highly encouraged for debugging. But remove debugging/understanding code before submitting.\n",
    "\n",
    "12. Before subbmission ensure that you submit with the outputs (do not clear the outputs), so that when evaluating we can run selectively.\n",
    "\n",
    "13. Before submission ensure that the path for the folder containing the data is \"../../Data/\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : CodeRead\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 1: Learning Binary Bayes Classifiers from data with Max. Likelihood \n",
    "\n",
    "Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. \n",
    "\n",
    "1a) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, I)$ and  $X|Y=1 \\sim \\mathcal{N}(\\mu_+, I)$. *(Same known covariance)*\n",
    "\n",
    "1b) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, \\Sigma)$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_+, \\Sigma)$ *(Same unknown covariance)*\n",
    "\n",
    "1c) Assume $X|Y=-1 \\sim \\mathcal{N}(\\mu_-, \\Sigma_-)$ and $X|Y=1 \\sim \\mathcal{N}(\\mu_+, \\Sigma_+)$ *(different unknown covariance)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "\n",
    "def Bayes1a(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1a.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    # same known co-variance\n",
    "    # getting the mean for positive and negative classes \n",
    "\n",
    "    u_positive = np.mean(X_train[Y_train == 1], axis=0)\n",
    "    u_negative = np.mean(X_train[Y_train == -1], axis=0)\n",
    "\n",
    "    # Mean of features where Y_train is 1 and -1 respectively \n",
    "\n",
    "    # calculating the prior probabilities \n",
    "    PY_1positive = np.sum(Y_train == 1) / len(Y_train)  # Proportion of data points with Y_train = 1\n",
    "    PY_1negative = 1 - PY_1positive # Proportion of data points with Y_train = -1\n",
    "\n",
    "    # storing the predictions \n",
    "\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])  # Initialize array to store predictions\n",
    "    for i, x_test in enumerate(X_test):\n",
    "\n",
    "        # Calculates the likelihood of the test data point belonging to the positive class, assuming a normal distribution with mean u_positive and identity covariance.\n",
    "        likelihood_positive = np.exp(-0.5 * np.sum((x_test - u_positive)**2))  \n",
    "        likelihood_negative = np.exp(-0.5 * np.sum((x_test - u_negative)**2))\n",
    "\n",
    "        # Calculate posterior probabilities (avoiding division by zero):\n",
    "        # P(Y=1|X) = P(X|Y=1) * P(Y=1)\n",
    "        posterior_pos = likelihood_positive * PY_1positive \n",
    "        posterior_neg = likelihood_negative * PY_1negative   # P(Y=-1|X) = P(X|Y=-1) * P(Y=-1)\n",
    "\n",
    "        # Assigning the predicted label:\n",
    "        Y_test_pred[i] = 1 if posterior_pos >= posterior_neg else -1 \n",
    "\n",
    "    return Y_test_pred\n",
    "    \n",
    "def Bayes1b(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1b.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # same unknown covariance \n",
    "\n",
    "    # Calculating means for each class\n",
    "    u_positive = np.mean(X_train[Y_train == 1], axis=0)\n",
    "    u_negative = np.mean(X_train[Y_train == -1], axis=0)\n",
    "\n",
    "    # Calculating the shared covariance matrix\n",
    "    # a shared covariance matrix (cov) is calculated from the entire training data.\n",
    "    cov = np.cov(X_train, rowvar=False)\n",
    "    \n",
    "    # Handle singular covariance matrix (add small regularization)\n",
    "    if np.linalg.cond(cov) > 1e10:  # Check for ill-conditioned matrix\n",
    "        cov += np.eye(cov.shape[0]) * 1e-6  \n",
    "\n",
    "    # Calculate prior probabilities\n",
    "    PY_1positive = np.sum(Y_train == 1) / len(Y_train)\n",
    "    PY_1negative = 1 - PY_1positive\n",
    "\n",
    "    # Make predictions\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    inv_cov = np.linalg.inv(cov)  \n",
    "    for i, x_test in enumerate(X_test):\n",
    "        # Calculate likelihoods (using shared covariance)\n",
    "        likelihood_pos = np.exp(-0.5 * (x_test - u_positive).T @ inv_cov @ (x_test - u_positive)) \n",
    "        likelihood_neg = np.exp(-0.5 * (x_test - u_negative).T @ inv_cov @ (x_test - u_negative))\n",
    "\n",
    "        # Calculate posterior probabilities (avoiding division by zero)\n",
    "        posterior_pos = likelihood_pos * PY_1positive\n",
    "        posterior_neg = likelihood_neg * PY_1negative\n",
    "\n",
    "        # Assign predicted label\n",
    "        Y_test_pred[i] = 1 if posterior_pos >= posterior_neg else -1\n",
    "\n",
    "    return Y_test_pred\n",
    "\n",
    "def Bayes1c(X_train, Y_train, X_test):\n",
    "    \"\"\" Give prediction for test instance using assumption 1c.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: +1/-1 numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    # different unknown covariance \n",
    "\n",
    "    # Calculating means and covariances for each class\n",
    "    u_positive = np.mean(X_train[Y_train == 1], axis=0)\n",
    "    u_negative = np.mean(X_train[Y_train == -1], axis=0)\n",
    "    cov_positive = np.cov(X_train[Y_train == 1], rowvar=False)\n",
    "    cov_negative = np.cov(X_train[Y_train == -1], rowvar=False)\n",
    "    \n",
    "    # # Handle singular covariance matrices (add small regularization)\n",
    "    # if np.linalg.cond(cov_pos) > 1e10:\n",
    "    #     cov_pos += np.eye(cov_pos.shape[0]) * 1e-6\n",
    "    # if np.linalg.cond(cov_neg) > 1e10:\n",
    "    #     cov_neg += np.eye(cov_neg.shape[0]) * 1e-6\n",
    "\n",
    "    # Calculate prior probabilities\n",
    "    PY_pos = np.sum(Y_train == 1) / len(Y_train)\n",
    "    PY_neg = 1 - PY_pos\n",
    "\n",
    "    # Make predictions\n",
    "    Y_test_pred = np.zeros(X_test.shape[0])\n",
    "    inv_cov_pos = np.linalg.inv(cov_positive)\n",
    "    inv_cov_neg = np.linalg.inv(cov_negative)\n",
    "    for i, x_test in enumerate(X_test):\n",
    "        # Calculate likelihoods (using separate covariances)\n",
    "        likelihood_pos = np.exp(-0.5 * (x_test - u_positive).T @ inv_cov_pos @ (x_test - u_positive)) / np.sqrt(np.linalg.det(cov_positive))\n",
    "        likelihood_neg = np.exp(-0.5 * (x_test - u_negative).T @ inv_cov_neg @ (x_test - u_negative)) / np.sqrt(np.linalg.det(cov_negative))\n",
    "\n",
    "        # Calculate posterior probabilities (avoiding division by zero)\n",
    "        posterior_pos = likelihood_pos * PY_pos\n",
    "        posterior_neg = likelihood_neg * PY_neg\n",
    "        \n",
    "        # Assign predicted label\n",
    "        Y_test_pred[i] = 1 if posterior_pos >= posterior_neg else -1\n",
    "\n",
    "    return Y_test_pred\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : Convenience\n",
    "\n",
    "# Testing the functions above\n",
    "\n",
    "# To TAs: Replace this cell with the testing cell developed.\n",
    "\n",
    "# To students: You may use the example here for testing syntax issues \n",
    "# with your functions, and also as a sanity check. But the final evaluation\n",
    "# will be done for different inputs to the functions. (So you can't just \n",
    "# solve the problem for this one example given below.) \n",
    "\n",
    "\n",
    "X_train_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "X_train_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)\n",
    "Y_train = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "X_test_pos = np.random.randn(1000,2)+np.array([[1.,2.]])\n",
    "X_test_neg = np.random.randn(1000,2)+np.array([[2.,4.]])\n",
    "X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)\n",
    "Y_test = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))\n",
    "\n",
    "Y_pred_test_1a = Bayes1a(X_train, Y_train, X_test)\n",
    "Y_pred_test_1b = Bayes1b(X_train, Y_train, X_test)\n",
    "Y_pred_test_1c = Bayes1c(X_train, Y_train, X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1., ...,  1., -1., -1.], shape=(2000,)),\n",
       " array([ 1.,  1.,  1., ...,  1., -1., -1.], shape=(2000,)),\n",
       " array([ 1.,  1.,  1., ...,  1., -1., -1.], shape=(2000,)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be removed \n",
    "Y_pred_test_1a , Y_pred_test_1b , Y_pred_test_1c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "1d) Run the above three algorithms (Bayes1a,1b and 1c), for the three datasets given (dataset1_1.npz, dataset1_2.npz, dataset1_3.npz) in the cell below.\n",
    "\n",
    "In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 3 datasets = 9 plots) on a 2d plot (color the positively classified area light green, and negatively classified area light red, for reference see Bishop Fig 4.5). Add the training data points also on the plot. Plots to be organised into 3 plots follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 9 plots appropriately. \n",
    "\n",
    "In the next Textwrite cell, summarise (use the plots of the data and the assumptions in the problem to explain) your observations regarding the six learnt classifiers, and also give the error rate of the three classifiers on the three datasets (use X_test and Y_test) as 3x3 table, with appropriately named rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# write the code for loading the data, running the three algos, and plotting here. \n",
    "# (Use the functions written previously.)\n",
    "\n",
    "files = ['../../Data/dataset1_1.npz', '../../Data/dataset1_2.npz', '../../Data/dataset1_3.npz']\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite ** \n",
    "(Write your observations and table of errors here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** Cell type : TextRead ** \n",
    "\n",
    "\n",
    "# Problem 2 : Learning Multiclass Bayes Classifiers from data with Max. Likeli.\n",
    "\n",
    "Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. The $4\\times 4$ loss matrix giving the loss incurred for predicting $i$ when truth is $j$ is below.\n",
    "\n",
    "$L=\\begin{bmatrix} 0 &1 & 2& 3\\\\ 1 &0 & 1& 2\\\\ 2 &1 & 0& 1\\\\ 3 &2 & 1& 0 \\end{bmatrix}$ \n",
    "\n",
    "2a) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $I$.\n",
    "\n",
    "2b) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $\\Sigma$.\n",
    "\n",
    "2c) Assume $X|Y=a$ is distributed as Normal with mean $\\mu_a$ and variance $\\Sigma_a$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# Fill in functions in this cell\n",
    "\n",
    "\n",
    "def Bayes2a(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2a.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "def Bayes2b(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2b.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def Bayes2c(X_train, Y_train, X_test):\n",
    "    \"\"\" Give Bayes classifier prediction for test instances \n",
    "    using assumption 2c.\n",
    "\n",
    "    Arguments:\n",
    "    X_train: numpy array of shape (n,d)\n",
    "    Y_train: {1,2,3,4} numpy array of shape (n,)\n",
    "    X_test : numpy array of shape (m,d)\n",
    "\n",
    "    Returns:\n",
    "    Y_test_pred : {1,2,3,4} numpy array of shape (m,)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : Convenience\n",
    "\n",
    "# Testing the functions above\n",
    "\n",
    "# Data 1\n",
    "\n",
    "mat1=np.array([[1.,0.],[0.,1.]])\n",
    "mat2=np.array([[1.,0.],[0.,1.]])\n",
    "mat3=np.array([[1.,0.],[0.,1.]])\n",
    "mat4=np.array([[1.,0.],[0.,1.]])\n",
    "\n",
    "X_train_1 = np.dot(np.random.randn(1000,2), mat1)+np.array([[0.,0.]])\n",
    "X_train_2 = np.dot(np.random.randn(1000,2), mat2)+np.array([[0.,2.]])\n",
    "X_train_3 = np.dot(np.random.randn(1000,2), mat3)+np.array([[2.,0.]])\n",
    "X_train_4 = np.dot(np.random.randn(1000,2), mat4)+np.array([[2.,2.]])\n",
    "\n",
    "X_train = np.concatenate((X_train_1, X_train_2, X_train_3, X_train_4), axis=0)\n",
    "Y_train = np.concatenate(( np.ones(1000), 2*np.ones(1000), 3*np.ones(1000), 4*np.ones(1000) ))\n",
    "\n",
    "\n",
    "X_test_1 = np.dot(np.random.randn(1000,2), mat1)+np.array([[0.,0.]])\n",
    "X_test_2 = np.dot(np.random.randn(1000,2), mat2)+np.array([[0.,2.]])\n",
    "X_test_3 = np.dot(np.random.randn(1000,2), mat3)+np.array([[2.,0.]])\n",
    "X_test_4 = np.dot(np.random.randn(1000,2), mat4)+np.array([[2.,2.]])\n",
    "\n",
    "X_test = np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4), axis=0)\n",
    "Y_test = np.concatenate(( np.ones(1000), 2*np.ones(1000), 3*np.ones(1000), 4*np.ones(1000) ))\n",
    "\n",
    "\n",
    "\n",
    "Y_pred_test_2a = Bayes2a(X_train, Y_train, X_test)\n",
    "Y_pred_test_2b = Bayes2b(X_train, Y_train, X_test)\n",
    "Y_pred_test_2c = Bayes2c(X_train, Y_train, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# Problem 2\n",
    "\n",
    "2d) Run the above three algorithms (Bayes2a,2b and 2c), for the two datasets given (dataset2_1.npz, dataset2_2.npz) in the cell below.\n",
    "\n",
    "In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 2 datasets = 6 plots) on a 2d plot (color the 4 areas classified as 1,2,3 and 4 differently). Add the training data points also on the plot. Plots to be organised as follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 6 plots appropriately. \n",
    "\n",
    "In the next Textwrite cell, summarise your observations regarding the six learnt classifiers. Give the *expected loss* (use the Loss matrix given in the problem.) of the three classifiers on the two datasets (use X_test and Y_test) as 2x3 table, with appropriately named rows and columns. Also, give the 4x4 confusion matrix of the final classifier for all three algorithms and both datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite\n",
    "# write the code for loading the data, running the three algos, and plotting here. \n",
    "# (Use the functions written previously.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite ** \n",
    "(Write your observations and table of errors here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type: convenience\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "\n",
    "# Problem 3 : Analyse overfitting and underfitting in Regression\n",
    "\n",
    "\n",
    "Consider the 2-dimensional regression dataset \"dateset3_1.npz\". Do polynomial ridge regression for degrees = [1,2,4,8,16], and regularisation parameter $\\lambda$ = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]. Do all the above by using four different subset sizes of the training set : 50, 100, 200 and 1000. (Just take the approppriate number samples of X_train and Y_train.)\n",
    "\n",
    "Regularised Risk = $\\frac{1}{2} \\sum_{i=1}^m (w^\\top \\phi(x_i) - y_i)^2 + \\frac{\\lambda}{2} ||w||^2 $ \n",
    "\n",
    "The lambda value is given by the regularisation parameter.\n",
    "\n",
    "For each training set size compute how the train and test squared error varies with degree and regularisation parameter, via a 5x6 table (where $(i,j)^{th}$ entry corrosponds to $(degree, \\lambda)$ ) with appropriate row and column headings. Compute the \"best\" degree and regularisation parameter based on the test squared error. Give a contour plot of the learned function for the chosen hyper-parameters, with appropriate title and labels . \n",
    "\n",
    "Summarise your findings in the next text cell in a few sentences. And reproduce the tables showing train and test error for various training sizes. You may reuse functions defined previously.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "visualize_polynomial_contour() got an unexpected keyword argument 'plot_title'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 185\u001b[39m\n\u001b[32m    183\u001b[39m fig = plt.figure()\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, (opt_wt, size, deg) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(opt_weight_vectors, training_sizes, optimal_degrees), start=\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[43mplot_polynomial_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_wt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_padding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m plt.subplots_adjust(wspace=\u001b[32m0.50\u001b[39m)\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Uncomment the line below to print MSE values if needed\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mplot_polynomial_classifier\u001b[39m\u001b[34m(subplot_position, weight_vector, training_size, title_padding, font_size, polynomial_degree)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03mPlots a 2D polynomial classifier visualization for the given weight vector, polynomial degree,\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03mand training sample size at the specified subplot position.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m    None.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, subplot_position)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[43mvisualize_polynomial_contour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_title\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptimum Classifier: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtraining_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m training points\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimum Classifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training points\u001b[39m\u001b[33m\"\u001b[39m, pad=title_padding)\n\u001b[32m    133\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mX1\u001b[39m\u001b[33m'\u001b[39m, fontsize=font_size)\n",
      "\u001b[31mTypeError\u001b[39m: visualize_polynomial_contour() got an unexpected keyword argument 'plot_title'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell type : CodeWrite\n",
    "# \n",
    "def plot_ridge_regress_train(X_train, Y_train, degree=1, reg_param=0.01):\n",
    "    \"\"\"Trains a ridge regression model using the given data \n",
    "        @returns w a 1D numpy array of shape (n_features,) containing the learned model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate polynomial features manually\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_features = int((degree + 1) * (degree + 2) / 2)  # Calculate number of features\n",
    "    \n",
    "    phi = np.ones((num_samples, num_features))  # Initialize feature matrix\n",
    "    \n",
    "    col_index = 0\n",
    "    for i in range(degree + 1):\n",
    "        for j in range(degree + 1 - i):\n",
    "            phi[:, col_index] = (X_train[:, 0] ** i) * (X_train[:, 1] ** j)\n",
    "            col_index += 1\n",
    "    \n",
    "    # Calculate weight vector using ridge regression formula\n",
    "    K = phi.T @ phi\n",
    "    I = np.eye(num_features)\n",
    "    w = np.linalg.solve(K + reg_param * I, phi.T @ Y_train) \n",
    "    \n",
    "    return w\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def plot_ridge_regress_predict(X_test, wt_vector, degree=1):\n",
    "    \"\"\"Predicts output targets using the previously trained model and NumPy.\"\"\"\n",
    "    \n",
    "    # Generate polynomial features for test data\n",
    "    num_samples = X_test.shape[0]\n",
    "    num_features = int((degree + 1) * (degree + 2) / 2)  # Calculate number of features\n",
    "    \n",
    "    phi = np.ones((num_samples, num_features))  # Initialize feature matrix\n",
    "    \n",
    "    col_index = 0\n",
    "    for i in range(degree + 1):\n",
    "        for j in range(degree + 1 - i):\n",
    "            phi[:, col_index] = (X_test[:, 0] ** i) * (X_test[:, 1] ** j)\n",
    "            col_index += 1\n",
    "    \n",
    "    # Predict using the calculated weight vector\n",
    "    Y_test_pred = phi @ wt_vector\n",
    "    \n",
    "    return Y_test_pred\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def visualize_polynomial_contour(weights, poly_degree, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plots a 2D contour map of the polynomial function defined by the given weights and degree.\n",
    "    Expectation : Give a contour plot of the learned function for the chosen hyper-parameters, with appropriate title and labels\n",
    "    \"\"\"\n",
    "    grid_x = np.linspace(-1, 1, 100)\n",
    "    grid_y = np.linspace(-1, 1, 100)\n",
    "    X, Y = np.meshgrid(grid_x, grid_y)\n",
    "\n",
    "    Z_values = np.zeros_like(X)\n",
    "\n",
    "    for row_idx, _ in enumerate(X):\n",
    "        input_points = np.column_stack((X[row_idx, :], Y[row_idx, :]))\n",
    "        Z_values[row_idx, :] = plot_ridge_regress_predict(input_points, weights, poly_degree)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.contourf(X, Y, Z_values, levels=np.linspace(0., 1.2, 20), cmap='gist_ncar')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    ####################################################################\n",
    "def generate_mse_matrices(degrees, regularization_values):\n",
    "    \"\"\"\n",
    "    Creates a list of zero-filled 2D numpy arrays to store mean squared errors.\n",
    "\n",
    "    Args:\n",
    "        degrees (list): A list of integers specifying the polynomial degrees to be used.\n",
    "        regularization_values (list): A list of floats specifying the regularization parameters to be used.\n",
    "\n",
    "    Returns:\n",
    "        mse_matrices (list): A list of 2D numpy arrays of shape (len(degrees), len(regularization_values)) filled with zeros.\n",
    "    \"\"\"\n",
    "    return [np.zeros((len(degrees), len(regularization_values))) for _ in range(8)]\n",
    "\n",
    "    #########################################################################\n",
    "\n",
    "def train_and_evaluate_ridge(X_train, Y_train, X_test, Y_test, poly_degree, reg_param):\n",
    "    \"\"\"\n",
    "    Trains a ridge regression model using the given training data and regularization parameter,\n",
    "    then computes the predicted output, weight vector, and mean squared error (MSE) using the test data.\n",
    "\n",
    "    Takes the following arguments :\n",
    "        X_train (ndarray): Training input features (shape: (n_train_samples, 2)).\n",
    "        Y_train (ndarray): Training output targets (shape: (n_train_samples,)).\n",
    "        X_test (ndarray): Test input features (shape: (n_test_samples, 2)).\n",
    "        Y_test (ndarray): True test output targets (shape: (n_test_samples,)).\n",
    "        poly_degree (int): Polynomial degree for feature transformation.\n",
    "        reg_param (float): Regularization parameter for ridge regression.\n",
    "\n",
    "    Returns:\n",
    "        weight_vector (ndarray): Learned model parameters (shape: (n_features,)).\n",
    "        predicted_Y (ndarray): Predicted output targets for the test set (shape: (n_test_samples,)).\n",
    "        mse (float): Mean squared error between predicted and true test targets.\n",
    "    \"\"\"\n",
    "    weight_vector = plot_ridge_regress_train(X_train, Y_train, poly_degree, reg_param)\n",
    "    predicted_Y = plot_ridge_regress_predict(X_test, weight_vector, poly_degree)\n",
    "    mse = np.mean((predicted_Y - Y_test) ** 2)\n",
    "    \n",
    "    return weight_vector, predicted_Y, mse  \n",
    "  \n",
    "###############################################################################\n",
    "\n",
    "def plot_polynomial_classifier(subplot_position, weight_vector, training_size, title_padding, font_size, polynomial_degree):\n",
    "    \"\"\"\n",
    "    Plots a 2D polynomial classifier visualization for the given weight vector, polynomial degree,\n",
    "    and training sample size at the specified subplot position.\n",
    "\n",
    "    Args:\n",
    "        subplot_position (int): Position index in the 1x4 grid of subplots.\n",
    "        weight_vector (ndarray): Learned model parameters (shape: (n_features,)).\n",
    "        training_size (int): Number of training samples used to train the model.\n",
    "        title_padding (float): Padding distance of the subplot title from the top in points.\n",
    "        font_size (int): Font size for x and y axis labels.\n",
    "        polynomial_degree (int): Polynomial degree used in the model.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    plt.subplot(1, 4, subplot_position)\n",
    "    visualize_polynomial_contour(weight_vector, polynomial_degree, plot_title=f\"Optimum Classifier: {training_size} training points\")\n",
    "    plt.title(f\"Optimum Classifier: {training_size} training points\", pad=title_padding)\n",
    "    plt.xlabel('X1', fontsize=font_size)\n",
    "    plt.ylabel('X2', fontsize=font_size)\n",
    "\n",
    "def display_mse_results(*mse_values):\n",
    "    \"\"\"\n",
    "    Prints the mean squared errors (MSEs) for different test and training dataset sizes.\n",
    "\n",
    "    Args:\n",
    "        mse_values (tuple of ndarrays): MSEs for test and training datasets with different numbers of points.\n",
    "    \"\"\"\n",
    "    dataset_sizes = [50, 100, 200, 1000]\n",
    "    print(\"MSE for different dataset sizes:\")\n",
    "    print(\"--------------------------------\")\n",
    "    for i, size in enumerate(dataset_sizes):\n",
    "        print(f\"{size} Points - TEST Dataset:\\n {mse_values[i]}\")\n",
    "        print(f\"{size} Points - TRAIN Dataset:\\n {mse_values[i+4]}\")\n",
    "########################### func definations end here ####################\n",
    "\n",
    "data = np.load('./dataset3_1.npz')\n",
    "# data = np.load('../../Data/dataset3_1.npz')\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "\n",
    "# given values\n",
    "degree_values = [1, 2, 4, 8, 16]\n",
    "reg_params = [1e-9, 1e-7, 1e-5, 1e-3, 1e-1, 1e1]\n",
    "\n",
    "# Define subsets of training data with different sample sizes\n",
    "training_sizes = [50, 100, 200, 1000]\n",
    "training_subsets = [(X_train[:size, :], Y_train[:size]) for size in training_sizes]\n",
    "\n",
    "# Initialize MSE matrices\n",
    "mse_matrices = generate_mse_matrices(degree_values, reg_params)\n",
    "MSE_test = mse_matrices[:4]\n",
    "MSE_train = mse_matrices[4:]\n",
    "\n",
    "# Compute MSE for all combinations of degree and regularization\n",
    "for i, degree in enumerate(degree_values):\n",
    "    for j, reg_param in enumerate(reg_params):\n",
    "        for k, (X_subset, Y_subset) in enumerate(training_subsets):\n",
    "            MSE_test[k][i, j] = train_and_evaluate_ridge(X_subset, Y_subset, X_test, Y_test, degree, reg_param)[2]\n",
    "            MSE_train[k][i, j] = train_and_evaluate_ridge(X_subset, Y_subset, X_train, Y_train, degree, reg_param)[2]\n",
    "\n",
    "# Compute optimal weight vectors for different datasets\n",
    "optimal_degrees = [2, 4, 16, 16]\n",
    "optimal_regs = [1e-1, 1e-7, 1e-9, 1e-9]\n",
    "opt_weight_vectors = [plot_ridge_regress_train(X, Y, d, r) for (X, Y), d, r in zip(training_subsets, optimal_degrees, optimal_regs)]\n",
    "\n",
    "# Plot classifier results\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "fig = plt.figure()\n",
    "for idx, (opt_wt, size, deg) in enumerate(zip(opt_weight_vectors, training_sizes, optimal_degrees), start=1):\n",
    "    plot_polynomial_classifier(idx, opt_wt, size, title_padding=20, font_size=15, polynomial_degree=deg)\n",
    "plt.subplots_adjust(wspace=0.50)\n",
    "\n",
    "# Uncomment the line below to print MSE values if needed\n",
    "display_mse_results(*MSE_test, *MSE_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite **"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "\n",
    "# Problem 4 : Bayesian Regression\n",
    "\n",
    "\n",
    "Consider the 2-dimensional regression dataset with 4 training points given below:\n",
    "\n",
    "(x1,   x2)   : y\n",
    "\n",
    "(1.0, 2.0)  : 5.1\n",
    "\n",
    "(2.0, 2.1)  : 6.1\n",
    "\n",
    "(0.6, 1.5)  : 4.2\n",
    "\n",
    "(1.1, 0.9)  : 2.9\n",
    "\n",
    "Assume $Y_i=W.X_i + \\epsilon_i$, where W is a random variable with prior distribution given by a Gaussian with mean $[\\mu_1, \\mu_2]$ and covariance given by $\\tau^2*I$. The random variables $\\epsilon_i$ are independent, and normally distributed with variance $\\sigma^2$. For each of the settings below, give the contours of the prior distribution and posterior distribution of $W$ given the 4 data points. Plot using a contour map.\n",
    "\n",
    "1. $\\mu_1=\\mu_2=0$ and $\\tau^2=1$, and $\\sigma^2=1$\n",
    "2. $\\mu_1=\\mu_2=0$ and $\\tau^2=10$, and $\\sigma^2=1$\n",
    "3. $\\mu_1=\\mu_2=0$ and $\\tau^2=1$, and $\\sigma^2=10$\n",
    "4. $\\mu_1=\\mu_2=0$ and $\\tau^2=10$, and $\\sigma^2=10$\n",
    "5. $\\mu_1=\\mu_2=5$ and $\\tau^2=1$, and $\\sigma^2=1$\n",
    "6. $\\mu_1=\\mu_2=5$ and $\\tau^2=10$, and $\\sigma^2=1$\n",
    "7. $\\mu_1=\\mu_2=5$ and $\\tau^2=1$, and $\\sigma^2=10$\n",
    "8. $\\mu_1=\\mu_2=5$ and $\\tau^2=10$, and $\\sigma^2=10$\n",
    "\n",
    "Repeat all the above experiments also with 400 data points, which are simply the 4 data points above repeated 100 times each.\n",
    "\n",
    "A total of 16\\*2 contour plots are to be given. \n",
    "\n",
    "Summarise your findings and conclusions in the final textcell. e.g. what happens when tau, sigma and number of data points are increased.\n",
    "\n",
    "Hint 1: In class we saw the expression for posterior of W, but only used it to find the MAP estimate. But it can be analysed in greater detail. You may read up classic textbooks like Bishop to figure out the posterior or work it out yourself. This is not too hard.\n",
    "\n",
    "Hint 2: In particular, observe that the posterior looks like a product of exponential functions, and the term inside the exponent is always a degree 2 (or less) function of w. Can you think of any distribution which looks like that?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell type : CodeWrite \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cell type : TextWrite **"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# 5. Logistic Regression \n",
    "\n",
    "Write code for doing logistic regression below. Also write code for choosing best hyperparameters for each kernel type (use a part of training set as validation set). \n",
    "\n",
    "The range of hyperparameters is typically chosen on a log scale e.g. 1e-4, 1e-3, 1e-2... 1e3.\n",
    "\n",
    "Write code for running in the cell after (You may be asked to demonstrate your code during the viva using this cell.)\n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For each classification data set report the best kernel and regularisation parameters for linear, RBF and Poly kernels. (Linear has no kernel parameter.) Report the training and test zero-one error for those hyperparameters. \n",
    "\n",
    "For each given hyperparameter setting (kernel and regularisation) you will have to do some exploring to find the right learning rate to use in gradient descent. The optimisation learning rate is not a model hyperparameter and hence can be chosen based on just the training set. i.e. choose the learning rate for which the training loss decreases the most.\n",
    "\n",
    "For the synthetic classification datasets (dataset_A and dataset_B) in 2-dimensions, also illustrate the learned classifier for each kernel setting. Do this in the last codeWrite cell for this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite \n",
    "#Write logistic regression code from scratch. Use gradient descent.\n",
    "# Only write functions here\n",
    "\n",
    "def train_pred_logistic_regression(X, Y, kernel='linear', reg_param=0., \n",
    "                                   kernel_param=1., num_iter_gd=100):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X : (n,d) shape numpy array\n",
    "    Y : (n,)  shape numpy array\n",
    "    X_test : (m,d) shape numpy array\n",
    "    kernel = 'linear' or 'rbf' or 'poly' \n",
    "    reg_param = $\\lambda$\n",
    "    num_iter_gd = number of GD iterations.\n",
    "\n",
    "    Returns the result of kernel logistic regression :\n",
    "    alpha: Vector of solutions for the dual. Numpy array of shape (n,)\n",
    "\n",
    "    Primal problem:\n",
    "    $ \\min_w  \\sum_{i=1}^n \\log(1+\\exp(-y_i* \\w^\\top \\phi(\\x_i)))  + \\frac{\\lambda}{2} ||\\w||^2 $\n",
    "\n",
    "    the dual of which is\n",
    "\n",
    "    $ \\min_alpha \\sum_{i=1}^n \\log(1+\\exp(-y_i* \\alpha^\\top K_{:,i} ))  + \\frac{\\lambda}{2} \\alpha^\\top K \\alpha $\n",
    "    where $\\phi$ is the feature got by the kernel.\n",
    "\n",
    "    Where K is the nxn kernel matrix computed on the training data.\n",
    "\n",
    "    The kernel is defined by the kernel_param:\n",
    "    If kernel=linear: K(\\u,\\v) = \\u^\\top \\v  \n",
    "    If kernel=poly:  K(\\u,\\v) = (1+\\u^\\top \\v)^(kernel_param)\n",
    "    If kernel=rbf:  K(\\u,\\v) = \\exp(-kernel_param*||\\u-\\v||^2)\n",
    "    \"\"\"\n",
    "\n",
    "def test_pred(alpha, train_X, train_Y, test_X, kernel, kernel_param):\n",
    "    \"\"\"\n",
    "    Return the predictions on test_X using the learnt alphas\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite : Use the functions above to do validation to get best hyperparameters \n",
    "# (i.e. kernel_param and regularisation_param).\n",
    "# Also, get the numbers you report below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite Cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error for all three kernel choices, for all 4 datasets (2 real world and 2 synthetic).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell: Generate plots of learned classifier for all three kernel types, on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 3 * 2 = 6\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# 6. Decision Tree\n",
    "\n",
    "Write code for learning decision tree below. Take as an argument a hyperparameter on what size node to stop splitting. Use a part of training set as validation set.\n",
    "\n",
    "Write code for running in the cell after (You may be asked to demonstrate your code during the viva using this cell.)\n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For all four data sets  report the best node size to stop splitting. Report the training and test zero-one error for those hyperparameters.\n",
    "\n",
    "For datasets A and B, also illustrate the learned classifier. Do this in the last codeWrite cell for this question.\n",
    "\n",
    "Important: Think about how you will represent a decision tree. (Possible soln: Store as a list of tuples containing node position, attribute to split, threshold, class to classifiy (if leaf node) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write Decision tree classifier from scratch, \n",
    "# write only functions here (you may write extra functions here if you wish)\n",
    "def train_decision_tree(X, Y, num_nodes_stop=1, criterion='accuracy'):\n",
    "    \"\"\" Returns a decision tree trained on X and Y. \n",
    "    Stops splitting nodes when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Returns a tree (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def eval_decision_tree(tree, test_X):\n",
    "    \"\"\" Takes in a tree, and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write code here for doing validation to find the best hyperparameters (i.e. num_nodes_stop)\n",
    "# Also Generate the numbers that you report below. \n",
    "# Repeat with criterion set to entropy also.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error, for all 4 classification datasets A,B,C,D. (2 real world and 2 synthetic).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Codewrite cell: Generate plots of learned decision tree classifier on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Plots only required for the accuracy criterion.\n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "\n",
    "# 7 Random Forest classifier\n",
    "\n",
    "Write code for learning RandomForests below. Fix the following hyper parameters: (Fraction of data to learn tree=0.5, Fraction of number of features chosen in each node=0.5, num_nodes_stop=1).  Choose the number of trees to add in the forest by using a validation set. You may use a slightly modified version of the decision tree code you had written earlier.\n",
    "\n",
    "Write code for running in the cell after the nest. (You may be asked to demonstrate your code during the viva using this cell.) \n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For all 4 classification data sets (A,B,C,D)  report the best number of trees found. Report the training and test zero-one error for those hyperparameters.\n",
    "\n",
    "For the synthetic classification datasets (datasets A and B) in 2-dimensions, also illustrate the learned classifier for each kernel setting. Do this in the last codeWrite cell for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write Random Forest classifier. \n",
    "def train_random_forest(X, Y, num_trees=10, num_nodes_stop=1, \n",
    "                        criterion='accuracy', a=0.5, b=0.5):\n",
    "    \"\"\" Returns a random forest trained on X and Y. \n",
    "    Trains num_trees.\n",
    "    Stops splitting nodes in each tree when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Fraction of data used per tree = a\n",
    "    Fraction of features used in each node = b\n",
    "    Returns a random forest (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def eval_random_forest(random_forest, test_X):\n",
    "    \"\"\" Takes in a  random forest object (however you want to store it), and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CodeWrite cell\n",
    "# Write code for choosing the best hyperparameters (num_trees, num_nodes_stop)\n",
    "# Write code here for generating the numbers that you report below.\n",
    "# Repeat above for criterion set to entropy also.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextWrite cell: Give your observations and the list of hyperparameter choices and train zero-one error  and test zero-one error, for all 4 datasets (2 real world and 2 synthetic).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Codewrite cell: Generate plots of learned Random Forest classifier on dataset_A and datasset_B.\n",
    "# Plots should give both the learned classifier and the train data. \n",
    "# Plots required only for the accuracy criterion.\n",
    "# Similar to  Bishop Figure 4.5 (with just two classes here.)\n",
    "# Total number of plots = 2 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# 8 AdaBoost\n",
    "\n",
    "Write code for learning using AdaBoost below. Use 3 different weak learners below. (You may reuse code written above)\n",
    "\n",
    "1. 1 node decision tree \n",
    "2. Decision tree of fixed depth = 3 (Root, child, grand child)\n",
    "3. Decision tree of fixed depth = 7 (Root, child, grand child, ..., great^4 grand child)\n",
    "\n",
    "Run for 50 iterations. You may use the accuracy split criterion for all the three weak learners.\n",
    "\n",
    "Write code for running in the next cell. (You may be asked to demonstrate your code during the viva using this cell.) \n",
    "\n",
    "In text cell after that report the following numbers you get by running appropriate code:\n",
    "\n",
    "For all 4 classification data sets (A,B,C,D)  plot the train and test accuracy vs iterations. A total of 12 plots is expected. 4 datasets * 3 weak learners. Each plot should contain two curves, train and test error.  \n",
    "\n",
    "For the synthetic classification datasets (datasets A and B) in 2-dimensions, also illustrate the learned classifier for each weak learner setting. A total of 6 contourf style plots are expected here. Do this in the last codeWrite cell for this question.\n",
    "\n",
    "Summarise your observations in the last textwrite cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell\n",
    "# Write code to run here (no plotting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell \n",
    "# Plots for iteration vs error here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Codewrite cell \n",
    "# Plots for illustrating the classifier here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textwrite cell:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cell type : TextRead**\n",
    "\n",
    "# 9 Lasso Regression\n",
    "\n",
    "Write code for learning using Lasso Regression and give your conclusions. Use the dataset LassoReg_data.npz for this question. The file contains two matrices of size 120\\*1000 and 120\\*1, corresponding to 120 instance points with 1000 dimensional features and its targets.\n",
    "\n",
    " Split the data into train-validation-test on 50-25-25 ratio. Learn the best model using Lasso Regression (use projected gradient descent, the projection oracle code is given for your convenience). Try different learning rate parameters and L1 norm ball constraint radii. Choose an appropriate learning rate that allows for convergence of the training loss.  Train the models for different L1 norm radius parameters. Choose the L1 norm constraint that works best on the validation set. \n",
    "\n",
    "In the last textwrite cell below, report the test error of the learned model thus chosen. Also report the indices and weight values corresponding to the top 10 values of the weight vector (which is 1000 dimensional). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coderead cell\n",
    "\n",
    "def projection_oracle_l1(w, l1_norm):\n",
    "    # first remeber signs and store them. Modify w so that it is all positive then.\n",
    "    signs = np.sign(w)\n",
    "    w = w*signs\n",
    "    # project this modified w onto the simplex in first orthant.\n",
    "    d=len(w)\n",
    "    # if w is already in l1 norm ball return as it is.\n",
    "    if np.sum(w)<=l1_norm:\n",
    "        return w*signs\n",
    "    \n",
    "    # using 1e-7 as zero here to avoid foating point issues\n",
    "    for i in range(d):\n",
    "        w_next = w+0\n",
    "        w_next[w>1e-7] = w[w>1e-7] - np.min(w[w>1e-7])\n",
    "        if np.sum(w_next)<=l1_norm:\n",
    "            w = ((l1_norm - np.sum(w_next))*w + (np.sum(w) - l1_norm)*w_next)/(np.sum(w)-np.sum(w_next))\n",
    "            return w*signs\n",
    "        else:\n",
    "            w=w_next\n",
    "\n",
    "# test above code\n",
    "# print (projection_oracle_l1(w=np.array([1.,2,3,4]), l1_norm=2))\n",
    "# Projecting the vector (1,2,3,4) on to the l1 norm ball of radius 2 will give (0,0,0.5,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codewrite cell:\n",
    "# Use this cell to do read the data and do preprocessing (split data into train, test, val etc) and write any helper functions you may need \n",
    "# like evaluating the mean squared error or the gradient w.r.t. w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codewrite cell:\n",
    "\n",
    "# Write the code for the gradient descent routine on the training set mean square error loss function.\n",
    "# Also write code for doing validation of the learned model using the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Textwrite cell:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

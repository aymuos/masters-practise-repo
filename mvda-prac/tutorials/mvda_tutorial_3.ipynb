{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mceaPYSJ5uD"
      },
      "source": [
        "# CH5440: Multivariate Data Analysis for Process Modelling\n",
        "\n",
        "#### Tutorial #3 will cover:\n",
        "\n",
        "1. Preprocessing data for regression tasks\n",
        "2. Simple and multiple linear regression\n",
        "3. Confidence intervals for linear regression estimates (coefficients and predictions)\n",
        "4. Model evaluation using key metrics and plots\n",
        "5. Mapping non-linear relations to OLS framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: statsmodels in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (0.14.4)\n",
            "Requirement already satisfied: numpy>=1.19.5 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from statsmodels) (2.2.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\documents\\github\\masters-practise-repo\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZesVx_pyJ5uF"
      },
      "source": [
        "Importing Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aU1CVI0HJ5uF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv4Hl3eyJ5uG"
      },
      "source": [
        "Defining function for evaluation of models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XErTsdB0J5uG"
      },
      "outputs": [],
      "source": [
        "def adjusted_r2(y_true, y_pred,n,p):\n",
        "\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
        "    return adj_r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PYRNmAkFJ5uH"
      },
      "outputs": [],
      "source": [
        "def metrics(y_test,predictions):\n",
        "    residuals=y_test-predictions\n",
        "\n",
        "    print(f'\\nThe Mean Absolute Error on the test data is: {mean_absolute_error(y_test,predictions):.3f}\\n')\n",
        "    print(f'\\nThe RMSE on the test data is: {mean_squared_error(y_test,predictions)**0.5:.3f}\\n')\n",
        "    print(f'\\nThe R2 value is: {r2_score(y_test,predictions):.3f}\\n')\n",
        "\n",
        "    fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(16,6),dpi=150)\n",
        "\n",
        "    ### scatter plot of y_test and predictions\n",
        "    axes[0].scatter(y_test, predictions)\n",
        "    axes[0].set_xlabel('Given Test Values')\n",
        "    axes[0].set_ylabel('Predicted Values')\n",
        "    axes[0].set_title('Scatter: Test vs. Predicted Values')\n",
        "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')  # Diagonal line\n",
        "\n",
        "    ### residual plot\n",
        "    axes[1].scatter(predictions, residuals)\n",
        "    axes[1].axhline(y=0, color='red', linestyle='--')  # Horizontal line at zero\n",
        "    axes[1].set_xlabel('Predicted Values')\n",
        "    axes[1].set_ylabel('Residuals')\n",
        "    axes[1].set_title('Residuals Plot')\n",
        "\n",
        "    # Q-Q plot\n",
        "    sm.qqplot(residuals, line='s',ax=axes[2])  # 's' for standardized line\n",
        "    axes[2].set_title('Q-Q Plot')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.3)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPhUxGtnJ5uH"
      },
      "source": [
        "# Preproceesing Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xWJsjxyJ5uI"
      },
      "source": [
        "### Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_5UTzuJ5uI"
      },
      "source": [
        "Load and check dataset info: We'll be working with Advertising dataset which we already explored in Linear Algebra tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ObdgZI29J5uI",
        "outputId": "ecceb82d-9117-4415-f8af-77f36d509bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200 entries, 0 to 199\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   TV         194 non-null    float64\n",
            " 1   radio      194 non-null    float64\n",
            " 2   newspaper  193 non-null    float64\n",
            " 3   sales      200 non-null    float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 6.4 KB\n"
          ]
        }
      ],
      "source": [
        "advert=pd.read_csv('Advertising_unclean.csv')\n",
        "advert.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUC8jMn6J5uJ"
      },
      "source": [
        "There are some missing values in this dataset, so let's see, how we can clean up the dataframe, so it's ready for carrying out regression analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e5LXczvJ5uJ"
      },
      "outputs": [],
      "source": [
        "### displaying all rows which contain any null values\n",
        "advert[advert.isnull().any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzuKOqGLJ5uJ"
      },
      "source": [
        "There are some rows which contain null values for TV, radio and newspaper columns. We can drop those rows, as they have too many missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqLmsmpRJ5uK"
      },
      "outputs": [],
      "source": [
        "advert[advert.drop('sales',axis=1).isnull().all(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psznFfh2J5uK"
      },
      "outputs": [],
      "source": [
        "row_to_drop=[14,46,83,109,145,195]\n",
        "advert = advert.drop(row_to_drop)\n",
        "advert.reset_index(drop=True, inplace=True)\n",
        "advert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohximY4gJ5uK"
      },
      "source": [
        "Checking if any more invalid entries are present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwfM9WhgJ5uK"
      },
      "outputs": [],
      "source": [
        "advert[advert.isnull().any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWlH9PQQJ5uK"
      },
      "source": [
        "As only value for newspaper column is missing for this row, we can set it equal to the mean of all newspaper entries as an approximation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C286TdWAJ5uL"
      },
      "outputs": [],
      "source": [
        "advert.iloc[193,2]=advert['newspaper'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUYHTU85J5uL"
      },
      "outputs": [],
      "source": [
        "advert.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZBnMktEJ5uL"
      },
      "source": [
        "Dataset is clean now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTvQQh5mJ5uL"
      },
      "source": [
        "### Splitting data into Training and Test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nfgBRQ-J5uL"
      },
      "source": [
        "Defining X and y for regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhDWQYloJ5uL"
      },
      "outputs": [],
      "source": [
        "X,y=advert.drop('sales',axis=1).values,advert['sales'].values.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJIPpIn2J5uL"
      },
      "source": [
        "Splitting helps in understand how well the model performs on unseen data, in this case, the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmNEQvtaJ5uM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiw1WjKmJ5uM"
      },
      "source": [
        "### One Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbx8M_wjJ5uM"
      },
      "source": [
        "Below is the Fish Market dataset, where we have different characteristics of fish - species and physical measurement. The weight of the fish is the target variable, and we are interested in building a model to predict the weight of a fish given its characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGdE8_NBJ5uM"
      },
      "outputs": [],
      "source": [
        "fish=pd.read_csv('Fish.csv')\n",
        "fish.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsQPRuOJ5uM"
      },
      "source": [
        "The 'Species' column contains categorical data. We can one-hot encode it to convert to a numerical format, which is necessary for algorithms like linear regression which require numerical input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IffghQjuJ5uM"
      },
      "outputs": [],
      "source": [
        "fish = pd.get_dummies(fish, columns=['Species'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "361Uv8WcJ5uN"
      },
      "outputs": [],
      "source": [
        "fish.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf_BkVSOJ5uN"
      },
      "source": [
        "# Simple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJvY4ywvJ5uN"
      },
      "source": [
        "Below we are synthetically generating data for 'Exam Score' vs 'Hours Studied' to understand how linear regression can be implemented in python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fzvbzgDJ5uN"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # Feature: Hours studied (0-10)\n",
        "y = 2.5 * X + np.random.randn(100, 1) * 2  # Target: Exam scores with some noise\n",
        "\n",
        "# Convert to a DataFrame\n",
        "example = pd.DataFrame(data={'Hours_Studied': X.flatten(), 'Exam_Score': y.flatten()})\n",
        "example.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbYMD0rwJ5uN"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(example[['Hours_Studied']].values.reshape(-1,1), example['Exam_Score'].values.reshape(-1,1), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SikVjv6lJ5uN"
      },
      "source": [
        "Using OLS formula and matrix operations to find the coefficients of the linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luck0cKkJ5uO"
      },
      "outputs": [],
      "source": [
        "X0=np.ones(X_train.shape[0]).reshape(-1,1)\n",
        "X_new=np.hstack((X0,X_train.reshape(-1,1)))\n",
        "beta=np.linalg.inv(X_new.T @ X_new) @ (X_new.T @ y_train)\n",
        "print(f'The intercept is : {beta[0][0]:.3f} and slope is {beta[1][0]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iShlKVSUJ5uO"
      },
      "source": [
        "Using sklearn's inbuilt fucntion for performing linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtdG-hpeJ5uS"
      },
      "outputs": [],
      "source": [
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "##evaluate\n",
        "\n",
        "metrics(y_test,predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAbwm6TUJ5uS"
      },
      "source": [
        "Understanding evaluation metrics:\n",
        "\n",
        "MAE and RMSE are in the same unit as the dependent variable 'y', and are a measure of error.\n",
        "The r2 value indicates how well the model is able to explain the variance in the dependent variable 'y'\n",
        "\n",
        "1. The test vs predicted values scatter plot helps assess how well the model predictions match the actual test values. Points close to the 45 deg line indicate good predictions.\n",
        "2. A residual plot shows the difference between actual test values and predicted values. If the residuals are randomly scattered around zero, this indicates that the model captures the underlying pattern well, and the assumptions of linear regression are likely satisfied\n",
        "3. A Q-Q plot helps assess whether the residuals follow a normal distribution, which is an important assumption in linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQe6rGkYJ5uS"
      },
      "source": [
        "### Comparing results using OLS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhqw-PtWJ5uS"
      },
      "outputs": [],
      "source": [
        "print(f'The coeffient of the model is {model.coef_[0][0]:.3f} and the intercept obtained is {model.intercept_[0]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QW1XBixJ5uT"
      },
      "outputs": [],
      "source": [
        "beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wM0sm5jJ5uT"
      },
      "source": [
        "Thus, from above results we can see that sklearn's LinearRegression uses OLS for computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG-dwkraJ5uT"
      },
      "source": [
        "# Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubo_0IfZJ5uT"
      },
      "source": [
        "We go back to the advertising dataset and explore how to do multiple linear regression. We use statsmodels.api for linear regression this time. This library is useful for detailed statistical analysis and inference from the linear regression model, and in this case we can use it for looking at the confidence intervals for the regression coefficients and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYNLHUucJ5uT"
      },
      "outputs": [],
      "source": [
        "X,y=advert.drop('sales',axis=1).values,advert['sales'].values.reshape(-1,1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train=sm.add_constant(X_train)\n",
        "X_test=sm.add_constant(X_test)\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Step 5: Print the model summary\n",
        "print(model.summary())\n",
        "\n",
        "conf_int = model.conf_int()\n",
        "print(\"\\nConfidence Intervals for Coefficients:\\n\")\n",
        "print(conf_int)\n",
        "\n",
        "predictions = model.get_prediction(X_test)\n",
        "pred_int = predictions.conf_int()\n",
        "\n",
        "plt.scatter(y_test, predictions.predicted_mean, label='Predicted vs Actual', alpha=0.5)\n",
        "plt.plot(y_test, y_test, color='red', linestyle='--', label='Perfect Prediction')\n",
        "plt.fill_between(y_test.flatten(), pred_int[:, 0], pred_int[:, 1], color='blue', alpha=0.2, label='95% Confidence Interval')\n",
        "plt.title('Predictions and Confidence Intervals')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLpIMOPAJ5uT"
      },
      "outputs": [],
      "source": [
        "metrics(y_test,predictions.predicted_mean.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfk81GE9J5uU"
      },
      "source": [
        "Understanding adjusted r2 metric:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx_eAvfzJ5uU"
      },
      "source": [
        "Let's add an irrelevant feature to the above advertisement data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMrQzdODJ5uU"
      },
      "outputs": [],
      "source": [
        "X,y=advert.drop('sales',axis=1).values,advert['sales'].values.reshape(-1,1)\n",
        "np.random.seed(0)\n",
        "random_features = np.random.rand(X.shape[0], 10)  # Adding 10 random noise features\n",
        "X_new = np.hstack((X, random_features))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "##evaluate\n",
        "\n",
        "metrics(y_test,predictions)\n",
        "\n",
        "print(f'adjusted r2 is: {adjusted_r2(y_test,predictions,X_test.shape[0],X_test.shape[1]):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS_06NKeJ5uU"
      },
      "source": [
        "### Mapping non-linear relations to OLS/linear framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1IeSx-RJ5uU"
      },
      "source": [
        "By transforming non-linear relationships into a linear form, you can apply familiar techniques and interpret the results in a straightforward manner.\n",
        "\n",
        "Given below is an example, where we have been given enzyme rate measurements and substrate concentration. We want to find a suitable model which describes the kinetics of the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98buAHVHJ5uU"
      },
      "outputs": [],
      "source": [
        "kinetic=pd.read_csv('kinetic_dataset_2.csv')\n",
        "kinetic.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5YzFpDEJ5uV"
      },
      "source": [
        "Using linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHcMdiz2J5uV"
      },
      "outputs": [],
      "source": [
        "X,y=kinetic['substrate'].values.reshape(-1,1),kinetic['reaction_rate'].values.reshape(-1,1)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "model=LinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "predictions=model.predict(X_test)\n",
        "metrics(y_test,predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4n1uGG3J5uV"
      },
      "source": [
        "The linear regression model has a poor r2 value, meaning the model is unable to explain variation in rate measurement data.\n",
        "\n",
        "Let's try with polynomial regression. But how to select degree of suitable polynomial? We can test polynomials from degree 1 to degree 5 and see which is a better fit model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDPhvCGiJ5uV"
      },
      "outputs": [],
      "source": [
        "degrees = range(1, 6)\n",
        "results = []\n",
        "rmse=[]\n",
        "mae=[]\n",
        "r2=[]\n",
        "\n",
        "X,y=kinetic['substrate'].values.reshape(-1,1),kinetic['reaction_rate'].values.reshape(-1,1)\n",
        "\n",
        "for degree in degrees:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    X_poly=sm.add_constant(X_poly)\n",
        "    X_train,X_test,y_train,y_test=train_test_split(X_poly, y, test_size=0.3, random_state=42)\n",
        "    model = sm.OLS(y_train, X_train).fit()\n",
        "    predictions = model.get_prediction(X_test)\n",
        "\n",
        "    coefficients = model.params\n",
        "    p_values = model.pvalues\n",
        "    results.append((degree, coefficients, p_values))\n",
        "    mae.append(mean_absolute_error(y_test,predictions.predicted_mean.reshape(-1,1)))\n",
        "    rmse.append(mean_squared_error(y_test,predictions.predicted_mean.reshape(-1,1))**0.5)\n",
        "    r2.append(r2_score(y_test,predictions.predicted_mean.reshape(-1,1)))\n",
        "\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['Degree', 'Coefficients', 'P-values'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3DQ8HYLJ5uV"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWVz3YuJ5uV"
      },
      "source": [
        "From above results on coefficients and p-values, all the p-values are less than 0.05 (signicance level for our case here), so all these polynomials seem to be significant as per this analysis. Let's check the metrics of each of these polynomials to narrow down on the better fitting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMkR3rlWJ5uW"
      },
      "outputs": [],
      "source": [
        "fig,ax1 = plt.subplots(figsize=(16,6),dpi=150)\n",
        "\n",
        "\n",
        "ax1.plot(degrees, mae)\n",
        "ax1.set_xlabel('Degree of Polynomial')\n",
        "ax1.set_ylabel('MAE')\n",
        "ax1.set_title('MAE and R2 score vs Degree Plot')\n",
        "\n",
        "\n",
        "ax2=ax1.twinx()\n",
        "ax2.plot(degrees, r2)\n",
        "ax2.set_xlabel('Degree of Polynomial')\n",
        "ax2.set_ylabel('R2 value')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSJtH8UGJ5uW"
      },
      "source": [
        "The elbow method provides a straightforward way to visualize and select the best polynomial degree by analyzing how the model's error changes with increasing complexity. The degree corresponding to this elbow point is considered optimal for the polynomial regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ql3cZfJ5uW"
      },
      "source": [
        "The above plot shows us degree 2 polynomial is giving the best performance, so let's implement and check it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxZxr_-tJ5uW"
      },
      "outputs": [],
      "source": [
        "poly_converter=PolynomialFeatures(degree=2)\n",
        "X_poly=poly_converter.fit_transform(X)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_poly, y, test_size=0.3, random_state=42)\n",
        "model=LinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "predictions=model.predict(X_test)\n",
        "metrics(y_test,predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFJIY8kFJ5uX"
      },
      "source": [
        "There is an improved r2 value and lower mae and mse observed with polynomial mapping, indicating the variables contain some non-linear relationships."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

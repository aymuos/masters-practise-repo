{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TSA_Nifty_50_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pending Activities** -\n",
        "\n",
        "Titles and axes names for all graphs\n",
        "check if auto_arima works here instead of your for loop\n",
        "\n",
        "model using ARCH"
      ],
      "metadata": {
        "id": "NbDa9qbEZT1s"
      },
      "id": "NbDa9qbEZT1s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Group - 8**\n",
        "\n",
        "CH24M548 - Mounika Chowdary Pamulapati\n",
        "\n",
        "CH24M571 - Soumya Mukherjee\n",
        "\n",
        "CH24M577 - Surya Kandala"
      ],
      "metadata": {
        "id": "85iU1mSF56Se"
      },
      "id": "85iU1mSF56Se"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps followed to model the given Nifty 50 data were the below:\n",
        "\n",
        "1. **Importing the dataset**.\n",
        "2. **Cleaning the dataset**: Ensured that missing dates were added and forward fill of the data for the missing dates. Dropped NaN records whereever observed.\n",
        "3. **Stationartiy Check**: Validated if the timeseries is stationary or not by performing ADF and KSS test. For ADF test if p<0.05, then it says the timeseries is stationary. If p>0.05 for KSS test, it indicates the timeseries is stationary.\n",
        "4. **First Order Differencing & Log Differencing**: Performing first order differencing to make the timeseries stationary.\n",
        "5. **ACF and PACF**: Compute Auto Correlation and Partial correlation to understand the presence of MA and AR components.\n",
        "6. **Power Spectral Density**: For a process to be stationary, both mean and variance need to be independent of the absolute time. Hence, validating if Variance is dependent on the absolute time.\n",
        "7. **Random Walk**: Based on the ACF and PACF values, going with ARMA model of 010 since ACF and PACF results indicate complete white noise.\n",
        "8. **Akaike's Information Criteria**: Since the forecast is only a straight line, checked the AIC values for models starting from 000 till 332. Chose that model which has less AIC value\n",
        "9. **Train, Test & Generate Forecast**: Generated the forecast for 122 model which got less AIC value in step 7.\n",
        "10. **MSE, RMSE, MAPE**: Identify the Mean Square Error and Root Mean Square Error to check the residual details.\n",
        "11. **Alternate Non-Lionear models**: Since we have variance as a function of absolute time, linear models might not be the right choice to move ahead, hence trying out the non-linear models ARCH and GARCH to model the data."
      ],
      "metadata": {
        "id": "ZK1_3yGm3Go7"
      },
      "id": "ZK1_3yGm3Go7"
    },
    {
      "cell_type": "markdown",
      "id": "dd68711e-70d3-4343-8d5c-ed4fb5898cf9",
      "metadata": {
        "id": "dd68711e-70d3-4343-8d5c-ed4fb5898cf9"
      },
      "source": [
        "Packages Required!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install statsmodels\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install arch\n",
        "!pip install statsforecast\n",
        "!pip install dask[dataframe]\n",
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "sj5KEWEMl4hR"
      },
      "id": "sj5KEWEMl4hR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f333dc-8833-4c6f-baec-2a5ddc0670ee",
      "metadata": {
        "id": "d2f333dc-8833-4c6f-baec-2a5ddc0670ee"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from scipy import signal\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from arch import arch_model\n",
        "from statsforecast import StatsForecast\n",
        "import pmdarima as pm\n",
        "import warnings\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import boxcox"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the dataset**:\n",
        "\n",
        "Reading the Training dataset from Google Drive and set the date field as index."
      ],
      "metadata": {
        "id": "ocvNRscJ786w"
      },
      "id": "ocvNRscJ786w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d665e4a-0cba-4ad9-999a-e0e31eee27f5",
      "metadata": {
        "id": "9d665e4a-0cba-4ad9-999a-e0e31eee27f5"
      },
      "outputs": [],
      "source": [
        "# Generate or load your dataset\n",
        "url='https://drive.google.com/file/d/16h5NCff5s-2V9lwksc9aCEKJIAn2Znyv/view?usp=drive_link'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url)\n",
        "# df = pd.read_csv('NIFTY_50_2015-2023.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='mixed')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Test dataset\n",
        "\n",
        "url='https://drive.google.com/file/d/1L-hcnQLVffdxz1PwwMTJkB6AmB2BkjBn/view?usp=drive_link'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df_test = pd.read_csv(url)\n",
        "## df_test = pd.read_csv('NIFTY_50_2024.csv')\n",
        "df_test['Date'] = pd.to_datetime(df_test['Date'], format='mixed')\n",
        "df_test['Date'] = pd.to_datetime(df_test['Date'])\n",
        "df_test.set_index('Date', inplace=True)\n",
        "\n",
        "\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "OMgOJK78wTqT"
      },
      "id": "OMgOJK78wTqT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Description** of Train and Test datasets"
      ],
      "metadata": {
        "id": "txtOmXYZwgyd"
      },
      "id": "txtOmXYZwgyd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Description of training data\n",
        "\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "lljo9t15lkGI"
      },
      "id": "lljo9t15lkGI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Description of the test data\n",
        "\n",
        "df_test.describe().T"
      ],
      "metadata": {
        "id": "aAnXvIUwwc16"
      },
      "id": "aAnXvIUwwc16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if there are any outliers in the data using Box Plot\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.boxplot(df['Close'],patch_artist = True, vert = 0)\n",
        "plt.title('Box Plot of Close Value')\n",
        "plt.xlabel('Closing Value')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g3B91wA8vHb2"
      },
      "id": "g3B91wA8vHb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning the dataset**:\n",
        "\n",
        "Checking for missing dates and adding them back to the dataset.\n",
        "Forwardfill the values from the previous rows. -- Currently, even if stock market is not open on Saturday and Sunday, still Friday closing will be the opening for Monday. So, assuming forward fill imputation would not be a problem."
      ],
      "metadata": {
        "id": "gcmtoFDOCvET"
      },
      "id": "gcmtoFDOCvET"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a00383-751d-4465-a681-e9a9acfa9281",
      "metadata": {
        "id": "e0a00383-751d-4465-a681-e9a9acfa9281"
      },
      "outputs": [],
      "source": [
        "start = df.index[0].date() # minimum date from the dataset\n",
        "end = df.index[len(df)-1].date() # maximum date from the dataset\n",
        "new_dates = pd.date_range(start=start,end=end,freq='D') # Generating new dates starting from minimum and extending till maximum\n",
        "\n",
        "# Re-indexing new dates with the old dates\n",
        "\n",
        "df = df.reindex(new_dates)\n",
        "df = df.rename_axis('Fin_Dt')\n",
        "df.reset_index(inplace=True)\n",
        "df = df.ffill()\n",
        "\n",
        "# Maintaining one date column as index and other date column for our reference.\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Fin_Dt'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing dates and add them to the dataset\n",
        "# Identify Start and End dates from the dataset and generate new dates from those dates which will be daily dates\n",
        "\n",
        "start = df_test.index[0].date()\n",
        "end = df_test.index[len(df_test)-1].date()\n",
        "new_dates = pd.date_range(start=start,end=end,freq='D')\n",
        "\n",
        "# Re-indexing new dates with the old dates\n",
        "\n",
        "df_test = df_test.reindex(new_dates)\n",
        "df_test = df_test.rename_axis('Fin_Dt')\n",
        "df_test.reset_index(inplace=True)\n",
        "df_test = df_test.ffill()\n",
        "\n",
        "df_test['Date'] = pd.to_datetime(df_test['Fin_Dt'])\n",
        "df_test.set_index('Date', inplace=True)\n",
        "\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "pdsg5Sc0xJ68"
      },
      "id": "pdsg5Sc0xJ68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the Training & Test dataset**.\n",
        "\n",
        "Graph is being plotted for the Close values as training and testing to be done for that column of the dataset."
      ],
      "metadata": {
        "id": "vMKYP2AEEG3-"
      },
      "id": "vMKYP2AEEG3-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc90738-13ca-4ddb-a2e0-ca621251316b",
      "metadata": {
        "id": "ecc90738-13ca-4ddb-a2e0-ca621251316b"
      },
      "outputs": [],
      "source": [
        "# Use the 'Close' prices as the target variable and plot the Original Data\n",
        "final_data = df[['Fin_Dt','Close']] ## will be used at a later point in the code.\n",
        "\n",
        "final_data_test = df_test[['Fin_Dt','Close']]\n",
        "\n",
        "close_plot_test = final_data_test['Close']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(final_data['Close'], label='Training Data')\n",
        "plt.plot(close_plot_test, color = 'red', label = 'Test Data')\n",
        "plt.title(\"Nifty 50 - Training & Test Data\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Closing Value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stationarity Check**:\n",
        "\n",
        "To check if the data is stationary, here we used Augmented Dickey Fuller's Test.\n",
        "This test helps us understand if there is a change in the Mean with absolute time. But, it doesn't give any information on the variance of the timeseries.\n"
      ],
      "metadata": {
        "id": "9q0AXuyMEkdu"
      },
      "id": "9q0AXuyMEkdu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb93cf8-e162-4071-b82d-7fe8025c0e83",
      "metadata": {
        "id": "5fb93cf8-e162-4071-b82d-7fe8025c0e83"
      },
      "outputs": [],
      "source": [
        "# Perform the ADF test\n",
        "adf_res = adfuller(df['Close'], autolag='AIC')\n",
        "\n",
        "print(f\"ADF Statistic: {adf_res[0]}\")\n",
        "print(f\"p-value: {adf_res[1]}\")\n",
        "print(f\"No Lags Used: {adf_res[2]}\")\n",
        "print(f\"Number of observations used: {adf_res[3]}\")\n",
        "print(f\"Critical Values: {adf_res[4]}\")\n",
        "\n",
        "if adf_res[1] <= 0.05:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"The data is stationary\")\n",
        "\n",
        "else:\n",
        "    print(\"The null hypothesis cannot be rejected\")\n",
        "    print(\"The data is not stationary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa532b80-f8a7-42b1-8073-3108ec8f1237",
      "metadata": {
        "id": "fa532b80-f8a7-42b1-8073-3108ec8f1237"
      },
      "source": [
        "**First Order Differencing**:\n",
        "\n",
        "Since the p-value is greater than 0.05, we fail to reject the null hypothesis, which means the data is not stationary.\n",
        "\n",
        "Lets do the first order differencing of the data to remove the varying Mean component. Below are the different ways that can be used to remove the varying mean and variance components.\n",
        "\n",
        "**Differencing**: Subtracting the current observation from the previous one to achieve stationarity.\n",
        "**Detrending**: Another technique for removing seasonality from time series data.\n",
        "**Seasonal decomposition**: Decomposing the time series into seasonal, trend, and residual components.\n",
        "**Log transformation**: Taking the natural logarithm of the data to stabilize variance.\n",
        "**Log difference**: Combining log transformation and differencing to address both trend and seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1f8574-afb1-4c37-bdea-e612efee163a",
      "metadata": {
        "id": "ce1f8574-afb1-4c37-bdea-e612efee163a"
      },
      "outputs": [],
      "source": [
        "# Seasonal adjustment\n",
        "df['Seasonal_Adjusted'] = df['Close'].diff(periods=7).dropna() ## An integer that controls the number of periods over which the difference is calculated.\n",
        "\n",
        "# Differencing\n",
        "df['Differenced'] = df['Close'].diff().dropna()\n",
        "\n",
        "# Log Differencing\n",
        "df['Log_Differenced'] = np.log(df['Close']/df['Close'].shift(1)).dropna()\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the data**\n",
        "\n",
        "Validate the difference between actual, differenced, seasonal, and log differenced data."
      ],
      "metadata": {
        "id": "8YcfQ5eXJxo8"
      },
      "id": "8YcfQ5eXJxo8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c22e10-b9ea-4307-b574-bb5b653fa657",
      "metadata": {
        "id": "19c22e10-b9ea-4307-b574-bb5b653fa657"
      },
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Original time series\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(final_data['Close'], color = 'blue')\n",
        "plt.title('Nifty 50 - Original Data')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Closing Value')\n",
        "\n",
        "# Seasonal Adjusted\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(df['Seasonal_Adjusted'], color = 'orange')\n",
        "plt.title('Seasonal Adjusted Time Series')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Seasonal Adjusted Closing Value')\n",
        "\n",
        "# Differenced\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(df['Differenced'], color = 'green')\n",
        "plt.title('Differenced Time Series')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Differenced Closing Value')\n",
        "\n",
        "# Log Differenced\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(df['Log_Differenced'], color = 'red')\n",
        "plt.title('Log Differenced Time Series')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Log Differenced Closing Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of HeatMap**:\n",
        "\n",
        "1. Seasonal Patterns\n",
        "2. Increasing Trend Over Time\n",
        "3. Anomalies and Deviations: 2020 shows an anamoly in both the Heat Maps"
      ],
      "metadata": {
        "id": "7AXaEEaF-XNY"
      },
      "id": "7AXaEEaF-XNY"
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_date = df[['Year','Month','Close']]\n",
        "pivot_diff = df[['Year','Month','Differenced']]\n",
        "pivot_log = df[['Year','Month','Log_Differenced']]\n",
        "\n",
        "df_pivot = pivot_date.pivot_table(index=\"Month\", columns='Year', values='Close', aggfunc='mean')\n",
        "df_diff_pivot = pivot_diff.pivot_table(index=\"Month\", columns='Year', values='Differenced', aggfunc='mean')\n",
        "\n",
        "# df_pivot.head()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.subplot(1,2,1)\n",
        "sns.heatmap(df_pivot, cmap='YlGnBu', annot=False)\n",
        "plt.title(\"Nifty 50 - Closing Value (2015 - 2023)\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.heatmap(df_diff_pivot, cmap='YlGnBu', annot=False)\n",
        "plt.title(\"Nifty 50 - Differenced Value (2015 - 2023)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_EcwY1a32otk"
      },
      "id": "_EcwY1a32otk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "87301401-178d-4ea0-bbee-39c9bc307ce7",
      "metadata": {
        "id": "87301401-178d-4ea0-bbee-39c9bc307ce7"
      },
      "source": [
        "**Stationary check post differencing**.\n",
        "\n",
        "The differenced data has approximately more or less constant mean when we apply windows.\n",
        "\n",
        "But, lets calculate the ADF value again to re-confirm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2b2938-94b4-46c4-ae21-4dcbcb9f93b1",
      "metadata": {
        "id": "0a2b2938-94b4-46c4-ae21-4dcbcb9f93b1"
      },
      "outputs": [],
      "source": [
        "# Perform the ADF test\n",
        "\n",
        "adf_res_mod = adfuller(df['Differenced'].dropna(), autolag='AIC')\n",
        "print(f\"ADF Statistic: {adf_res_mod[0]}\")\n",
        "print(f\"p-value: {adf_res_mod[1]}\")\n",
        "print(f\"#Lags Used: {adf_res_mod[2]}\")\n",
        "print(f\"Number of Observations Used: {adf_res_mod[3]}\")\n",
        "print(f\"Critical Values: {adf_res_mod[4]}\")\n",
        "\n",
        "if adf_res_mod[1] <= 0.05:\n",
        "    print(\"Reject the null hypothesis\")\n",
        "    print(\"The data is stationary\")\n",
        "\n",
        "else:\n",
        "    print(\"The null hypothesis cannot be rejected\")\n",
        "    print(\"The data is not stationary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2009809e-7916-4631-85d3-493c8c34c3d5",
      "metadata": {
        "id": "2009809e-7916-4631-85d3-493c8c34c3d5"
      },
      "source": [
        "The p-value obtained is less than the significance level of 0.05, and the ADF statistic is less than any of the critical values.\n",
        "we reject the Null hypothesis in favor of the alternative. So, the time series is, in fact, stationary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram** representation of actual data and its differenced form.\n",
        "Differenced data is approximately following Guassian Distribution\n",
        "\n",
        "Variance plot as well for the differenced and log differenced data. Is the differenced data stationary? Though the differenced data follows Gaussian distribution, we see changes in the variance plot.\n"
      ],
      "metadata": {
        "id": "3R2N9r4xQfS0"
      },
      "id": "3R2N9r4xQfS0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e53db3-739c-45c3-9637-6eac8b94c71c",
      "metadata": {
        "id": "50e53db3-739c-45c3-9637-6eac8b94c71c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "## Histogram of the Original data\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "final_data['Close'].dropna().hist()\n",
        "plt.title(\"Original Data\")\n",
        "\n",
        "## Histogram of the Differenced data\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "df['Differenced'].dropna().hist()\n",
        "plt.title(\"Differenced Data\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now lets check how is the variance. Is the differenced data stationary or not\n",
        "plt.figure(figsize=(12, 6))\n",
        "# add the 20 day rolling standard deviation:\n",
        "df['Differenced'].dropna().rolling(window=20).std().plot(style='r')\n",
        "df['Log_Differenced'].dropna().rolling(window=20).std().plot(style='b')\n",
        "# So the Variance changing over time.\n"
      ],
      "metadata": {
        "id": "eKcKArZ5Vkw7"
      },
      "id": "eKcKArZ5Vkw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "50b84f06-2271-44c1-9aee-0a826b6ec9b7",
      "metadata": {
        "id": "50b84f06-2271-44c1-9aee-0a826b6ec9b7"
      },
      "source": [
        "**ACF and PACF** Plots\n",
        "\n",
        "Lets check ACF and PACF details of the differenced data for 20 lags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678080dd-b4cb-490a-a140-856911740623",
      "metadata": {
        "id": "678080dd-b4cb-490a-a140-856911740623"
      },
      "outputs": [],
      "source": [
        "# ACF and PACF of the differenced series\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plot_acf(df['Differenced'].dropna(), lags=20)\n",
        "\n",
        "plot_pacf(df['Differenced'].dropna(), lags=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the ACF and PACF plots, it shows complete white noise which need to be modelled.**\n",
        "\n",
        "Let's check the PSD of the data to understand the frequency domain details of its respective time domain."
      ],
      "metadata": {
        "id": "G2PBYeTeMyt0"
      },
      "id": "G2PBYeTeMyt0"
    },
    {
      "cell_type": "markdown",
      "id": "414eadb5-1c04-459d-8fe2-30eb0e21a3cc",
      "metadata": {
        "id": "414eadb5-1c04-459d-8fe2-30eb0e21a3cc"
      },
      "source": [
        "**Power Spectral Density** - **Welch Method** using **Hann** window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89b9149b-6928-4d51-8774-70c30589b499",
      "metadata": {
        "id": "89b9149b-6928-4d51-8774-70c30589b499"
      },
      "outputs": [],
      "source": [
        "# PSD of the Differenced timeseries\n",
        "f,PSD = signal.welch(df['Close'],fs=len(df['Differenced'].dropna()), window='hann')\n",
        "plt.semilogy(f,PSD)\n",
        "plt.show()\n",
        "\n",
        "# PSD shows varying frequency details"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35ff1f1-82b0-46cf-933e-7c88131a01a6",
      "metadata": {
        "id": "f35ff1f1-82b0-46cf-933e-7c88131a01a6"
      },
      "source": [
        "Get the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train and test sets\n",
        "X = df['Close'].dropna().values\n",
        "Y = df_test['Close'].dropna().values\n",
        "train, test = X, Y"
      ],
      "metadata": {
        "id": "KA7Y_iVezty-"
      },
      "id": "KA7Y_iVezty-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb42b205-3949-4a11-989c-00708fff60dd",
      "metadata": {
        "id": "eb42b205-3949-4a11-989c-00708fff60dd"
      },
      "source": [
        "**Random walk with drift**\n",
        "\n",
        "The ACF and PACF happens to show pure white noise.\n",
        "\n",
        "If after differencing your time series, both the ACF and PACF plots indicate only white noise, then the most likely model to identify is a simple ARIMA(0,1,0) model, which essentially represents a \"random walk with drift\" where the only significant component is the first difference of the data, meaning the change between consecutive observations is essentially random noise\n",
        "\n",
        "ARIMA(0,1,0):\n",
        "\"0\" for AR: No autoregressive component (no dependence on past values).\n",
        "\"1\" for I: One order of differencing is required to achieve stationarity.\n",
        "\"0\" for MA: No moving average component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2da6b5-cce2-4ff4-837c-88b6ef6a3dc5",
      "metadata": {
        "id": "fe2da6b5-cce2-4ff4-837c-88b6ef6a3dc5"
      },
      "outputs": [],
      "source": [
        "# Fit ARMA(0,1,0) model\n",
        "model = ARIMA(df['Close'].dropna(), order=(0,1,0), freq = \"D\")\n",
        "fitted_model = model.fit()\n",
        "print(fitted_model.summary())\n",
        "step = len(df_test['Close'].dropna())\n",
        "\n",
        "# Forecast next length of the test data steps\n",
        "forecast = fitted_model.get_forecast(steps=step)\n",
        "forecast_summary = forecast.summary_frame()\n",
        "\n",
        "residuals = pd.DataFrame(fitted_model.resid, columns=[\"Values\"])\n",
        "# summary stats of residuals\n",
        "print(residuals.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot** the Residuals of Random Walk with Drift\n",
        "\n",
        "One of the most important steps in ARIMA modeling is to check if the residual series that is generated is **stationary**. Residuals are the difference between observed values and those produced by a model. It help us to determine whether a linear model is appropriate in modeling the given data"
      ],
      "metadata": {
        "id": "73pBlcxaXuP8"
      },
      "id": "73pBlcxaXuP8"
    },
    {
      "cell_type": "code",
      "source": [
        "# line plot of residuals\n",
        "residuals.plot()\n",
        "plt.title(\"Line Plot of Residuals.\")\n",
        "\n",
        "# density plot of residuals\n",
        "residuals.plot(kind='kde')\n",
        "plt.title(\"Density Plot of Residuals.\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "--NeOhuM1CHv"
      },
      "id": "--NeOhuM1CHv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_vals = fitted_model.forecast(steps=step)\n",
        "\n",
        "# Set forecast index to a date range for 2024\n",
        "forecast_summary.index = pd.date_range(start='2024-01-01', periods=step, freq='D')\n",
        "\n",
        "# Plot the forecasted values against the actual test data\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(df['Close'].dropna().index, df['Close'], label='Training Data', color='blue')\n",
        "plt.plot(df_test['Close'].index, df_test['Close'], label='Actual 2024 Data', color='red')\n",
        "plt.plot(forecast_summary.index, forecast_vals, label='Forecasted 2024 Data', color='green', linestyle='--')\n",
        "\n",
        "#plt.plot(np.arange(len(df['Close']), len(df['Close']) + 300), forecast.predicted_mean, label=\"Forecast\", color='orange')\n",
        "\n",
        "plt.title(\"ARMA(0,1,0) Model Forecast\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Zyf-mdw90ARU"
      },
      "id": "Zyf-mdw90ARU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8c7c1848-0f9b-46a1-971d-04a8763a62f7",
      "metadata": {
        "id": "8c7c1848-0f9b-46a1-971d-04a8763a62f7"
      },
      "source": [
        "Checking MSE, RMSE, R^2, and MAPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd490e3-b73c-49a7-b1a8-97c3ecc1bed3",
      "metadata": {
        "id": "cdd490e3-b73c-49a7-b1a8-97c3ecc1bed3"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model using error metrics\n",
        "# Ensure both series have the same length and index before calculating R^2\n",
        "\n",
        "# Reset index of df_test['Close'] to match forecast_vals index\n",
        "y_true = df_test['Close'].dropna().reset_index(drop=True)\n",
        "y_pred = pd.Series(forecast_vals).reset_index(drop=True)\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "# Calculate R^2 using the aligned series\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(f'R-squared (R^2): {r2}')\n",
        "\n",
        "# Calculate MAPE\n",
        "masked_actual = np.ma.masked_array(y_true, mask=y_true==0) #Mask values where y_true is 0 to avoid division by zero.\n",
        "MAPE = (np.fabs(masked_actual.filled(0) - y_pred.to_numpy())/masked_actual.filled(1)).mean() #Convert masked_actual and y_pred to NumPy arrays for compatibility.\n",
        "\n",
        "print(f'MAPE: {MAPE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b866f1-cd52-4817-ba26-1acc7a3a379e",
      "metadata": {
        "id": "a4b866f1-cd52-4817-ba26-1acc7a3a379e"
      },
      "source": [
        "Lets choose an appropriate model by identifying AIC values for a set of models that include variations of AR, MA, ARMA, and ARIMA\n",
        "\n",
        "Checking AIC Values for various models. Since auto_arima is ignoring a few models, we are manually geenrating AIC for sample models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1700ffe-941a-430a-a04d-0dc053e582b7"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
        "\n",
        "# Create a list to store the models and their AIC values\n",
        "models = []\n",
        "\n",
        "# Iterate through p, d, and q values up to the specified limits\n",
        "for p in range(4):  # Iterate up to p=3\n",
        "    for d in range(3): # Iterate up to d=2\n",
        "        for q in range(4): # Iterate up to q=3\n",
        "            try:\n",
        "                model = ARIMA(df['Close'].dropna(), order=(p,d,q))\n",
        "                results = model.fit()\n",
        "                models.append((p, d, q, results.aic))\n",
        "                print(f\"ARIMA({p},{d},{q}): AIC={results.aic}\") # Print the result of each iteration\n",
        "            except Exception as e:\n",
        "                print(f\"Error fitting ARIMA({p},{d},{q}): {e}\") # print the exception if there is one\n",
        "\n",
        "# Find the model with the lowest AIC\n",
        "best_model = min(models, key=lambda x: x[3])\n",
        "\n",
        "print(f\"Best Model: ARIMA({best_model[0]},{best_model[1]},{best_model[2]}) with AIC={best_model[3]}\")\n"
      ],
      "id": "b1700ffe-941a-430a-a04d-0dc053e582b7"
    },
    {
      "cell_type": "markdown",
      "id": "aaf7f347-0326-480f-b082-4bce39515555",
      "metadata": {
        "id": "aaf7f347-0326-480f-b082-4bce39515555"
      },
      "source": [
        "**ARIMA Modelling**:\n",
        "\n",
        "As observed AIC value is less for ARIMA Order (ignoring seasonality) -  (1 2 2) - 36137.73064054128.\n",
        "So lets try to fit 1,2,2 and check the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690615e1-b832-42a7-8d96-93c841d46311",
      "metadata": {
        "id": "690615e1-b832-42a7-8d96-93c841d46311"
      },
      "outputs": [],
      "source": [
        "# Forecast next steps based on the length of the forecast\n",
        "train_data = df['Close'].dropna()\n",
        "test_data = df_test['Close']\n",
        "forecast_steps = len(test_data)\n",
        "\n",
        "model = ARIMA(train_data, order=(best_model[0],best_model[1],best_model[2]),freq=\"D\")\n",
        "fitted_model = model.fit()\n",
        "forecast = fitted_model.get_forecast(steps=forecast_steps,freq=\"D\")\n",
        "forecast_summary = forecast.summary_frame()\n",
        "\n",
        "forecast_vals = fitted_model.forecast(steps=forecast_steps)\n",
        "\n",
        "# Set forecast index to a date range for 2024\n",
        "forecast_summary.index = pd.date_range(start='2024-01-01', periods=forecast_steps, freq='D')\n",
        "\n",
        "# Plot the forecasted values against the actual test data\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "# plt.plot(train_data.index, train_data, label='Training Data', color='blue')\n",
        "plt.plot(test_data.index, test_data, label='Actual 2024 Data', color='red')\n",
        "plt.plot(forecast_summary.index, forecast_vals, label='Forecasted 2024 Data', color='green', linestyle='--')\n",
        "plt.title('NSE Data: Actual vs Forecasted for 2024')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE, RMSE, R^2 and MAPE**"
      ],
      "metadata": {
        "id": "pCVkp0I9BmAH"
      },
      "id": "pCVkp0I9BmAH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d4af2d-dc24-44cb-a829-499de1b8b7bc",
      "metadata": {
        "id": "45d4af2d-dc24-44cb-a829-499de1b8b7bc"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model using error metrics\n",
        "# Ensure both series have the same length and index before calculating R^2\n",
        "\n",
        "# Reset index of df_test['Close'] to match forecast_vals index\n",
        "y_true = df_test['Close'].dropna().reset_index(drop=True)\n",
        "y_pred = pd.Series(forecast_vals).reset_index(drop=True)\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "# Calculate R^2 using the aligned series\n",
        "\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(f'R-squared (R^2): {r2}')\n",
        "\n",
        "# Calculate MAPE\n",
        "masked_actual = np.ma.masked_array(y_true, mask=y_true==0) #Mask values where y_true is 0 to avoid division by zero.\n",
        "MAPE = (np.fabs(masked_actual.filled(0) - y_pred.to_numpy())/masked_actual.filled(1)).mean() #Convert masked_actual and y_pred to NumPy arrays for compatibility.\n",
        "\n",
        "print(f'MAPE: {MAPE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying **SARIMAX** -\n",
        "As the ARIMA has forecasted the trend accurately but the seasonality component is missing, checking the minimum AIC value for various model orders and seasonal orders."
      ],
      "metadata": {
        "id": "TwBJGekyBbyB"
      },
      "id": "TwBJGekyBbyB"
    },
    {
      "cell_type": "code",
      "source": [
        "model = pm.auto_arima(train_data, start_p=1, start_q=1,\n",
        "                      test='adf',       # use adftest to find optimal 'd'\n",
        "                      max_p=3, max_q=3, # maximum p and q\n",
        "                      m=7,              # frequency of series (weekly)\n",
        "                      d=2,           # let model determine 'd'\n",
        "                      seasonal=True,   # Seasonality\n",
        "                      start_P=0,\n",
        "                      D=1,\n",
        "                      trace=True,\n",
        "                      error_action='ignore',\n",
        "                      suppress_warnings=True,\n",
        "                      stepwise=True)\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "Yxn9C8d2qzYA"
      },
      "id": "Yxn9C8d2qzYA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per Auto ARIMA ***among the given set of models***, with seasonality, the best model reported was **SARIMAX(3,2,0)(2,1,0)[7]**\n",
        "Let's try to model the data using this order and seasonality."
      ],
      "metadata": {
        "id": "4tDVQ-b_AYGH"
      },
      "id": "4tDVQ-b_AYGH"
    },
    {
      "cell_type": "code",
      "source": [
        "seasonal_order = (2, 1, 0, 7)  # (seasonality for daily data)\n",
        "model = SARIMAX(df['Close'].dropna(), order=(3, 2, 0), seasonal_order=seasonal_order)\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Forecast the data for 2024 (forecasting steps = length of test data)\n",
        "forecast_steps = len(df_test['Close'].dropna())\n",
        "forecast_Values = model_fit.forecast(steps=forecast_steps)\n",
        "\n",
        "# Plot the forecasted values against the actual test data\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "#plt.plot(df.index, train_data, label='Training Data', color='blue')\n",
        "plt.plot(df_test.index, test_data, label='Actual 2024 Data', color='red')\n",
        "plt.plot(forecast_summary.index, forecast_Values, label='Forecasted 2024 Data', color='green', linestyle='--')\n",
        "plt.title('NSE Data: Actual vs Forecasted for 2024')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8enyUioOAalL"
      },
      "id": "8enyUioOAalL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MSE, RMSE, R^2, MAPE**"
      ],
      "metadata": {
        "id": "SFOEJ_Reul6X"
      },
      "id": "SFOEJ_Reul6X"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using error metrics\n",
        "# Ensure both series have the same length and index before calculating R^2\n",
        "\n",
        "# Reset index of df_test['Close'] to match forecast_vals index\n",
        "y_true = df_test['Close'].dropna().reset_index(drop=True)\n",
        "y_pred = pd.Series(forecast_vals).reset_index(drop=True)\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "# Calculate R^2 using the aligned series\n",
        "\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(f'R-squared (R^2): {r2}')\n",
        "\n",
        "# Calculate MAPE\n",
        "masked_actual = np.ma.masked_array(y_true, mask=y_true==0) #Mask values where y_true is 0 to avoid division by zero.\n",
        "MAPE = (np.fabs(masked_actual.filled(0) - y_pred.to_numpy())/masked_actual.filled(1)).mean() #Convert masked_actual and y_pred to NumPy arrays for compatibility.\n",
        "\n",
        "print(f'MAPE: {MAPE}')"
      ],
      "metadata": {
        "id": "_znr_uSHueQe"
      },
      "id": "_znr_uSHueQe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ebb5de69-2d8f-407c-80fc-26ef129e3c84",
      "metadata": {
        "id": "ebb5de69-2d8f-407c-80fc-26ef129e3c84"
      },
      "source": [
        "**Why Non-Linear model??**\n",
        "\n",
        "Though we differenced the data and assumed the process to be stationary, as we have seen that there is still varying component of the Variance with absolute time. This indicates **Heteroscedasticity**. So, lets try Non-linear model to fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ARCH** is a statistical model used to **analyze and forecast the *volatility* of time series data**, particularly in financial markets. It's based on the idea that the variance of a time series is not constant but rather depends on past values of the series and its own past variances.\n",
        "\n",
        "1. The time series data should be stationary before applying ARCH. Hence, modelling the differenced output.\n",
        "2. p - The model order of the ARCH determines how many past squared errors need to be included."
      ],
      "metadata": {
        "id": "nLO_Kj6L3Dz6"
      },
      "id": "nLO_Kj6L3Dz6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot** the ARCH model fit Vs Train and Test data."
      ],
      "metadata": {
        "id": "0-lPScIlygqc"
      },
      "id": "0-lPScIlygqc"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data = df['Close'],df_test['Close']\n",
        "\n",
        "# Fit a simple ARCH model to get residuals\n",
        "basic_arch_model = arch_model(train_data, vol='ARCH', p=1).fit(disp='off')\n",
        "residuals = basic_arch_model.resid\n",
        "\n",
        "# Check for normality (e.g., using Shapiro-Wilk test)\n",
        "shapiro_test = stats.shapiro(residuals)\n",
        "print(f\"Shapiro-Wilk Test p-value: {shapiro_test.pvalue}\")\n",
        "\n",
        "# If not normal, consider Box-Cox transformation\n",
        "if shapiro_test.pvalue < 0.05:\n",
        "    transformed_data, lambda_value = boxcox(train_data)  # Apply Box-Cox\n",
        "    print(f\"Box-Cox Transformation Lambda: {lambda_value}\")\n",
        "    # Proceed with modeling using 'transformed_data'\n",
        "else:\n",
        "    transformed_data = train_data  # No transformation needed\n",
        "\n"
      ],
      "metadata": {
        "id": "DSRMXqjhKzl4"
      },
      "id": "DSRMXqjhKzl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
        "\n",
        "# Define parameter grid for 'p' (and potentially other parameters)\n",
        "param_grid = {'p': [1, 2, 3, 4, 5]}\n",
        "\n",
        "# Use TimeSeriesSplit for cross-validation with time series data\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Iterate through models and evaluate\n",
        "best_model = None\n",
        "best_score = float('inf')  # Initialize with a large value\n",
        "\n",
        "for p_value in param_grid['p']:\n",
        "    model = arch_model(transformed_data, vol='ARCH', p=p_value)\n",
        "    scores = []\n",
        "    for train_index, val_index in tscv.split(transformed_data):\n",
        "        train_fold, val_fold = transformed_data[train_index], transformed_data[val_index]\n",
        "        fold_model = model.fit(disp='off', last_obs=len(train_fold))  # Fit on train fold\n",
        "        forecast = fold_model.forecast(horizon=len(val_fold), reindex=False)\n",
        "        # Calculate a score (e.g., RMSE) on the validation fold\n",
        "        score = np.sqrt(mean_squared_error(val_fold, forecast.variance.values[-1, :]))\n",
        "        scores.append(score)\n",
        "    avg_score = np.mean(scores)\n",
        "\n",
        "    if avg_score < best_score:\n",
        "        best_score = avg_score\n",
        "        best_model = model\n",
        "        best_p = p_value\n",
        "\n",
        "print(f\"Best p value: {best_p}, Best Score (RMSE): {best_score}\")"
      ],
      "metadata": {
        "id": "Or2VP-10P7-5",
        "outputId": "a7194e15-e938-44c3-8e00-b0fe49284909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Or2VP-10P7-5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
            "Inequality constraints incompatible\n",
            "See scipy.optimize.fmin_slsqp for code meaning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
            "Inequality constraints incompatible\n",
            "See scipy.optimize.fmin_slsqp for code meaning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
            "Inequality constraints incompatible\n",
            "See scipy.optimize.fmin_slsqp for code meaning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
            "Inequality constraints incompatible\n",
            "See scipy.optimize.fmin_slsqp for code meaning.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/arch/univariate/base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
            "Inequality constraints incompatible\n",
            "See scipy.optimize.fmin_slsqp for code meaning.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a simple ARCH model to get residuals\n",
        "basic_arch_model = arch_model(transformed_data, vol='ARCH', p=4).fit(disp='off')\n",
        "# Fit the model\n",
        "model_fit = model.fit()\n",
        "\n",
        "# Print model summary\n",
        "print(model_fit.summary())\n",
        "\n",
        "# Forecast the next 10 days of volatility\n",
        "forecast = model_fit.forecast(horizon=10)\n",
        "\n",
        "# Extract the forecasted volatility (standard deviation)\n",
        "forecasted_volatility = forecast.variance[-1:] ** 0.5\n",
        "\n",
        "# Plot the forecasted volatility\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(forecasted_volatility.T)\n",
        "plt.title(f\"Forecasted Volatility for the Next 10 Days\")\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Volatility')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lDUzdn9hSaDA"
      },
      "id": "lDUzdn9hSaDA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GARCH(1, 1) model\n",
        "garch_model = arch_model(transformed_data, vol='GARCH', p=1, q=1).fit(disp='off')\n",
        "\n",
        "# EGARCH(1, 1) model\n",
        "egarch_model = arch_model(transformed_data, vol='EGARCH', p=1, o=1, q=1).fit(disp='off')\n",
        "\n",
        "# ... (compare performance using metrics and visualization)"
      ],
      "metadata": {
        "id": "JvaFzbGKP9cZ"
      },
      "id": "JvaFzbGKP9cZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate rolling window variance of test data\n",
        "window_size = 20  # Adjust as needed\n",
        "rolling_variance = pd.Series(test_data).rolling(window=window_size).var().dropna()\n",
        "\n",
        "# Get predicted volatility from the best model\n",
        "forecast = best_model.forecast(horizon=len(test_data), reindex=False)\n",
        "predicted_volatility = forecast.variance.values[-1, :]  # Extract volatility\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(rolling_variance.index, rolling_variance, label='Actual Rolling Variance', color='blue')\n",
        "plt.plot(rolling_variance.index, predicted_volatility[:len(rolling_variance)], label='Predicted Volatility', color='red', linestyle='--')\n",
        "plt.title('Predicted Volatility vs. Actual Rolling Variance')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n1KMCe3MQCFM"
      },
      "id": "n1KMCe3MQCFM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
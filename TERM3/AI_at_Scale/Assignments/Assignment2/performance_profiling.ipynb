{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOobRIUZPQtLE4YG2fFjE92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TERM3/AI_at_Scale/Assignments/Assignment2/performance_profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import regexp_replace, col, countDistinct, col, sum as spark_sum\n",
        "from pyspark.sql.functions import count, when, lit, create_map\n",
        "from pyspark.sql.types import StringType, DoubleType, IntegerType ,StructType,StructField\n",
        "from pyspark.ml.feature import Imputer, VectorAssembler,StandardScaler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
        "import time\n",
        "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor\n",
        "from pyspark.ml import PipelineModel, Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "RqIJ77D6ttHw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "48GL-u-qtjoj"
      },
      "outputs": [],
      "source": [
        "def create_spark_session(executor_cores=\"1\", max_cores=\"2\", executor_memory=\"1g\"):\n",
        "    return SparkSession.builder \\\n",
        "        .appName(\"Assignment2_ch24m571\") \\\n",
        "        .config(\"spark.executor.cores\", executor_cores) \\\n",
        "        .config(\"spark.cores.max\", max_cores) \\\n",
        "        .config(\"spark.executor.memory\", executor_memory) \\\n",
        "        .config(\"spark.driver.memory\", \"2g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "def get_schema():\n",
        "    return StructType([\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"job\", StringType(), True),\n",
        "    StructField(\"marital\", StringType(), True),\n",
        "    StructField(\"education\", StringType(), True),\n",
        "    StructField(\"default\", StringType(), True),\n",
        "    StructField(\"balance\", DoubleType(), True),\n",
        "    StructField(\"housing\", StringType(), True),\n",
        "    StructField(\"loan\", StringType(), True),\n",
        "    StructField(\"contact\", StringType(), True),\n",
        "    StructField(\"day\", IntegerType(), True),\n",
        "    StructField(\"month\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True),\n",
        "    StructField(\"campaign\", IntegerType(), True),\n",
        "    StructField(\"pdays\", IntegerType(), True),\n",
        "    StructField(\"previous\", IntegerType(), True),\n",
        "    StructField(\"poutcome\", StringType(), True),\n",
        "    StructField(\"y\", StringType(), True)\n",
        "\n",
        "])\n",
        "\n",
        "def run_single_performance_profiling(config):\n",
        "    \"\"\"Runs performance profiling for a single Spark configuration.\"\"\"\n",
        "    print(f\"\\nConfiguration: {config}\")\n",
        "    spark = create_spark_session(config[\"executor_cores\"], config[\"max_cores\"], config[\"executor_memory\"])\n",
        "\n",
        "    if config[\"parallelism\"]:\n",
        "        spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", \"true\")\n",
        "        spark.conf.set(\"spark.default.parallelism\", config[\"parallelism\"])\n",
        "\n",
        "    times = {}\n",
        "    config_label = f\"Cores: {config['executor_cores']}, Max: {config['max_cores']}, Mem: {config['executor_memory']}\"\n",
        "    if config['parallelism']:\n",
        "        config_label += f\", Parallelism: {config['parallelism']}\"\n",
        "    config_results = {'Configuration': config_label}\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        data_path = \"bank-full.csv\"\n",
        "        schema = get_schema()\n",
        "        data = spark.read.csv(data_path, header=True, schema=schema, sep=';')\n",
        "        train_df, test_df = data.randomSplit([0.8, 0.2], seed=42)\n",
        "        times['Data Loading'] = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        config_results['Data Loading Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "        numerical_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "\n",
        "        indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "        encoders = [OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\") for col in categorical_cols]\n",
        "\n",
        "        feature_cols = numerical_cols + [f\"{col}_encoded\" for col in categorical_cols]\n",
        "\n",
        "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "        label_indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
        "\n",
        "        pipeline = Pipeline(stages=indexers + encoders + [assembler, label_indexer])\n",
        "        preproc_model = pipeline.fit(train_df)\n",
        "        train_df_processed = preproc_model.transform(train_df)\n",
        "        test_df_processed = preproc_model.transform(test_df)\n",
        "        times['Preprocessing'] = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        config_results['Preprocessing Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10, seed=42)\n",
        "        model = rf.fit(train_df_processed)\n",
        "        times['Model Training'] = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        config_results['Model Training Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        predictions = model.transform(test_df_processed)\n",
        "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "        accuracy = evaluator.evaluate(predictions)\n",
        "        times['Model Evaluation'] = time.time() - start_time\n",
        "        times['Accuracy'] = accuracy\n",
        "    except Exception as e:\n",
        "        config_results['Model Evaluation Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        total_time = 0.0\n",
        "        for k, v in times.items():\n",
        "            if k != 'Accuracy' and isinstance(v, (int, float)):\n",
        "                total_time += v\n",
        "        times['Total Time'] = total_time\n",
        "        config_results.update(times)\n",
        "    except Exception as e:\n",
        "        config_results['Total Time Calculation Error'] = str(e)\n",
        "\n",
        "    spark.stop()\n",
        "    return config_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration 1\n",
        "config1 = {\"executor_cores\": \"1\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": None}\n",
        "results1 = run_single_performance_profiling(config1)\n",
        "print(\"\\nResults for Configuration 1:\")\n",
        "print(pd.DataFrame([results1]).to_string(index=False))\n",
        "\n",
        "# Configuration 2\n",
        "config2 = {\"executor_cores\": \"2\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": None}\n",
        "results2 = run_single_performance_profiling(config2)\n",
        "print(\"\\nResults for Configuration 2:\")\n",
        "print(pd.DataFrame([results2]).to_string(index=False))\n",
        "\n",
        "# Configuration 3\n",
        "config3 = {\"executor_cores\": \"2\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": \"1\"}\n",
        "results3 = run_single_performance_profiling(config3)\n",
        "print(\"\\nResults for Configuration 3:\")\n",
        "print(pd.DataFrame([results3]).to_string(index=False))\n",
        "\n",
        "# Configuration 4\n",
        "config4 = {\"executor_cores\": \"2\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": \"2\"}\n",
        "results4 = run_single_performance_profiling(config4)\n",
        "print(\"\\nResults for Configuration 4:\")\n",
        "print(pd.DataFrame([results4]).to_string(index=False))"
      ],
      "metadata": {
        "id": "v5cAS6zWtrAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41674c49-01cb-4adf-905f-300ada3ceed5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration: {'executor_cores': '1', 'max_cores': '2', 'executor_memory': '1g', 'parallelism': None}\n",
            "\n",
            "Results for Configuration 1:\n",
            "            Configuration  Data Loading  Preprocessing  Model Training  Model Evaluation  Accuracy  Total Time\n",
            "Cores: 1, Max: 2, Mem: 1g      0.133385      27.449638       16.747763          3.863088  0.893589   48.193874\n",
            "\n",
            "Configuration: {'executor_cores': '2', 'max_cores': '2', 'executor_memory': '1g', 'parallelism': None}\n",
            "\n",
            "Results for Configuration 2:\n",
            "            Configuration  Data Loading  Preprocessing  Model Training  Model Evaluation  Accuracy  Total Time\n",
            "Cores: 2, Max: 2, Mem: 1g      0.090955      12.321197        8.740624          3.033906  0.893589   24.186681\n",
            "\n",
            "Configuration: {'executor_cores': '2', 'max_cores': '2', 'executor_memory': '1g', 'parallelism': '1'}\n",
            "\n",
            "Results for Configuration 3:\n",
            "                            Configuration  Data Loading  Preprocessing  Model Training  Model Evaluation  Accuracy  Total Time\n",
            "Cores: 2, Max: 2, Mem: 1g, Parallelism: 1      0.072748      10.330697        8.837097          1.652712  0.893589   20.893255\n",
            "\n",
            "Configuration: {'executor_cores': '2', 'max_cores': '2', 'executor_memory': '1g', 'parallelism': '2'}\n",
            "\n",
            "Results for Configuration 4:\n",
            "                            Configuration  Data Loading  Preprocessing  Model Training  Model Evaluation  Accuracy  Total Time\n",
            "Cores: 2, Max: 2, Mem: 1g, Parallelism: 2       0.05069      10.282457        8.682612          1.641304  0.893589   20.657063\n"
          ]
        }
      ]
    }
  ]
}
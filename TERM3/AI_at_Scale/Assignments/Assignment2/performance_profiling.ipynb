{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8OxbAJXTwc7oXz5vI4ggt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TERM3/AI_at_Scale/Assignments/Assignment2/performance_profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import regexp_replace, col, countDistinct, col, sum as spark_sum\n",
        "from pyspark.sql.functions import count, when, lit, create_map\n",
        "from pyspark.sql.types import StringType, DoubleType, IntegerType ,StructType,StructField\n",
        "from pyspark.ml.feature import Imputer, VectorAssembler,StandardScaler\n",
        "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
        "import time\n",
        "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor\n",
        "from pyspark.ml import PipelineModel"
      ],
      "metadata": {
        "id": "RqIJ77D6ttHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GL-u-qtjoj"
      },
      "outputs": [],
      "source": [
        "def run_single_performance_profiling(config):\n",
        "    \"\"\"Runs performance profiling for a single Spark configuration.\"\"\"\n",
        "    print(f\"\\nConfiguration: {config}\")\n",
        "    spark = create_spark_session(config[\"executor_cores\"], config[\"max_cores\"], config[\"executor_memory\"])\n",
        "\n",
        "    if config[\"parallelism\"]:\n",
        "        spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", \"true\")\n",
        "        spark.conf.set(\"spark.default.parallelism\", config[\"parallelism\"])\n",
        "\n",
        "    times = {}\n",
        "    config_label = f\"Cores: {config['executor_cores']}, Max: {config['max_cores']}, Mem: {config['executor_memory']}\"\n",
        "    if config['parallelism']:\n",
        "        config_label += f\", Parallelism: {config['parallelism']}\"\n",
        "    config_results = {'Configuration': config_label}\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        data_path = \"bank-full.csv\"\n",
        "        schema = get_schema()\n",
        "        data = spark.read.csv(data_path, header=True, schema=schema, sep=';')\n",
        "        train_df, test_df = data.randomSplit([0.8, 0.2], seed=42)\n",
        "        times['Data Loading'] = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        config_results['Data Loading Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "        numerical_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
        "\n",
        "        indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "        encoders = [OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\") for col in categorical_cols]\n",
        "\n",
        "        feature_cols = numerical_cols + [f\"{col}_encoded\" for col in categorical_cols]\n",
        "\n",
        "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "        label_indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
        "\n",
        "        pipeline = Pipeline(stages=indexers + encoders + [assembler, label_indexer])\n",
        "        preproc_model = pipeline.fit(train_df)\n",
        "        train_df_processed = preproc_model.transform(train_df)\n",
        "        test_df_processed = preproc_model.transform(test_df)\n",
        "        times['Preprocessing'] = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        config_results['Preprocessing Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10, seed=42)\n",
        "        model = rf.fit(train_df_processed)\n",
        "        times['Model Training'] = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        config_results['Model Training Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        predictions = model.transform(test_df_processed)\n",
        "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "        accuracy = evaluator.evaluate(predictions)\n",
        "        times['Model Evaluation'] = time.time() - start_time\n",
        "        times['Accuracy'] = accuracy\n",
        "    except Exception as e:\n",
        "        config_results['Model Evaluation Error'] = str(e)\n",
        "        spark.stop()\n",
        "        return config_results\n",
        "\n",
        "    try:\n",
        "        total_time = 0.0\n",
        "        for k, v in times.items():\n",
        "            if k != 'Accuracy' and isinstance(v, (int, float)):\n",
        "                total_time += v\n",
        "        times['Total Time'] = total_time\n",
        "        config_results.update(times)\n",
        "    except Exception as e:\n",
        "        config_results['Total Time Calculation Error'] = str(e)\n",
        "\n",
        "    spark.stop()\n",
        "    return config_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration 1\n",
        "config1 = {\"executor_cores\": \"1\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": None}\n",
        "results1 = run_single_performance_profiling(config1)\n",
        "print(\"\\nResults for Configuration 1:\")\n",
        "print(pd.DataFrame([results1]).to_string(index=False))\n",
        "\n",
        "# Configuration 2\n",
        "config2 = {\"executor_cores\": \"2\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": None}\n",
        "results2 = run_single_performance_profiling(config2)\n",
        "print(\"\\nResults for Configuration 2:\")\n",
        "print(pd.DataFrame([results2]).to_string(index=False))\n",
        "\n",
        "# Configuration 3\n",
        "config3 = {\"executor_cores\": \"2\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": \"1\"}\n",
        "results3 = run_single_performance_profiling(config3)\n",
        "print(\"\\nResults for Configuration 3:\")\n",
        "print(pd.DataFrame([results3]).to_string(index=False))\n",
        "\n",
        "# Configuration 4\n",
        "config4 = {\"executor_cores\": \"2\", \"max_cores\": \"2\", \"executor_memory\": \"1g\", \"parallelism\": \"2\"}\n",
        "results4 = run_single_performance_profiling(config4)\n",
        "print(\"\\nResults for Configuration 4:\")\n",
        "print(pd.DataFrame([results4]).to_string(index=False))"
      ],
      "metadata": {
        "id": "v5cAS6zWtrAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
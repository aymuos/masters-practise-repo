{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3rVHFV9sOTmH"},"outputs":[],"source":["# Import libraries\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, from_json, when, size, explode, expr\n","from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType, DoubleType, MapType\n","import time\n","\n","student_id = \"CH24M000\"  # your roll number, letters must be in capital letter\n","app_name = student_id + \"_Assignment_1\"\n","assignment_no = \"Assignment_1\"\n","\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .config(\"spark.executor.instances\", \"1\") \\\n","    .config(\"spark.executor.memory\", \"2G\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .config(\"spark.driver.memory\", \"3G\") \\\n","    .getOrCreate()\n","# File paths (unique for each student)\n","# base path to access the dataset from server. Uncomment when submitting the job to server to use the spark cluster\n","base_path = f\"/opt/spark/data/{student_id}/\"\n","\n","# base path to access the sample dataset from your google drive\n","# base_path = \"PATH\"\n","\n","\n","# Load CSV files\n","customers_df = spark.read.option(\"multiline\", True) \\\n","                         .option(\"escape\", \"\\\"\") \\\n","                         .option(\"quote\", \"\\\"\") \\\n","                         .option(\"header\", True) \\\n","                         .csv(base_path + \"customers.csv\")\n","products_df = spark.read.option(\"multiline\", True) \\\n","                         .option(\"escape\", \"\\\"\") \\\n","                         .option(\"quote\", \"\\\"\") \\\n","                         .option(\"header\", True) \\\n","                         .csv(base_path + \"products.csv\")\n","orders_df = spark.read.option(\"multiline\", True) \\\n","                         .option(\"escape\", \"\\\"\") \\\n","                         .option(\"quote\", \"\\\"\") \\\n","                         .option(\"header\", True) \\\n","                         .csv(base_path + \"orders.csv\")\n","\n"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"CH24M000\").getOrCreate()\n","input_path = \"/opt/spark/data/CH24M000/Assignment_1/data.csv\"\n","df = spark.read.option(\"header\", \"true\").csv(input_path)\n","df.show()\n"],"metadata":{"id":"YN0LDw_lvRR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import pprint\n","\n","# Read code from the file\n","with open(\"CH24M000.py\", \"r\") as file:\n","    code_content = file.read()\n","\n","url = \"https://lab.samsai.io/submit\"\n","\n","# DO NOT MODIFY\n","headers = {\n","    \"Authorization\": \"Bearer f9318293d4e405e5cff5d03a348a02ae0c4331916cd390041a700045d1bcb16a\",\n","    \"Content-Type\": \"application/json\"\n","}\n","payload = {\n","    \"student_id\": \"CH24M000\",\n","    \"code\": code_content\n","}\n","\n","response = requests.post(url, headers=headers, json=payload)\n","\n","print(\"Status Code:\", response.status_code)\n","print(\"Response Body:\", response.text)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SjpAouhvi45","executionInfo":{"status":"ok","timestamp":1749037456063,"user_tz":-330,"elapsed":878,"user":{"displayName":"Jashaswimalya Acharjee da23d403","userId":"09583076657516930178"}},"outputId":"c3650c9c-886f-46c7-bcc0-1c87abcf1b79"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Status Code: 200\n","Response Body: {\"command\":\"docker exec -i spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=4g --conf spark.driver.memory=2g --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.coalescePartitions.enabled=true /opt/spark/code/7834b89d-a091-456c-ad47-354fa80e3c0f.py\",\"dag_url\":\"https://lab.samsai.io/spark-ui\",\"history_url\":\"https://lab.samsai.io/history\",\"job_id\":\"7834b89d-a091-456c-ad47-354fa80e3c0f\",\"log_url\":\"https://lab.samsai.io/logs/7834b89d-a091-456c-ad47-354fa80e3c0f.log\",\"script_path\":\"/opt/spark/code/7834b89d-a091-456c-ad47-354fa80e3c0f.py\",\"status\":\"submitted\",\"student_id\":\"CH24M000\"}\n","\n"]}]},{"cell_type":"markdown","source":["Status Code: 200\n","Response Body: {\"command\":\"docker exec -i spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=4g --conf spark.driver.memory=2g --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.coalescePartitions.enabled=true /opt/spark/code/96d07c81-c0a8-4f99-b5cc-83f8b84fed2f.py\",\"dag_url\":\"https://lab.samsai.io/spark-ui\",\"history_url\":\"https://lab.samsai.io/history\",\"job_id\":\"96d07c81-c0a8-4f99-b5cc-83f8b84fed2f\",\"log_url\":\"https://lab.samsai.io/logs/96d07c81-c0a8-4f99-b5cc-83f8b84fed2f.log\",\"script_path\":\"/opt/spark/code/96d07c81-c0a8-4f99-b5cc-83f8b84fed2f.py\",\"status\":\"submitted\",\"student_id\":\"CH24M000\"}"],"metadata":{"id":"ll841lH9wBAD"}},{"cell_type":"code","source":["!curl https://lab.samsai.io/logs/96d07c81-c0a8-4f99-b5cc-83f8b84fed2f.log"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWNe0OWmvucw","executionInfo":{"status":"ok","timestamp":1749037391931,"user_tz":-330,"elapsed":1376,"user":{"displayName":"Jashaswimalya Acharjee da23d403","userId":"09583076657516930178"}},"outputId":"228072df-f7f0-4281-ae77-7aff3e49cc29"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Command: docker exec -i spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --conf spark.executor.memory=4g --conf spark.driver.memory=2g --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.coalescePartitions.enabled=true /opt/spark/code/96d07c81-c0a8-4f99-b5cc-83f8b84fed2f.py\n","==================================================\n","25/06/04 11:42:31 INFO SparkContext: Running Spark version 3.4.0\n","25/06/04 11:42:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","25/06/04 11:42:31 INFO ResourceUtils: ==============================================================\n","25/06/04 11:42:31 INFO ResourceUtils: No custom resources configured for spark.driver.\n","25/06/04 11:42:31 INFO ResourceUtils: ==============================================================\n","25/06/04 11:42:31 INFO SparkContext: Submitted application: CH24M000\n","25/06/04 11:42:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n","25/06/04 11:42:31 INFO ResourceProfile: Limiting resource is cpu\n","25/06/04 11:42:31 INFO ResourceProfileManager: Added ResourceProfile id: 0\n","25/06/04 11:42:31 INFO SecurityManager: Changing view acls to: root\n","25/06/04 11:42:31 INFO SecurityManager: Changing modify acls to: root\n","25/06/04 11:42:31 INFO SecurityManager: Changing view acls groups to: \n","25/06/04 11:42:31 INFO SecurityManager: Changing modify acls groups to: \n","25/06/04 11:42:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n","25/06/04 11:42:31 INFO Utils: Successfully started service 'sparkDriver' on port 41429.\n","25/06/04 11:42:31 INFO SparkEnv: Registering MapOutputTracker\n","25/06/04 11:42:31 INFO SparkEnv: Registering BlockManagerMaster\n","25/06/04 11:42:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n","25/06/04 11:42:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n","25/06/04 11:42:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/06/04 11:42:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a276267b-1503-4357-8868-1e3f9687911d\n","25/06/04 11:42:31 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB\n","25/06/04 11:42:31 INFO SparkEnv: Registering OutputCommitCoordinator\n","25/06/04 11:42:31 INFO JettyUtils: Start Jetty spark-master:4040 for SparkUI\n","25/06/04 11:42:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n","25/06/04 11:42:32 INFO TransportClientFactory: Successfully created connection to spark-master/192.168.0.4:7077 after 17 ms (0 ms spent in bootstraps)\n","25/06/04 11:42:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250604114232-0010\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250604114232-0010/0 on worker-20250604073635-192.168.0.6-8083 (192.168.0.6:8083) with 6 core(s)\n","25/06/04 11:42:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250604114232-0010/0 on hostPort 192.168.0.6:8083 with 6 core(s), 4.0 GiB RAM\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250604114232-0010/1 on worker-20250604073634-192.168.0.5-8082 (192.168.0.5:8082) with 6 core(s)\n","25/06/04 11:42:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250604114232-0010/1 on hostPort 192.168.0.5:8082 with 6 core(s), 4.0 GiB RAM\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250604114232-0010/2 on worker-20250604073635-192.168.0.8-8084 (192.168.0.8:8084) with 6 core(s)\n","25/06/04 11:42:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250604114232-0010/2 on hostPort 192.168.0.8:8084 with 6 core(s), 4.0 GiB RAM\n","25/06/04 11:42:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37043.\n","25/06/04 11:42:32 INFO NettyBlockTransferService: Server created on c66e281f0341:37043\n","25/06/04 11:42:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n","25/06/04 11:42:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c66e281f0341, 37043, None)\n","25/06/04 11:42:32 INFO BlockManagerMasterEndpoint: Registering block manager c66e281f0341:37043 with 1048.8 MiB RAM, BlockManagerId(driver, c66e281f0341, 37043, None)\n","25/06/04 11:42:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c66e281f0341, 37043, None)\n","25/06/04 11:42:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c66e281f0341, 37043, None)\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250604114232-0010/2 is now RUNNING\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250604114232-0010/0 is now RUNNING\n","25/06/04 11:42:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250604114232-0010/1 is now RUNNING\n","25/06/04 11:42:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n","25/06/04 11:42:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n","25/06/04 11:42:32 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.\n","25/06/04 11:42:33 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.\n","25/06/04 11:42:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n","25/06/04 11:42:33 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.5:37782) with ID 1,  ResourceProfileId 0\n","25/06/04 11:42:33 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.8:39914) with ID 2,  ResourceProfileId 0\n","25/06/04 11:42:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.5:37797 with 2.2 GiB RAM, BlockManagerId(1, 192.168.0.5, 37797, None)\n","25/06/04 11:42:33 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.6:46158) with ID 0,  ResourceProfileId 0\n","25/06/04 11:42:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.8:38027 with 2.2 GiB RAM, BlockManagerId(2, 192.168.0.8, 38027, None)\n","25/06/04 11:42:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.6:46597 with 2.2 GiB RAM, BlockManagerId(0, 192.168.0.6, 46597, None)\n","25/06/04 11:42:34 INFO FileSourceStrategy: Pushed Filters: \n","25/06/04 11:42:34 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n","25/06/04 11:42:34 INFO CodeGenerator: Code generated in 89.16547 ms\n","25/06/04 11:42:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 1048.6 MiB)\n","25/06/04 11:42:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 1048.6 MiB)\n","25/06/04 11:42:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c66e281f0341:37043 (size: 34.3 KiB, free: 1048.8 MiB)\n","25/06/04 11:42:34 INFO SparkContext: Created broadcast 0 from csv at <unknown>:0\n","25/06/04 11:42:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n","25/06/04 11:42:34 INFO SparkContext: Starting job: csv at <unknown>:0\n","25/06/04 11:42:34 INFO DAGScheduler: Got job 0 (csv at <unknown>:0) with 1 output partitions\n","25/06/04 11:42:34 INFO DAGScheduler: Final stage: ResultStage 0 (csv at <unknown>:0)\n","25/06/04 11:42:34 INFO DAGScheduler: Parents of final stage: List()\n","25/06/04 11:42:34 INFO DAGScheduler: Missing parents: List()\n","25/06/04 11:42:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0), which has no missing parents\n","25/06/04 11:42:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 1048.6 MiB)\n","25/06/04 11:42:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 1048.6 MiB)\n","25/06/04 11:42:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c66e281f0341:37043 (size: 6.0 KiB, free: 1048.8 MiB)\n","25/06/04 11:42:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n","25/06/04 11:42:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n","25/06/04 11:42:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n","25/06/04 11:42:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.8, executor 2, partition 0, PROCESS_LOCAL, 7937 bytes) \n","25/06/04 11:42:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.8:38027 (size: 6.0 KiB, free: 2.2 GiB)\n","25/06/04 11:42:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.8:38027 (size: 34.3 KiB, free: 2.2 GiB)\n","25/06/04 11:42:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 594 ms on 192.168.0.8 (executor 2) (1/1)\n","25/06/04 11:42:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n","25/06/04 11:42:35 INFO DAGScheduler: ResultStage 0 (csv at <unknown>:0) finished in 0.654 s\n","25/06/04 11:42:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n","25/06/04 11:42:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n","25/06/04 11:42:35 INFO DAGScheduler: Job 0 finished: csv at <unknown>:0, took 0.676658 s\n","25/06/04 11:42:35 INFO CodeGenerator: Code generated in 6.99086 ms\n","25/06/04 11:42:35 INFO FileSourceStrategy: Pushed Filters: \n","25/06/04 11:42:35 INFO FileSourceStrategy: Post-Scan Filters: \n","25/06/04 11:42:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 1048.4 MiB)\n","25/06/04 11:42:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 1048.3 MiB)\n","25/06/04 11:42:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c66e281f0341:37043 (size: 34.3 KiB, free: 1048.7 MiB)\n","25/06/04 11:42:35 INFO SparkContext: Created broadcast 2 from csv at <unknown>:0\n","25/06/04 11:42:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n","25/06/04 11:42:35 INFO FileSourceStrategy: Pushed Filters: \n","25/06/04 11:42:35 INFO FileSourceStrategy: Post-Scan Filters: \n","25/06/04 11:42:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 KiB, free 1048.1 MiB)\n","25/06/04 11:42:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 1048.1 MiB)\n","25/06/04 11:42:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c66e281f0341:37043 (size: 34.3 KiB, free: 1048.7 MiB)\n","25/06/04 11:42:35 INFO SparkContext: Created broadcast 3 from showString at <unknown>:0\n","25/06/04 11:42:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n","25/06/04 11:42:35 INFO SparkContext: Starting job: showString at <unknown>:0\n","25/06/04 11:42:35 INFO DAGScheduler: Got job 1 (showString at <unknown>:0) with 1 output partitions\n","25/06/04 11:42:35 INFO DAGScheduler: Final stage: ResultStage 1 (showString at <unknown>:0)\n","25/06/04 11:42:35 INFO DAGScheduler: Parents of final stage: List()\n","25/06/04 11:42:35 INFO DAGScheduler: Missing parents: List()\n","25/06/04 11:42:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at showString at <unknown>:0), which has no missing parents\n","25/06/04 11:42:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.8 KiB, free 1048.1 MiB)\n","25/06/04 11:42:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 1048.1 MiB)\n","25/06/04 11:42:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c66e281f0341:37043 (size: 5.9 KiB, free: 1048.7 MiB)\n","25/06/04 11:42:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n","25/06/04 11:42:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n","25/06/04 11:42:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n","25/06/04 11:42:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.5, executor 1, partition 0, PROCESS_LOCAL, 7937 bytes) \n","25/06/04 11:42:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.5:37797 (size: 5.9 KiB, free: 2.2 GiB)\n","25/06/04 11:42:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.5:37797 (size: 34.3 KiB, free: 2.2 GiB)\n","25/06/04 11:42:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 603 ms on 192.168.0.5 (executor 1) (1/1)\n","25/06/04 11:42:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n","25/06/04 11:42:36 INFO DAGScheduler: ResultStage 1 (showString at <unknown>:0) finished in 0.609 s\n","25/06/04 11:42:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n","25/06/04 11:42:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n","25/06/04 11:42:36 INFO DAGScheduler: Job 1 finished: showString at <unknown>:0, took 0.612046 s\n","25/06/04 11:42:36 INFO CodeGenerator: Code generated in 8.17693 ms\n","+-----+-------+----+\n","| name| rollno| age|\n","+-----+-------+----+\n","|alice|    234|  22|\n","|david|    345|  21|\n","+-----+-------+----+\n","\n","25/06/04 11:42:36 INFO SparkContext: Invoking stop() from shutdown hook\n","25/06/04 11:42:36 INFO SparkContext: SparkContext is stopping with exitCode 0.\n","25/06/04 11:42:36 INFO SparkUI: Stopped Spark web UI at http://c66e281f0341:4040\n","25/06/04 11:42:36 INFO StandaloneSchedulerBackend: Shutting down all executors\n","25/06/04 11:42:36 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n","25/06/04 11:42:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n","25/06/04 11:42:36 INFO MemoryStore: MemoryStore cleared\n","25/06/04 11:42:36 INFO BlockManager: BlockManager stopped\n","25/06/04 11:42:36 INFO BlockManagerMaster: BlockManagerMaster stopped\n","25/06/04 11:42:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n","25/06/04 11:42:36 INFO SparkContext: Successfully stopped SparkContext\n","25/06/04 11:42:36 INFO ShutdownHookManager: Shutdown hook called\n","25/06/04 11:42:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b529a7c-8322-4971-9687-a4cb80864824\n","25/06/04 11:42:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdb09cb3-5227-409b-bd36-94b41fcbab2e\n","25/06/04 11:42:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdb09cb3-5227-409b-bd36-94b41fcbab2e/pyspark-53d015ef-268a-4bd7-a0e9-db3be3c375d5\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fPC_MWRzwJl_"},"execution_count":null,"outputs":[]}]}
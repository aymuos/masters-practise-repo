{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vhL9HSMbVWxA",
      "metadata": {
        "id": "vhL9HSMbVWxA"
      },
      "outputs": [],
      "source": [
        "# !python3 dataset_generator.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a49966",
      "metadata": {
        "id": "d8a49966"
      },
      "source": [
        "# TODO: Feature Extraction and Transfer Learning\n",
        "\n",
        "This notebook covers:\n",
        "- Feature Extraction and Fine-tuning in Python\n",
        "- Feature Extraction in Spark\n",
        "- Fine-tuning using Orca\n",
        "- Instance and Mapping-based Transfer Learning using `adapt`\n",
        "\n",
        "Complete the TODOs in each section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bf8e1c",
      "metadata": {
        "id": "30bf8e1c"
      },
      "source": [
        "## 1. Feature Extraction and Fine-tuning using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ee875e",
      "metadata": {
        "id": "f1ee875e"
      },
      "outputs": [],
      "source": [
        "# TODO: Load a pretrained CNN and freeze feature layers\n",
        "# - Load CIFAR-100\n",
        "# - Preprocess the dataset\n",
        "# - Replace final layer\n",
        "# - Train only the classifier layer\n",
        "# - Print the f1, precision and recall\n",
        "# - Plot a confusion matrix too\n",
        "\n",
        "# List of models (Choose any one)\n",
        "  # AlexNet\n",
        "  # ConvNeXt\n",
        "  # DenseNet\n",
        "  # EfficientNet\n",
        "  # EfficientNetV2\n",
        "  # GoogLeNet\n",
        "  # Inception V3\n",
        "  # MaxVit\n",
        "  # MNASNet\n",
        "  # MobileNet V2\n",
        "  # MobileNet V3\n",
        "  # RegNet\n",
        "  # ResNet\n",
        "  # ResNeXt\n",
        "  # ShuffleNet V2\n",
        "  # SqueezeNet\n",
        "  # SwinTransformer\n",
        "  # VGG\n",
        "  # VisionTransformer\n",
        "  # Wide ResNet\n",
        "\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace4e42d",
      "metadata": {
        "id": "ace4e42d"
      },
      "source": [
        "## 2. Feature Extraction in Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7246b413",
      "metadata": {
        "id": "7246b413"
      },
      "outputs": [],
      "source": [
        "# Use PySpark to load and process the toy_dataset.csv\n",
        "# - Perform feature encoding and vectorization\n",
        "# - Apply PCA to reduce dimensions\n",
        "# - Visualize or print PCA components\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming toy_dataset.csv is in the current directory or a path you specify\n",
        "file_path = \"/content/toy_dataset.csv\" # Replace with the actual path to your file\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the schema and some data\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "# Perform feature encoding and vectorization\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = [col for col, dtype in df.dtypes if dtype == 'string']\n",
        "numerical_cols = [col for col, dtype in df.dtypes if dtype != 'string' and col != 'label'] # Assuming 'label' is the target column if any\n",
        "\n",
        "# Index categorical columns\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"skip\") for col in categorical_cols]\n",
        "\n",
        "# Assemble all feature columns into a single vector\n",
        "assembler_inputs = [indexer.getOutputCol() for indexer in indexers] + numerical_cols\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "# Create a pipeline to apply transformations\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "\n",
        "# Fit and transform the data\n",
        "pipeline_model = pipeline.fit(df)\n",
        "df_transformed = pipeline_model.transform(df)\n",
        "\n",
        "# Show the transformed data with the features column\n",
        "df_transformed.select(\"features\").show(5, truncate=False)\n",
        "\n",
        "# Apply PCA to reduce dimensions\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Set the number of principal components\n",
        "# You can choose a suitable number based on your analysis or requirements\n",
        "k = 3 # Example: reduce to 3 dimensions\n",
        "\n",
        "pca = PCA(k=k, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "\n",
        "# Fit the PCA model and transform the data\n",
        "pca_model = pca.fit(df_transformed)\n",
        "df_pca = pca_model.transform(df_transformed)\n",
        "\n",
        "# Show the PCA results\n",
        "df_pca.select(\"pcaFeatures\").show(5, truncate=False)\n",
        "\n",
        "# Print the explained variance ratio for each component\n",
        "print(\"Explained Variance Ratio:\", pca_model.explainedVariance)\n",
        "\n",
        "# Print the principal components (loadings)\n",
        "print(\"Principal Components (Loadings):\")\n",
        "print(pca_model.pc)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2N1wgxRvXMqW",
      "metadata": {
        "id": "2N1wgxRvXMqW"
      },
      "outputs": [],
      "source": [
        "# Create a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FeatureExtraction\").getOrCreate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

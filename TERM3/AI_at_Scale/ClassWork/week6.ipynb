{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f27d2471",
      "metadata": {
        "id": "f27d2471"
      },
      "source": [
        "# Spark ML Assignment: Fill-in-the-Blanks\n",
        "\n",
        "Follow the instructions and complete the TODOs below. Solutions are provided for verification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20146084",
      "metadata": {
        "id": "20146084"
      },
      "source": [
        "## Data Cleaning & Type Casting\n",
        "**Instructions:** Load a dataset, remove unwanted characters, and cast column types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2eedda6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
            "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741299 sha256=12df986b338b65f0069f1d2a9658889e2d95ac06646d676270a02140d61a65c0\n",
            "  Stored in directory: /home/aymuos/.cache/pip/wheels/62/69/eb/eef3014e40bbcff88f1d6dd762baebf6bf5d0266ba57be1ef8\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
            "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.9 pyspark-4.0.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c63d580d",
      "metadata": {
        "id": "c63d580d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+------------------+-----------+\n",
            "|employee_id| department|               age|     salary|\n",
            "+-----------+-----------+------------------+-----------+\n",
            "|          1|      Sales|26.036648168706982| $87,242.29|\n",
            "|          2|  Marketing| 33.88012179493058| $82,075.61|\n",
            "|          3|Engineering|49.689412854323926| $70,717.66|\n",
            "|          4|  Marketing|23.761016654599633| $90,979.39|\n",
            "|          5|  Marketing|  44.5000539550718| $87,809.97|\n",
            "|          6|    Finance| 52.26516468575364|$101,828.91|\n",
            "|          7|Engineering| 39.57885077000015| $83,503.41|\n",
            "|          8|Engineering|18.157126216341187| $72,532.49|\n",
            "|          9|Engineering| 38.26845223970014| $72,855.14|\n",
            "|         10|  Marketing| 34.18881045841313| $45,981.45|\n",
            "|         11|      Sales| 39.67794753263152| $77,259.89|\n",
            "|         12|Engineering|42.361223506692575| $74,764.28|\n",
            "|         13|  Marketing|27.202981193401214| $55,048.89|\n",
            "|         14|    Finance|26.561036396560745| $74,620.69|\n",
            "|         15|      Sales| 33.49466141740217| $95,147.52|\n",
            "|         16|    Finance|25.344423281050254| $63,981.79|\n",
            "|         17|      Sales| 36.50489078110594| $60,601.58|\n",
            "|         18|  Marketing| 33.86578748653606| $91,573.92|\n",
            "|         19|         HR|61.335282206121235| $84,513.60|\n",
            "|         20|      Sales|24.749091111891353|$120,916.43|\n",
            "+-----------+-----------+------------------+-----------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from pyspark.sql.types import DoubleType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkML_Cleaning\").getOrCreate()\n",
        "\n",
        "# TODO: Load the dataset from 'data.csv'\n",
        "\n",
        "file = \"data.csv\"\n",
        "df = spark.read.csv(file, header=True, inferSchema=True)\n",
        "\n",
        "df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6628646",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+------------------+---------+\n",
            "|employee_id| department|               age|   salary|\n",
            "+-----------+-----------+------------------+---------+\n",
            "|          1|      Sales|26.036648168706982| 87242.29|\n",
            "|          2|  Marketing| 33.88012179493058| 82075.61|\n",
            "|          3|Engineering|49.689412854323926| 70717.66|\n",
            "|          4|  Marketing|23.761016654599633| 90979.39|\n",
            "|          5|  Marketing|  44.5000539550718| 87809.97|\n",
            "|          6|    Finance| 52.26516468575364|101828.91|\n",
            "|          7|Engineering| 39.57885077000015| 83503.41|\n",
            "|          8|Engineering|18.157126216341187| 72532.49|\n",
            "|          9|Engineering| 38.26845223970014| 72855.14|\n",
            "|         10|  Marketing| 34.18881045841313| 45981.45|\n",
            "|         11|      Sales| 39.67794753263152| 77259.89|\n",
            "|         12|Engineering|42.361223506692575| 74764.28|\n",
            "|         13|  Marketing|27.202981193401214| 55048.89|\n",
            "|         14|    Finance|26.561036396560745| 74620.69|\n",
            "|         15|      Sales| 33.49466141740217| 95147.52|\n",
            "|         16|    Finance|25.344423281050254| 63981.79|\n",
            "|         17|      Sales| 36.50489078110594| 60601.58|\n",
            "|         18|  Marketing| 33.86578748653606| 91573.92|\n",
            "|         19|         HR|61.335282206121235| 84513.60|\n",
            "|         20|      Sales|24.749091111891353|120916.43|\n",
            "+-----------+-----------+------------------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# TODO: Remove '$' sign from 'salary' column and cast to DoubleType\n",
        "df = df.withColumn(\"salary\", regexp_replace(col(\"salary\"), \"\\\\$\", \"\"))\n",
        "df = df.withColumn(\"salary\", regexp_replace(col(\"salary\"), \",\", \"\"))\n",
        "\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ed5fa9e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+------------------+---------+\n",
            "|employee_id| department|               age|   salary|\n",
            "+-----------+-----------+------------------+---------+\n",
            "|          1|      Sales|26.036648168706982| 87242.29|\n",
            "|          2|  Marketing| 33.88012179493058| 82075.61|\n",
            "|          3|Engineering|49.689412854323926| 70717.66|\n",
            "|          4|  Marketing|23.761016654599633| 90979.39|\n",
            "|          5|  Marketing|  44.5000539550718| 87809.97|\n",
            "|          6|    Finance| 52.26516468575364|101828.91|\n",
            "|          7|Engineering| 39.57885077000015| 83503.41|\n",
            "|          8|Engineering|18.157126216341187| 72532.49|\n",
            "|          9|Engineering| 38.26845223970014| 72855.14|\n",
            "|         10|  Marketing| 34.18881045841313| 45981.45|\n",
            "|         11|      Sales| 39.67794753263152| 77259.89|\n",
            "|         12|Engineering|42.361223506692575| 74764.28|\n",
            "|         13|  Marketing|27.202981193401214| 55048.89|\n",
            "|         14|    Finance|26.561036396560745| 74620.69|\n",
            "|         15|      Sales| 33.49466141740217| 95147.52|\n",
            "|         16|    Finance|25.344423281050254| 63981.79|\n",
            "|         17|      Sales| 36.50489078110594| 60601.58|\n",
            "|         18|  Marketing| 33.86578748653606| 91573.92|\n",
            "|         19|         HR|61.335282206121235|  84513.6|\n",
            "|         20|      Sales|24.749091111891353|120916.43|\n",
            "+-----------+-----------+------------------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn(\"salary\", col(\"salary\").cast(DoubleType()))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "05967d16",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DoubleType()\n"
          ]
        }
      ],
      "source": [
        "print(df.schema[\"salary\"].dataType)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cb61ec",
      "metadata": {
        "id": "c2cb61ec"
      },
      "source": [
        "## Feature Engineering\n",
        "**Instructions:** Handle missing values, index categorical columns, and one-hot encode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a848ac2f",
      "metadata": {
        "id": "a848ac2f"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder\n",
        "\n",
        "# TODO: Fill missing values in 'age' column using mean strategy\n",
        "\n",
        "# TODO: Index 'department' column\n",
        "\n",
        "# TODO: One-hot encode 'department_index'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fda2ed4",
      "metadata": {
        "id": "1fda2ed4"
      },
      "source": [
        "## Feature Scaling and Vector Assembler\n",
        "**Instructions:** Combine features into a vector and apply standard scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75756c0e",
      "metadata": {
        "id": "75756c0e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "# TODO: Assemble 'age_imputed' and 'salary' into a single feature vector\n",
        "\n",
        "# TODO: Scale the feature vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aaeecc",
      "metadata": {
        "id": "11aaeecc"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "**Instructions:** Perform basic data analysis using Spark and convert to Pandas for visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1589cd7",
      "metadata": {
        "id": "c1589cd7"
      },
      "outputs": [],
      "source": [
        "# TODO: Display summary statistics for numerical columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaff642d",
      "metadata": {
        "id": "eaff642d"
      },
      "outputs": [],
      "source": [
        "# TODO: Check for null values in each column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51a9813",
      "metadata": {
        "id": "d51a9813"
      },
      "outputs": [],
      "source": [
        "# TODO: Analyze salary distribution by department\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f3ff476",
      "metadata": {
        "id": "2f3ff476"
      },
      "outputs": [],
      "source": [
        "# TODO: Visualize distributions using Pandas and Matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2b00ad",
      "metadata": {
        "id": "3f2b00ad"
      },
      "outputs": [],
      "source": [
        "# TODO: Visualize age distribution per department\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeaf8642",
      "metadata": {
        "id": "eeaf8642"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute correlation between numeric features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4568122b",
      "metadata": {
        "id": "4568122b"
      },
      "source": [
        "## Feature Transformations and Scaling\n",
        "**Instructions:** Experiment with scaling techniques ([MinMaxScaler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinMaxScaler.html)), bucketing, and feature creation ([VectorAssembler\n",
        "](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352b7233",
      "metadata": {
        "id": "352b7233"
      },
      "outputs": [],
      "source": [
        "# TODO: Apply MinMaxScaler to 'salary' and 'age'\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Assemble features before scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1df8129",
      "metadata": {
        "id": "b1df8129"
      },
      "outputs": [],
      "source": [
        "# TODO: Categorize 'age' into bins (e.g., Young, Mid, Senior)\n",
        "from pyspark.sql.functions import when\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ea0dd3",
      "metadata": {
        "id": "98ea0dd3"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a new column 'salary_per_year_of_age'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f215b0",
      "metadata": {
        "id": "23f215b0"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute average salary per age group\n",
        "df_binned.groupBy('age_group').agg({'salary': 'avg'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be9f010d",
      "metadata": {
        "id": "be9f010d"
      },
      "outputs": [],
      "source": [
        "# TODO: Visualize salary per age group\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qsWADqGizwBO",
      "metadata": {
        "id": "qsWADqGizwBO"
      },
      "source": [
        "# Check the libraries and experiment with different parameter values : [OneHotEncoder](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html), [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html), [LinearRegression](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LwRH_TIe15ci",
      "metadata": {
        "id": "LwRH_TIe15ci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "E4tC1Ngo14rh",
      "metadata": {
        "id": "E4tC1Ngo14rh"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

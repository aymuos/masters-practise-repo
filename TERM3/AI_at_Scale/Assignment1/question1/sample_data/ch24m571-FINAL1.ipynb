{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd81b0ee",
   "metadata": {},
   "source": [
    "CH24M571"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533d010",
   "metadata": {},
   "source": [
    "Server Execution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dd43fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "453bfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTE_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ef73ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "if EXECUTE_FLAG == 'True':\n",
    "    # Read code from the file,. replace test1.py with your file name\n",
    "    with open(\"CH24M571.py\", \"r\") as file:\n",
    "        code_content = file.read()\n",
    "\n",
    "    url = \"https://lab.samsai.io/submit\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer f9318293d4e405e5cff5d03a348a02ae0c4331916cd390041a700045d1bcb16a\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"code\": code_content ,\n",
    "        \"student_id\": \"CH24M571\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"Response Body:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0195c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType ,FloatType , ArrayType\n",
    "from pyspark.sql.functions import col, sum , mean , mode , when , stddev , count , isnull ,from_json ,transform ,struct , collect_list,flatten , explode , expr\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855da35",
   "metadata": {},
   "source": [
    "Question 1 & preliminary checks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9272d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = \"CH24M571\"  # your roll number, letters must be in capital letter\n",
    "app_name = student_id + \"_Assignment_1\"\n",
    "assignment_no = \"Assignment_1\"\n",
    "base_path = f'./'\n",
    "\n",
    "#base_path = f\"/opt/spark/data/{student_id}/\"\n",
    "\n",
    "class SparkSessionBuilder:\n",
    "    def __init__(self, app_name, student_id):\n",
    "        self.app_name = app_name\n",
    "        self.student_id = student_id\n",
    "\n",
    "    # Initialize Spark session with the default configurations - optimised for samsai server\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(self.app_name) \\\n",
    "            .config(\"spark.executor.instances\", \"1\") \\\n",
    "            .config(\"spark.executor.memory\", \"512M\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.memory\", \"3G\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    # Change Spark configurations based on the provided configuration for the 4th question\n",
    "    # combo is a tuple of (num_executors, num_cores, executor_memory\n",
    "    def change_config(self, combo):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(self.app_name) \\\n",
    "            .config(\"spark.executor.instances\", combo[0]) \\\n",
    "            .config(\"spark.executor.memory\", combo[2]) \\\n",
    "            .config(\"spark.executor.cores\", combo[1]) \\\n",
    "            .config(\"spark.driver.memory\", \"3G\") \\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47dc733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkSessionBuilder(app_name, student_id)\n",
    "spark = spark_config.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dafc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q1_Data_Ingestion:\n",
    "    def __init__(self):\n",
    "        self.customers_schema = StructType([\n",
    "                                            StructField(\"customer_id\", StringType(), nullable=False),\n",
    "                                            StructField(\"name\", StringType(), True),\n",
    "                                            StructField(\"age\", IntegerType(), True),\n",
    "                                            StructField(\"email\", StringType(), True),\n",
    "                                            StructField(\"preferences\", StringType(), True),\n",
    "                                            StructField(\"address\", StringType(), True)       \n",
    "        ])\n",
    "\n",
    "        # Orders schema\n",
    "        self.orders_schema = StructType([\n",
    "            StructField(\"order_id\", StringType(), nullable=False),\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"order_date\", StringType(), True),\n",
    "            StructField(\"items\", StringType(), True),\n",
    "            StructField(\"shipping_address\", StringType(), True),\n",
    "            StructField(\"total_amount\", FloatType(), True)\n",
    "        ])\n",
    "\n",
    "        # Products schema\n",
    "        self.products_schema = StructType([\n",
    "            StructField(\"product_id\", StringType(), nullable=False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"price\", FloatType(), True),\n",
    "            StructField(\"tags\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    def read_csvs_to_spark_df(self, base_path,spark):\n",
    "            \"\"\"\n",
    "            Read the 3 CSV files into Spark DataFrames using the defined schemas\n",
    "\n",
    "            @return: Tuple of DataFrames (customers_df, orders_df, products_df)\n",
    "            \"\"\"\n",
    "            customers_df = spark.read.csv(base_path + \"customers.csv\", header=True, schema=self.customers_schema, escape=\"\\\"\",quote=\"\\\"\")\n",
    "            products_df = spark.read.csv(base_path + \"products.csv\", header=True, schema=self.products_schema, escape=\"\\\"\",quote=\"\\\"\")\n",
    "            orders_df = spark.read.csv(base_path + \"orders.csv\", header=True, schema=self.orders_schema, escape=\"\\\"\",quote=\"\\\"\")\n",
    "\n",
    "            return customers_df, products_df ,orders_df\n",
    "        \n",
    "\n",
    "    def print_dfs(self, customers_df, orders_df, products_df):\n",
    "            \"\"\"\n",
    "            Print the first 5 rows of each DataFrame\n",
    "\n",
    "            @param customers_df: DataFrame containing customer data\n",
    "            @param orders_df: DataFrame containing order data\n",
    "            @param products_df: DataFrame containing product data\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"----------------------------------\")\n",
    "            print(\"Customers DataFrame:\")\n",
    "            customers_df.show(5, truncate=False)\n",
    "\n",
    "            print(\"----------------------------------\")\n",
    "\n",
    "            print(\"Orders DataFrame:\")\n",
    "            orders_df.show(5, truncate=False)\n",
    "\n",
    "            print(\"----------------------------------\")\n",
    "\n",
    "            print(\"Products DataFrame:\")\n",
    "            products_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1064a775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Customers DataFrame:\n",
      "+-----------+----------+---+---------------------+-----------------------------------------+-------------------------------------------------------------------+\n",
      "|customer_id|name      |age|email                |preferences                              |address                                                            |\n",
      "+-----------+----------+---+---------------------+-----------------------------------------+-------------------------------------------------------------------+\n",
      "|CUST000001 |Customer_1|61 |customer1@example.com|[\"sports\", \"travel\", \"fitness\", \"movies\"]|{\"street\": \"2796 Main St\", \"city\": \"Chicago\", \"zip\": \"25933\"}      |\n",
      "|CUST000002 |Customer_2|55 |customer2@example.com|NULL                                     |{\"street\": \"5409 Main St\", \"city\": \"San Francisco\", \"zip\": \"27728\"}|\n",
      "|CUST000003 |Customer_3|79 |customer3@example.com|[\"movies\", \"books\", \"travel\"]            |{\"street\": \"6081 Main St\", \"city\": \"New York\", \"zip\": \"89893\"}     |\n",
      "|CUST000004 |Customer_4|73 |customer4@example.com|[\"books\", \"fitness\", \"sports\", \"travel\"] |{\"street\": \"3303 Main St\", \"city\": \"San Francisco\", \"zip\": \"24824\"}|\n",
      "|CUST000005 |Customer_5|80 |customer5@example.com|[\"music\", \"books\", \"fitness\", \"movies\"]  |{\"street\": \"4864 Main St\", \"city\": \"Austin\", \"zip\": \"33253\"}       |\n",
      "+-----------+----------+---+---------------------+-----------------------------------------+-------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "----------------------------------\n",
      "Orders DataFrame:\n",
      "+----------+---------+-----------+------+-------------------------------+\n",
      "|product_id|name     |category   |price |tags                           |\n",
      "+----------+---------+-----------+------+-------------------------------+\n",
      "|PROD00001 |Product_1|electronics|5.32  |[\"new\", \"sale\", \"eco-friendly\"]|\n",
      "|PROD00002 |Product_2|sports     |59.11 |[\"limited\", \"eco-friendly\"]    |\n",
      "|PROD00003 |Product_3|sports     |370.91|[\"new\"]                        |\n",
      "|PROD00004 |Product_4|sports     |69.84 |[\"sale\"]                       |\n",
      "|PROD00005 |Product_5|sports     |135.9 |[\"sale\", \"new\", \"popular\"]     |\n",
      "+----------+---------+-----------+------+-------------------------------+\n",
      "only showing top 5 rows\n",
      "----------------------------------\n",
      "Products DataFrame:\n",
      "+----------+-----------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+------------+\n",
      "|order_id  |customer_id|order_date|items                                                                                                                                                                                                                                                                                                        |shipping_address                                              |total_amount|\n",
      "+----------+-----------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+------------+\n",
      "|ORD0000001|CUST000046 |2023-08-16|[{\"product_id\": \"PROD00003\", \"quantity\": 3, \"price\": 370.91}]                                                                                                                                                                                                                                                |{\"street\": \"703 Main St\", \"city\": \"New York\", \"zip\": \"32515\"} |1112.73     |\n",
      "|ORD0000002|CUST000027 |2023-11-20|[{\"product_id\": \"PROD00008\", \"quantity\": 2, \"price\": 40.45}, {\"product_id\": \"PROD00009\", \"quantity\": 2, \"price\": 187.57}]                                                                                                                                                                                    |{\"street\": \"6503 Main St\", \"city\": \"Seattle\", \"zip\": \"60739\"} |456.04      |\n",
      "|ORD0000003|CUST000093 |2023-11-27|[{\"product_id\": \"PROD00006\", \"quantity\": 3, \"price\": 428.33}, {\"product_id\": \"PROD00005\", \"quantity\": 1, \"price\": 135.9}]                                                                                                                                                                                    |{\"street\": \"9664 Main St\", \"city\": \"Chicago\", \"zip\": \"85761\"} |1420.89     |\n",
      "|ORD0000004|CUST000017 |2023-04-08|[{\"product_id\": \"PROD00010\", \"quantity\": 2, \"price\": 176.2}, {\"product_id\": \"PROD00005\", \"quantity\": 1, \"price\": 135.9}, {\"product_id\": \"PROD00004\", \"quantity\": 3, \"price\": 69.84}, {\"product_id\": \"PROD00010\", \"quantity\": 2, \"price\": 176.2}, {\"product_id\": \"PROD00007\", \"quantity\": 1, \"price\": 400.44}]|{\"street\": \"1329 Main St\", \"city\": \"New York\", \"zip\": \"10614\"}|1450.66     |\n",
      "|ORD0000005|CUST000092 |2023-09-24|[{\"product_id\": \"PROD00003\", \"quantity\": 3, \"price\": 370.91}, {\"product_id\": \"PROD00009\", \"quantity\": 2, \"price\": 187.57}, {\"product_id\": \"PROD00010\", \"quantity\": 3, \"price\": 176.2}]                                                                                                                       |{\"street\": \"3075 Main St\", \"city\": \"Austin\", \"zip\": \"30353\"}  |2016.47     |\n",
      "+----------+-----------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "customers_df, products_df, orders_df = Q1_Data_Ingestion().read_csvs_to_spark_df(base_path, spark)\n",
    "Q1_Data_Ingestion().print_dfs(customers_df,  products_df,orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e6366",
   "metadata": {},
   "source": [
    "1. Read all the files into spark dataframe with schema inference\n",
    "2. Print the size of each dataframe and schema\n",
    "3. Identify the corrupt values in each file and report the number of corrupt rows and\n",
    "total corrupt values in each file.\n",
    "4. For numerical columns, fill the missing values with mean value\n",
    "5. For categorical or string columns, fill the missing values with most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9d9d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers DataFrame:\n",
      "Rows: 100, Columns: 6\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- preferences: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      "\n",
      "\n",
      "Orders DataFrame:\n",
      "Rows: 500, Columns: 6\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- shipping_address: string (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n",
      "\n",
      "Products DataFrame:\n",
      "Rows: 10, Columns: 5\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_df_sizes_and_schema(customers_df, orders_df, products_df):\n",
    "    print(\"Customers DataFrame:\")\n",
    "    print(f\"Rows: {customers_df.count()}, Columns: {len(customers_df.columns)}\")\n",
    "    customers_df.printSchema()\n",
    "\n",
    "    print(\"\\nOrders DataFrame:\")\n",
    "    print(f\"Rows: {orders_df.count()}, Columns: {len(orders_df.columns)}\")\n",
    "    orders_df.printSchema()\n",
    "\n",
    "    print(\"\\nProducts DataFrame:\")\n",
    "    print(f\"Rows: {products_df.count()}, Columns: {len(products_df.columns)}\")\n",
    "    products_df.printSchema()\n",
    "\n",
    "print_df_sizes_and_schema(customers_df, orders_df, products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3332f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "\n",
    "    # idea is to create a common object that can be used to clean all the dataframes\n",
    "\n",
    "    def print_null_records(self, df):\n",
    "        null_df = df.select([count(when(isnull(cols), cols)).alias(cols) for cols in df.columns])\n",
    "        return null_df\n",
    "    \n",
    "    # clean dataframes by filling null values with mean or mode based on the data type\n",
    "    # for string type columns, fill with mode, for integer and float type columns, fill with mean\n",
    "    # this function will also print the number of null values in each column\n",
    "\n",
    "\n",
    "    def clean_df(self, df, null_df):\n",
    "\n",
    "        for col_name in null_df.columns:\n",
    "            null_count = null_df.select(col(col_name)).collect()[0][0]\n",
    "            # If the column is not present in the DataFrame, skip it\n",
    "            # Check if the column has null values\n",
    "            if(null_count !=0):\n",
    "                print(f\"DataFrame column '{col_name}' has {null_count} null values. \\\n",
    "                      Filing null values with {'mean' if df.schema[col_name].dataType in [IntegerType(), FloatType()] else 'mode' }.\")\n",
    "                \n",
    "                # Filing the null values in the string type column with the mode\n",
    "                if df.schema[col_name].dataType == StringType():\n",
    "                    mode_col = df.agg(mode(col(col_name))).collect()[0][0]\n",
    "                    df = df.withColumn(col_name, when(col(col_name).isNull(), mode_col).otherwise(col(col_name)))\n",
    "\n",
    "                # Filing the null values in the integer type column with the mean\n",
    "                elif df.schema[col_name].dataType == IntegerType():\n",
    "                    mean_col = df.agg(mean(col(col_name))).collect()[0][0]\n",
    "                    df = df.withColumn(col_name, when(col(col_name).isNull(), mean_col).otherwise(col(col_name)))\n",
    "\n",
    "                # Filing the null values in the float type column with the mean\n",
    "                elif df.schema[col_name].dataType == FloatType():\n",
    "                    mean_col = df.agg(mean(col(col_name))).collect()[0][0]\n",
    "                    df = df.withColumn(col_name, when(col(col_name).isNull(), mean_col).otherwise(col(col_name)))\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7af6e",
   "metadata": {},
   "source": [
    "- executing the corrupt records query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "690cb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_corrupt_records(customers_df, orders_df, products_df):\n",
    "    print(\"Corrupt Records in Customers DataFrame:\")\n",
    "    customer_null_df = DataCleaning().print_null_records(customers_df)\n",
    "    customer_null_df.show()\n",
    "\n",
    "    print(\"Corrupt Records in Orders DataFrame:\")\n",
    "    orders_null_df = DataCleaning().print_null_records(orders_df)\n",
    "    orders_null_df.show()\n",
    "\n",
    "    print(\"Corrupt Records in Products DataFrame:\")\n",
    "    products_null_df = DataCleaning().print_null_records(products_df)\n",
    "    products_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2634116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupt Records in Customers DataFrame:\n",
      "+-----------+----+---+-----+-----------+-------+\n",
      "|customer_id|name|age|email|preferences|address|\n",
      "+-----------+----+---+-----+-----------+-------+\n",
      "|          0|   0|  1|    5|          5|      0|\n",
      "+-----------+----+---+-----+-----------+-------+\n",
      "\n",
      "Corrupt Records in Orders DataFrame:\n",
      "+--------+-----------+----------+-----+----------------+------------+\n",
      "|order_id|customer_id|order_date|items|shipping_address|total_amount|\n",
      "+--------+-----------+----------+-----+----------------+------------+\n",
      "|       0|          0|         0|    0|              23|          14|\n",
      "+--------+-----------+----------+-----+----------------+------------+\n",
      "\n",
      "Corrupt Records in Products DataFrame:\n",
      "+----------+----+--------+-----+----+\n",
      "|product_id|name|category|price|tags|\n",
      "+----------+----+--------+-----+----+\n",
      "|         0|   0|       0|    0|   0|\n",
      "+----------+----+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_corrupt_records(customers_df, orders_df, products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee071743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_values(customers_df, orders_df, products_df):\n",
    "    customers_df = DataCleaning().clean_df(customers_df, DataCleaning().print_null_records(customers_df))\n",
    "    orders_df = DataCleaning().clean_df(orders_df, DataCleaning().print_null_records(orders_df))\n",
    "    products_df = DataCleaning().clean_df(products_df, DataCleaning().print_null_records(products_df))\n",
    "\n",
    "    return customers_df, orders_df, products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66a95e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame column 'age' has 1 null values.                       Filing null values with mean.\n",
      "DataFrame column 'email' has 5 null values.                       Filing null values with mode.\n",
      "DataFrame column 'preferences' has 5 null values.                       Filing null values with mode.\n",
      "DataFrame column 'shipping_address' has 23 null values.                       Filing null values with mode.\n",
      "DataFrame column 'total_amount' has 14 null values.                       Filing null values with mean.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[customer_id: string, name: string, age: double, email: string, preferences: string, address: string],\n",
       " DataFrame[order_id: string, customer_id: string, order_date: string, items: string, shipping_address: string, total_amount: double],\n",
       " DataFrame[product_id: string, name: string, category: string, price: float, tags: string])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_null_values(customers_df, orders_df, products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e566ee",
   "metadata": {},
   "source": [
    "Problem 2: Data Transformation (Marks: 10)\n",
    "1. Compute total spending by each customer\n",
    "2. Identify top 3 products per customer by quantity\n",
    "3. Create a summary dataframe with\n",
    "•customer id\n",
    "•total orders\n",
    "•total spent\n",
    "•top 3 product ids\n",
    "4. Display top 50 customers in terms of total orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "620c19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_a(customers_df, orders_df):\n",
    "    \"\"\"\n",
    "    Q2: Find the top 5 customers based on the total amount spent on orders.\n",
    "    \"\"\"\n",
    "    # Join customers with orders to get total amount spent by each customer\n",
    "    total_spent_df = orders_df.groupBy(\"customer_id\").agg(sum(\"total_amount\").alias(\"total_spent\"))\n",
    "    \n",
    "    # Join with customers to get customer details\n",
    "    top_customers_df = total_spent_df.join(customers_df, \"customer_id\") \\\n",
    "                                       .orderBy(col(\"total_spent\").desc()) \\\n",
    "                                       .limit(5)\n",
    "    \n",
    "    return top_customers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Customers based on Total Amount Spent:\n",
      "+-----------+------------------+-----------+---+----------------------+----------------------------------------+--------------------------------------------------------------+\n",
      "|customer_id|total_spent       |name       |age|email                 |preferences                             |address                                                       |\n",
      "+-----------+------------------+-----------+---+----------------------+----------------------------------------+--------------------------------------------------------------+\n",
      "|CUST000033 |14896.74008178711 |Customer_33|68 |customer33example.com |[\"books\", \"sports\", \"fitness\"]          |{\"street\": \"7338 Main St\", \"city\": \"Chicago\", \"zip\": \"56209\"} |\n",
      "|CUST000016 |13439.240074157715|Customer_16|70 |customer16@example.com|[\"travel\", \"movies\"]                    |{\"street\": \"2764 Main St\", \"city\": \"Chicago\", \"zip\": \"16590\"} |\n",
      "|CUST000083 |12215.219970703125|Customer_83|59 |customer83@example.com|[\"sports\", \"books\", \"fitness\", \"movies\"]|{\"street\": \"4343 Main St\", \"city\": \"Chicago\", \"zip\": \"90952\"} |\n",
      "|CUST000050 |10136.719940185547|Customer_50|70 |customer50@example.com|[\"books\"]                               |{\"street\": \"337 Main St\", \"city\": \"New York\", \"zip\": \"61402\"} |\n",
      "|CUST000006 |10116.099697589874|Customer_6 |44 |customer6@example.com |[\"travel\", \"fitness\"]                   |{\"street\": \"3592 Main St\", \"city\": \"New York\", \"zip\": \"84828\"}|\n",
      "+-----------+------------------+-----------+---+----------------------+----------------------------------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2_a_result = q2_a(customers_df, orders_df)\n",
    "\n",
    "print(\"Top 5 Customers based on Total Amount Spent:\")\n",
    "q2_a_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b88805",
   "metadata": {},
   "source": [
    "- Identify top 3 products per customer by quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ab535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def top3productsperCustomer():\n",
    "\n",
    "    # Compute total spending by each customer from orders_df\n",
    "    customer_spending_df = orders_df.groupBy(\"customer_id\").sum(\"total_amount\").withColumnRenamed(\"sum(total_amount)\", \"total_spending\")\n",
    "    customer_spending_df = customer_spending_df.orderBy(\"customer_id\")\n",
    "\n",
    "    # Define the schema for each item in the items array\n",
    "    item_schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"product_ide\", StringType(), True),\n",
    "            StructField(\"quantity\", IntegerType(), True),\n",
    "            StructField(\"price\", FloatType(), True)\n",
    "        ])\n",
    "    )\n",
    "    # Convert the items column to JSON type (array of structs)\n",
    "    product_orders_df = orders_df.withColumn(\"items_json\", from_json(col(\"items\"), item_schema))\n",
    "\n",
    "    # Keep only product_ide and quantity in items_json\n",
    "    product_orders_df = product_orders_df.withColumn(\n",
    "        \"items_clean\",\n",
    "        transform(\n",
    "            col(\"items_json\"),\n",
    "            lambda x: struct(x[\"product_ide\"].alias(\"product_ide\"), x[\"quantity\"].alias(\"quantity\"))\n",
    "        )\n",
    "    ).select(\"customer_id\", \"items_clean\")\n",
    "\n",
    "    # Merge the Items arrays for each customer\n",
    "    merged_df = product_orders_df.groupBy(\"customer_id\").agg(collect_list(\"Items_clean\").alias(\"all_products\"))\n",
    "    merged_df = merged_df.withColumn(\"all_products\", flatten(col(\"all_products\"))) # Flatten the nested array structure making a single array of products\n",
    "\n",
    "    # Create a new DataFrame with three columns: Customer ID, Product ID, and Quantity by exploding the all_products array\n",
    "    exploded_df = merged_df.withColumn(\"product_id\", explode(col(\"all_products\"))).select(\n",
    "        \"customer_id\",\n",
    "        col(\"product_id.product_ide\").alias(\"product_id\"),\n",
    "        col(\"product_id.quantity\").cast(\"int\").alias(\"Quantity\")\n",
    "    )\n",
    "\n",
    "    # Group by Customer ID and Product ID, summing the Quantity to get total quantity per product per customer\n",
    "    customer_product_df = exploded_df.groupBy(\n",
    "        \"Customer ID\", \"product_id\").sum(\n",
    "        \"Quantity\").withColumnRenamed(\"sum(Quantity)\",\"Total Quantity\")\n",
    "    \n",
    "    # Order by Customer ID and Total Quantity (descending)\n",
    "    customer_product_df = customer_product_df.orderBy(\"customer_id\", col(\"Total Quantity\").desc())\n",
    "\n",
    "    # Merge the all the products with quantity for each customer\n",
    "    customer_product_df = customer_product_df.groupBy(\"customer_id\").agg(\n",
    "        collect_list(struct(\"product_id\", \"total_quantity\")).alias(\"total_orders\")).orderBy(\"customer_id\")\n",
    "\n",
    "    # Select top 3 products for each customer\n",
    "    customer_topthree_product_df = customer_product_df\\\n",
    "        .withColumn(\"Top 3 Products\",\n",
    "        expr(\"slice(Total_Orders, 1, 3)\") # Get the top 3 products for each customer row\n",
    "    ).select(\"customer_id\", \"Top 3 Products\")\n",
    "\n",
    "    summary_df = customer_spending_df \\\n",
    "    .join(customer_product_df, on=\"customer_id\", how=\"right\") \\\n",
    "    .join(customer_topthree_product_df, on=\"customer_id\", how=\"right\")\\\n",
    "    .orderBy(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5410a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-06-23 20:27:17.299\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id`.`product_id` cannot be resolved. Did you mean one of the following? [`all_products`, `Product ID`, `customer_id`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 34 in cell [72]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o965.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id`.`product_id` cannot be resolved. Did you mean one of the following? [`all_products`, `Product ID`, `customer_id`]. SQLSTATE: 42703;\\n'Project [customer_id#694, 'product_id.product_id AS Product ID#1280, cast(Product ID#1279.quantity as int) AS Quantity#1281]\\n+- Project [customer_id#694, all_products#1277, Product ID#1279]\\n   +- Generate explode(all_products#1277), false, [Product ID#1279]\\n      +- Project [customer_id#694, flatten(all_products#1273) AS all_products#1277]\\n         +- Aggregate [customer_id#694], [customer_id#694, collect_list(Items_clean#1271, 0, 0) AS all_products#1273]\\n            +- Project [customer_id#694, items_clean#1271]\\n               +- Project [order_id#693, customer_id#694, order_date#695, items#696, shipping_address#697, total_amount#698, items_json#1268, transform(items_json#1268, lambdafunction(struct(product_id, lambda x_2#1272.product_id, quantity, lambda x_2#1272.quantity), lambda x_2#1272, false)) AS items_clean#1271]\\n                  +- Project [order_id#693, customer_id#694, order_date#695, items#696, shipping_address#697, total_amount#698, from_json(ArrayType(StructType(StructField(product_id,StringType,true),StructField(quantity,IntegerType,true),StructField(price,FloatType,true)),true), items#696, Some(Asia/Kolkata), false) AS items_json#1268]\\n                     +- Relation [order_id#693,customer_id#694,order_date#695,items#696,shipping_address#697,total_amount#698] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 20 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/aymuos/Documents/Github/masters-practise-repo/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/aymuos/Documents/Github/masters-practise-repo/.venv/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id`.`product_id` cannot be resolved. Did you mean one of the following? [`all_products`, `Product ID`, `customer_id`]. SQLSTATE: 42703;\n'Project [customer_id#694, 'product_id.product_id AS Product ID#1280, cast(Product ID#1279.quantity as int) AS Quantity#1281]\n+- Project [customer_id#694, all_products#1277, Product ID#1279]\n   +- Generate explode(all_products#1277), false, [Product ID#1279]\n      +- Project [customer_id#694, flatten(all_products#1273) AS all_products#1277]\n         +- Aggregate [customer_id#694], [customer_id#694, collect_list(Items_clean#1271, 0, 0) AS all_products#1273]\n            +- Project [customer_id#694, items_clean#1271]\n               +- Project [order_id#693, customer_id#694, order_date#695, items#696, shipping_address#697, total_amount#698, items_json#1268, transform(items_json#1268, lambdafunction(struct(product_id, lambda x_2#1272.product_id, quantity, lambda x_2#1272.quantity), lambda x_2#1272, false)) AS items_clean#1271]\n                  +- Project [order_id#693, customer_id#694, order_date#695, items#696, shipping_address#697, total_amount#698, from_json(ArrayType(StructType(StructField(product_id,StringType,true),StructField(quantity,IntegerType,true),StructField(price,FloatType,true)),true), items#696, Some(Asia/Kolkata), false) AS items_json#1268]\n                     +- Relation [order_id#693,customer_id#694,order_date#695,items#696,shipping_address#697,total_amount#698] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtop3productsperCustomer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 32\u001b[0m, in \u001b[0;36mtop3productsperCustomer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_products\u001b[39m\u001b[38;5;124m\"\u001b[39m, flatten(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_products\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \u001b[38;5;66;03m# Flatten the nested array structure making a single array of products\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with three columns: Customer ID, Product ID, and Quantity by exploding the all_products array\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m exploded_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProduct ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_products\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_id.product_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProduct ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProduct ID.quantity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuantity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Group by Customer ID and Product ID, summing the Quantity to get total quantity per product per customer\u001b[39;00m\n\u001b[1;32m     39\u001b[0m customer_product_df \u001b[38;5;241m=\u001b[39m exploded_df\u001b[38;5;241m.\u001b[39mgroupBy(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum(Quantity)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Github/masters-practise-repo/.venv/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:991\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParentDataFrame:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Documents/Github/masters-practise-repo/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Github/masters-practise-repo/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id`.`product_id` cannot be resolved. Did you mean one of the following? [`all_products`, `Product ID`, `customer_id`]. SQLSTATE: 42703;\n'Project [customer_id#694, 'product_id.product_id AS Product ID#1280, cast(Product ID#1279.quantity as int) AS Quantity#1281]\n+- Project [customer_id#694, all_products#1277, Product ID#1279]\n   +- Generate explode(all_products#1277), false, [Product ID#1279]\n      +- Project [customer_id#694, flatten(all_products#1273) AS all_products#1277]\n         +- Aggregate [customer_id#694], [customer_id#694, collect_list(Items_clean#1271, 0, 0) AS all_products#1273]\n            +- Project [customer_id#694, items_clean#1271]\n               +- Project [order_id#693, customer_id#694, order_date#695, items#696, shipping_address#697, total_amount#698, items_json#1268, transform(items_json#1268, lambdafunction(struct(product_id, lambda x_2#1272.product_id, quantity, lambda x_2#1272.quantity), lambda x_2#1272, false)) AS items_clean#1271]\n                  +- Project [order_id#693, customer_id#694, order_date#695, items#696, shipping_address#697, total_amount#698, from_json(ArrayType(StructType(StructField(product_id,StringType,true),StructField(quantity,IntegerType,true),StructField(price,FloatType,true)),true), items#696, Some(Asia/Kolkata), false) AS items_json#1268]\n                     +- Relation [order_id#693,customer_id#694,order_date#695,items#696,shipping_address#697,total_amount#698] csv\n"
     ]
    }
   ],
   "source": [
    "top3productsperCustomer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

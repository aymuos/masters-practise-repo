{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymuos/masters-practise-repo/blob/main/TERM3/ReinforcementLearning/cleanup_rlproj0408.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3900a7e",
      "metadata": {
        "id": "c3900a7e"
      },
      "source": [
        "Starting point\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a70614e",
      "metadata": {
        "id": "6a70614e"
      },
      "source": [
        "Cleaned | D3QN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "74d2227b",
      "metadata": {
        "id": "74d2227b"
      },
      "outputs": [],
      "source": [
        "# from typing import List, Tuple, Optional\n",
        "# import numpy as np\n",
        "\n",
        "# class InventoryEnv:\n",
        "#     \"\"\"\n",
        "#     Inventory management environment for 3 products with volume constraints, lead times, and stochastic or deterministic demand.\n",
        "\n",
        "#     This environment simulates:\n",
        "#     - Warehouse inventory evolution with lead-time-based ordering.\n",
        "#     - Daily customer demand and fulfillment.\n",
        "#     - Cost computation due to holding, ordering, and stockouts.\n",
        "\n",
        "#     Attributes:\n",
        "#         volume_capacity (float): Max warehouse volume capacity.\n",
        "#         initial_inventory (List[int]): Initial stock for each product.\n",
        "#         product_volumes (List[float]): Volume per unit of each product.\n",
        "#         holding_cost_per_volume (float): Cost per unit volume per day for storing inventory.\n",
        "#         stockout_costs (List[float]): Penalty per unit of unfulfilled demand for each product.\n",
        "#         ordering_costs (List[float]): Fixed cost per order placed for each product.\n",
        "#         discard_costs (List[float]): Cost per unit discarded due to over-capacity.\n",
        "#         lead_times (List[int]): Days before an order arrives for each product.\n",
        "#         simulation_days (int): Episode length in days.\n",
        "#         demand_sequences (Optional[List[List[int]]]): Predefined demand for evaluation.\n",
        "#         demand_lambda (List[float]): Poisson mean for training demand generation.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         volume_capacity: float = 1000.0,\n",
        "#         initial_inventory: List[int] = [100, 100, 100],\n",
        "#         product_volumes: List[float] = [2.0, 3.0, 1.5],\n",
        "#         holding_cost_per_volume: float = 5.0,  # Updated holding cost\n",
        "#         stockout_costs: List[float] = [400.0, 500.0, 300.0],\n",
        "#         ordering_costs: List[float] = [80.0, 200.0, 120.0],\n",
        "#         discard_costs: List[float] = [200.0, 250.0, 150.0],  # New discard penalties\n",
        "#         lead_times: List[int] = [3, 2, 1],\n",
        "#         simulation_days: int = 50,\n",
        "#         demand_sequences: Optional[List[List[int]]] = None,\n",
        "#         demand_lambda: List[float] = [30, 25, 35],\n",
        "#         seed: int = 42\n",
        "#     ):\n",
        "#         self.volume_capacity = volume_capacity\n",
        "#         self.initial_inventory = initial_inventory[:]\n",
        "#         self.product_volumes = product_volumes\n",
        "#         self.holding_cost_per_volume = holding_cost_per_volume\n",
        "#         self.stockout_costs = stockout_costs\n",
        "#         self.ordering_costs = ordering_costs\n",
        "#         self.discard_costs = discard_costs\n",
        "#         self.lead_times = lead_times\n",
        "#         self.simulation_days = simulation_days\n",
        "#         self.demand_sequences = demand_sequences\n",
        "#         self.demand_lambda = demand_lambda\n",
        "#         self.random_state = np.random.RandomState(seed)\n",
        "\n",
        "#         self.last_orders = [0, 0, 0] # Initialize last orders for stability penalty\n",
        "#         self.reset()\n",
        "\n",
        "#     def reset(self) -> List[int]:\n",
        "#         \"\"\"\n",
        "#         Reset environment to initial state for a new episode.\n",
        "#         Returns the initial observation state.\n",
        "#         \"\"\"\n",
        "#         self.day = 0\n",
        "#         self.inventory = self.initial_inventory[:] # Resets current inventory to initial inventory\n",
        "#         self.pending_orders = [[] for _ in range(len(self.initial_inventory))]  # list of orders to be delivered (day_due, quantity)\n",
        "#         self.last_orders = [0, 0, 0] # Reset last orders\n",
        "#         return self._get_state() # Returns initial state of the environment\n",
        "\n",
        "#     def step(self, action: List[int]) -> Tuple[List[int], float, bool, dict]:\n",
        "#         \"\"\"\n",
        "#         Executes one simulation step.\n",
        "\n",
        "#         Args:\n",
        "#             action (List[int]): List of order quantities for each product. Each value must be in {0, 10, ..., 100}.\n",
        "\n",
        "#         Returns:\n",
        "#             state (List[int]): Updated state after taking the action.\n",
        "#             reward (float): Scaled negative cost for the step.\n",
        "#             done (bool): True if the episode is over.\n",
        "#             info (dict): Additional information (cost breakdown, demand, fulfillment).\n",
        "#         \"\"\"\n",
        "#         assert all(a in range(0, 101, 10) for a in action), \"Actions must be in {0, 10, ..., 100}\" # Invalid actions are rejected\n",
        "\n",
        "#         # 1. Receive due orders and add them to current inventory\n",
        "#         for i in range(3):\n",
        "#             arrivals = [qty for due, qty in self.pending_orders[i] if due == self.day]\n",
        "#             self.inventory[i] += sum(arrivals)\n",
        "#             self.pending_orders[i] = [(due, qty) for due, qty in self.pending_orders[i] if due > self.day]\n",
        "\n",
        "#         # 2. Place new orders and add them to pending orders\n",
        "#         order_cost = 0\n",
        "#         for i in range(3):\n",
        "#             if action[i] > 0:\n",
        "#                 order_cost += self.ordering_costs[i]\n",
        "#                 self.pending_orders[i].append((self.day + self.lead_times[i], action[i]))\n",
        "\n",
        "#         # 3. Generate demand if not provided\n",
        "#         if self.demand_sequences:\n",
        "#             demand = self.demand_sequences[self.day]\n",
        "#         else:\n",
        "#             demand = [self.random_state.poisson(lam) for lam in self.demand_lambda]\n",
        "\n",
        "#         # 4. Enforce volume capacity and compute discards\n",
        "#         total_volume = sum(self.inventory[i] * self.product_volumes[i] for i in range(3))\n",
        "#         discarded = [0, 0, 0]\n",
        "#         if total_volume > self.volume_capacity:\n",
        "#             overflow = total_volume - self.volume_capacity\n",
        "#             # discard from highest-volume items first\n",
        "#             for i in sorted(range(3), key=lambda j: self.product_volumes[j], reverse=True):\n",
        "#                 max_remove = int(overflow // self.product_volumes[i])\n",
        "#                 remove_qty = min(max_remove, self.inventory[i])\n",
        "#                 discarded[i] = remove_qty\n",
        "#                 self.inventory[i] -= remove_qty\n",
        "#                 overflow -= remove_qty * self.product_volumes[i]\n",
        "#                 if overflow <= 0:\n",
        "#                     break\n",
        "\n",
        "#         # 5. Fulfill demand and compute stockouts\n",
        "#         fulfilled = [min(self.inventory[i], demand[i]) for i in range(3)]\n",
        "#         unfulfilled = [demand[i] - fulfilled[i] for i in range(3)]\n",
        "#         self.inventory = [self.inventory[i] - fulfilled[i] for i in range(3)]\n",
        "\n",
        "#         # 6. Compute costs and reward\n",
        "#         holding_cost = sum(self.inventory[i] * self.product_volumes[i] * self.holding_cost_per_volume for i in range(3))\n",
        "#         stockout_cost = sum(unfulfilled[i] * self.stockout_costs[i] for i in range(3))\n",
        "#         discard_cost = sum(discarded[i] * self.discard_costs[i] for i in range(3))\n",
        "#         total_cost = holding_cost + stockout_cost + order_cost + discard_cost\n",
        "#         reward = - total_cost / 100.0  # scaled for stability\n",
        "\n",
        "#         # Reward Shaping (only during training)\n",
        "#         # Note: This part is typically done in the agent's learning step, not the environment step.\n",
        "#         # However, for simplicity and to demonstrate the concept, we'll add it here.\n",
        "#         # In a real-world scenario, you'd pass a flag to step or handle shaping in the agent.\n",
        "\n",
        "#         # Inventory balance bonus: Encourage inventory near a target level (e.g., 50 for each product)\n",
        "#         target_inventory = [50, 50, 50]\n",
        "#         inventory_balance_bonus = 0\n",
        "#         for i in range(3):\n",
        "#              inventory_balance_bonus -= abs(self.inventory[i] - target_inventory[i]) * 0.1 # Small penalty for deviation\n",
        "\n",
        "\n",
        "#         # Order stability penalty: Penalize large swings in daily order quantity\n",
        "#         order_stability_penalty = 0\n",
        "#         for i in range(3):\n",
        "#             order_stability_penalty -= abs(action[i] - self.last_orders[i]) * 0.05 # Small penalty for order changes\n",
        "\n",
        "#         self.last_orders = action # Update last orders\n",
        "\n",
        "\n",
        "#         # Stockout-free reward: Bonus for zero unfulfilled demand\n",
        "#         stockout_free_bonus = 0\n",
        "#         if all(u == 0 for u in unfulfilled):\n",
        "#             stockout_free_bonus += 1.0 # Small bonus for no stockouts\n",
        "\n",
        "#         reward += inventory_balance_bonus + order_stability_penalty + stockout_free_bonus\n",
        "\n",
        "\n",
        "#         # 7. Update state\n",
        "#         self.day += 1\n",
        "#         done = self.day >= self.simulation_days # True if episode ends\n",
        "#         info = {\n",
        "#             \"day\": self.day,\n",
        "#             \"inventory\": self.inventory[:],\n",
        "#             \"fulfilled\": fulfilled,\n",
        "#             \"unfulfilled\": unfulfilled,\n",
        "#             \"order_cost\": order_cost,\n",
        "#             \"holding_cost\": holding_cost,\n",
        "#             \"stockout_cost\": stockout_cost,\n",
        "#             \"discard_cost\": discard_cost,\n",
        "#             \"total_cost\": total_cost\n",
        "#         }\n",
        "\n",
        "#         return self._get_state(), reward, done, info\n",
        "\n",
        "#     def _get_state(self) -> List[int]:\n",
        "#         \"\"\"\n",
        "#         Constructs the state vector including inventory levels and outstanding orders.\n",
        "\n",
        "#         Returns:\n",
        "#             List[int]: State representation with 7 variables\n",
        "#         \"\"\"\n",
        "#         outstanding_orders = [sum(qty for _, qty in self.pending_orders[i]) for i in range(3)]\n",
        "#         return self.inventory + outstanding_orders + [self.day]\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import random\n",
        "# from collections import deque\n",
        "\n",
        "# class DuelingQNetwork(nn.Module):\n",
        "#     def __init__(self, state_size, action_size, hidden_size=128):\n",
        "#         super(DuelingQNetwork, self).__init__()\n",
        "#         self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "#         self.value_stream = nn.Linear(hidden_size, 1)\n",
        "#         self.advantage_stream = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "\n",
        "#         value = self.value_stream(x)\n",
        "#         advantage = self.advantage_stream(x)\n",
        "\n",
        "#         q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "#         return q_values\n",
        "\n",
        "# class DQNAgent:\n",
        "#     def __init__(self, state_size, action_size, gamma=0.99, lr=5e-4, # Increased learning rate\n",
        "#                  batch_size=128, buffer_size=100000, epsilon_start=1.0, # Increased batch size and buffer size\n",
        "#                  epsilon_end=0.01, epsilon_decay=0.99, alpha=0.6, beta_start=0.4, beta_frames=1000): # Added alpha and beta params\n",
        "#         self.state_size = state_size\n",
        "#         self.action_size = action_size\n",
        "#         self.gamma = gamma\n",
        "#         self.batch_size = batch_size\n",
        "#         # Replace deque with PrioritizedReplayBuffer\n",
        "#         self.memory = PrioritizedReplayBuffer(buffer_size, alpha)\n",
        "#         self.epsilon = epsilon_start\n",
        "#         self.epsilon_min = epsilon_end\n",
        "#         self.epsilon_decay = epsilon_decay\n",
        "\n",
        "#         self.beta = beta_start # For importance sampling\n",
        "#         self.beta_increment_per_frame = (1.0 - beta_start) / beta_frames # Beta annealing\n",
        "\n",
        "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#         self.qnetwork = DuelingQNetwork(state_size, action_size).to(self.device)\n",
        "#         self.target_network = DuelingQNetwork(state_size, action_size).to(self.device)\n",
        "#         self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=lr) # Using Adam optimizer\n",
        "\n",
        "#         # For state normalization (reset for new agent)\n",
        "#         self.state_mean = np.zeros(state_size)\n",
        "#         self.state_std = np.ones(state_size)\n",
        "#         self.state_count = 0\n",
        "\n",
        "\n",
        "#     def remember(self, state, action, reward, next_state, done):\n",
        "#         # Use the add method of PrioritizedReplayBuffer\n",
        "#         self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "#     def normalize_state(self, state):\n",
        "#         return (state - self.state_mean) / (self.state_std + 1e-8) # Add epsilon for stability\n",
        "\n",
        "#     def update_state_stats(self, state):\n",
        "#         # Simple running mean and variance update\n",
        "#         self.state_count += 1\n",
        "#         delta = state - self.state_mean\n",
        "#         self.state_mean += delta / self.state_count\n",
        "#         M2 = np.mean((state - self.state_mean)**2) * (self.state_count -1) if self.state_count > 1 else 0\n",
        "#         self.state_std = np.sqrt(M2 / self.state_count) if self.state_count > 1 else np.ones(self.state_size)\n",
        "\n",
        "\n",
        "#     def act(self, state):\n",
        "#         # Update state statistics before normalization\n",
        "#         self.update_state_stats(np.array(state))\n",
        "#         normalized_state = self.normalize_state(np.array(state))\n",
        "\n",
        "#         if np.random.rand() <= self.epsilon:\n",
        "#             return random.randrange(self.action_size)\n",
        "#         state = torch.FloatTensor(normalized_state).unsqueeze(0).to(self.device)\n",
        "#         self.qnetwork.eval() # Set network to evaluation mode\n",
        "#         with torch.no_grad():\n",
        "#             q_values = self.qnetwork(state)\n",
        "#         self.qnetwork.train() # Set network back to training mode\n",
        "#         return torch.argmax(q_values).item()\n",
        "\n",
        "#     def replay(self):\n",
        "#         if len(self.memory.experience) < self.batch_size: # Check if enough experiences are in the buffer\n",
        "#             return\n",
        "\n",
        "#         # Use sample method of PrioritizedReplayBuffer which returns weights and indices\n",
        "#         states, actions, rewards, next_states, dones, weights, indices = self.memory.sample(self.batch_size, self.beta)\n",
        "\n",
        "#         # Normalize states and next_states before converting to tensors\n",
        "#         normalized_states = [self.normalize_state(np.array(s)) for s in states]\n",
        "#         normalized_next_states = [self.normalize_state(np.array(s)) for s in next_states]\n",
        "\n",
        "#         states = torch.FloatTensor(normalized_states).to(self.device)\n",
        "#         actions = torch.LongTensor(actions).to(self.device)\n",
        "#         rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "#         next_states = torch.FloatTensor(normalized_next_states).to(self.device)\n",
        "#         dones = torch.FloatTensor(dones).to(self.device)\n",
        "#         weights = torch.FloatTensor(weights).to(self.device)\n",
        "\n",
        "\n",
        "#         q_values = self.qnetwork(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "#         # Double DQN update\n",
        "#         next_actions = self.qnetwork(next_states).max(1)[1].unsqueeze(1)\n",
        "#         next_q_values = self.target_network(next_states).gather(1, next_actions).squeeze(1)\n",
        "\n",
        "#         targets = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "#         # Compute TD errors and update priorities\n",
        "#         td_errors = targets - q_values.detach() # Use detach() to avoid backprop through target\n",
        "#         self.memory.update_priorities(indices, td_errors.abs().cpu().detach().numpy())\n",
        "\n",
        "#         # Apply importance sampling weights to the loss\n",
        "#         loss = (nn.MSELoss(reduction='none')(q_values, targets) * weights).mean()\n",
        "\n",
        "\n",
        "#         self.optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         self.optimizer.step()\n",
        "\n",
        "#         if self.epsilon > self.epsilon_min:\n",
        "#             self.epsilon *= self.epsilon_decay\n",
        "#         self.beta = min(1.0, self.beta + self.beta_increment_per_frame) # Anneal beta\n",
        "\n",
        "#     def update_target_network(self):\n",
        "#         self.target_network.load_state_dict(self.qnetwork.state_dict())\n",
        "\n",
        "# # Re-initialize agent with new class definition\n",
        "# agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# episodes = 500\n",
        "# target_update_freq = 10\n",
        "\n",
        "# for e in range(episodes):\n",
        "#     state = env.reset()\n",
        "#     total_reward = 0\n",
        "#     done = False\n",
        "\n",
        "#     while not done:\n",
        "#         action_idx = agent.act(state)\n",
        "\n",
        "#         # Convert flat index to 3 product orders\n",
        "#         orders = np.unravel_index(action_idx, (11, 11, 11))\n",
        "#         orders = [o * 10 for o in orders]\n",
        "\n",
        "#         next_state, reward, done, _ = env.step(orders)\n",
        "#         agent.remember(state, action_idx, reward, next_state, done)\n",
        "#         agent.replay()\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "\n",
        "#     if e % target_update_freq == 0:\n",
        "#         agent.update_target_network()\n",
        "\n",
        "#     # print(f\"Episode {e}, Total Reward: {total_reward}\") # Optionally keep print for monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df892b1e"
      },
      "source": [
        "---------------------------------------------------------------"
      ],
      "id": "df892b1e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6c8cf2e"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SumTree:\n",
        "    \"\"\"\n",
        "    A SumTree data structure for efficient storage and sampling of priorities.\n",
        "    Used internally by PrioritizedReplayBuffer.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.data_pointer = 0\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        \"\"\"Add data and update the tree.\"\"\"\n",
        "        tree_index = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data\n",
        "        self.update(tree_index, priority)\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_index, priority):\n",
        "        \"\"\"Update the priority of a data point.\"\"\"\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "        while tree_index != 0:\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"Retrieve a data point and its priority from the tree.\"\"\"\n",
        "        parent_index = 0\n",
        "        while True:\n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "            if left_child_index >= len(self.tree):\n",
        "                leaf_index = parent_index\n",
        "                break\n",
        "            else:\n",
        "                if v <= self.tree[left_child_index]:\n",
        "                    parent_index = left_child_index\n",
        "                else:\n",
        "                    v -= self.tree[left_child_index]\n",
        "                    parent_index = right_child_index\n",
        "        data_index = leaf_index - self.capacity + 1\n",
        "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
        "\n",
        "    @property\n",
        "    def total_priority(self):\n",
        "        \"\"\"Get the sum of all priorities.\"\"\"\n",
        "        return self.tree[0]\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"\n",
        "    Prioritized Experience Replay Buffer for DQN.\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size, alpha=0.6, seed=42):\n",
        "        self.sum_tree = SumTree(buffer_size)\n",
        "        self.alpha = alpha\n",
        "        self.buffer_size = buffer_size\n",
        "        self.experience = [None] * buffer_size  # Use a list to store experiences\n",
        "        self.current_size = 0 # To track the actual number of experiences stored\n",
        "        self.random_state = np.random.RandomState(seed)\n",
        "        self.max_priority = 1.0 # Initial max priority for new experiences\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adds an experience to the buffer with maximum priority.\"\"\"\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.sum_tree.add(self.max_priority, self.current_size) # Store index in SumTree\n",
        "        self.experience[self.current_size] = experience # Store experience in list\n",
        "        self.current_size = (self.current_size + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        \"\"\"Samples a batch of experiences based on priorities.\"\"\"\n",
        "        minibatch = []\n",
        "        indices = []\n",
        "        weights = []\n",
        "        total_priority = self.sum_tree.total_priority\n",
        "        segment = total_priority / batch_size\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            v = self.random_state.uniform(a, b)\n",
        "            tree_index, priority, data_index = self.sum_tree.get_leaf(v) # data_index is the index in the experience list\n",
        "\n",
        "            # Calculate importance sampling weight\n",
        "            sampling_probability = priority / total_priority\n",
        "            weight = (self.buffer_size * sampling_probability) ** -beta\n",
        "            weights.append(weight)\n",
        "            indices.append(tree_index) # Store the tree index for priority updates\n",
        "            minibatch.append(self.experience[data_index]) # Retrieve experience from list\n",
        "\n",
        "        # Normalize weights\n",
        "        max_weight = max(weights) if weights else 1.0\n",
        "        weights = [w / max_weight for w in weights]\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        return (torch.FloatTensor(states),\n",
        "                torch.LongTensor(actions),\n",
        "                torch.FloatTensor(rewards),\n",
        "                torch.FloatTensor(next_states),\n",
        "                torch.FloatTensor(dones),\n",
        "                torch.FloatTensor(weights),\n",
        "                indices)\n",
        "\n",
        "    def update_priorities(self, tree_indices, td_errors):\n",
        "        \"\"\"Updates the priorities of sampled experiences based on TD errors.\"\"\"\n",
        "        for tree_index, td_error in zip(tree_indices, td_errors):\n",
        "            priority = abs(td_error) ** self.alpha\n",
        "            self.sum_tree.update(tree_index, priority)\n",
        "            self.max_priority = max(self.max_priority, priority) # Update max priority"
      ],
      "id": "a6c8cf2e",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4yhYlBQFCU8"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, gamma=0.99, lr=5e-4,\n",
        "                 batch_size=128, buffer_size=100000, epsilon_start=1.0,\n",
        "                 epsilon_end=0.01, epsilon_decay=0.99, alpha=0.6, beta_start=0.4, beta_frames=1000):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = PrioritizedReplayBuffer(buffer_size, alpha)\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.beta = beta_start\n",
        "        self.beta_increment_per_frame = (1.0 - beta_start) / beta_frames\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qnetwork = DuelingQNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_network = DuelingQNetwork(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=lr)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        self.qnetwork.eval()\n",
        "        with torch.no_grad():\n",
        "            q_values = self.qnetwork(state)\n",
        "        self.qnetwork.train()\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.current_size < self.batch_size: # Check if enough experiences are in the buffer\n",
        "             return\n",
        "\n",
        "        states, actions, rewards, next_states, dones, weights, indices = self.memory.sample(self.batch_size, self.beta)\n",
        "\n",
        "        # Move tensors to the correct device\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        weights = weights.to(self.device)\n",
        "\n",
        "        q_values = self.qnetwork(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        next_actions = self.qnetwork(next_states).max(1)[1].unsqueeze(1)\n",
        "        next_q_values = self.target_network(next_states).gather(1, next_actions).squeeze(1)\n",
        "\n",
        "        targets = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        td_errors = targets - q_values.detach()\n",
        "        # Apply .detach() before converting to numpy\n",
        "        self.memory.update_priorities(indices, td_errors.abs().cpu().detach().numpy())\n",
        "\n",
        "        loss = (nn.MSELoss(reduction='none')(q_values, targets) * weights).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        self.beta = min(1.0, self.beta + self.beta_increment_per_frame)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.qnetwork.state_dict())\n",
        "\n",
        "# Re-initialize agent with new class definition\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "episodes = 500\n",
        "target_update_freq = 10\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action_idx = agent.act(state)\n",
        "\n",
        "        # Convert flat index to 3 product orders\n",
        "        orders = np.unravel_index(action_idx, (11, 11, 11))\n",
        "        orders = [o * 10 for o in orders]\n",
        "\n",
        "        next_state, reward, done, _ = env.step(orders)\n",
        "        agent.remember(state, action_idx, reward, next_state, done)\n",
        "        agent.replay()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if e % target_update_freq == 0:\n",
        "        agent.update_target_network()\n",
        "\n",
        "    # print(f\"Episode {e}, Total Reward: {total_reward}\") # Optionally keep print for monitoring"
      ],
      "id": "d4yhYlBQFCU8",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a48eb03",
        "outputId": "f12ecf15-feb4-4b64-bc7b-f599daed0dc2"
      },
      "source": [
        "# Continue the training loop execution from the previous cell\n",
        "# The loop runs for 'episodes' number of times, performing agent interactions and updates\n",
        "# The total reward for each episode is printed to the console within the loop\n",
        "\n",
        "# The training loop code was provided in the previous successful cell:\n",
        "# for e in range(episodes):\n",
        "#     state = env.reset()\n",
        "#     total_reward = 0\n",
        "#     done = False\n",
        "\n",
        "#     while not done:\n",
        "#         action_idx = agent.act(state)\n",
        "\n",
        "#         # Convert flat index to 3 product orders\n",
        "#         orders = np.unravel_index(action_idx, (11, 11, 11))\n",
        "#         orders = [o * 10 for o in orders]\n",
        "\n",
        "#         next_state, reward, done, _ = env.step(orders)\n",
        "#         agent.remember(state, action_idx, reward, next_state, done)\n",
        "#         agent.replay()\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "\n",
        "#     if e % target_update_freq == 0:\n",
        "#         agent.update_target_network()\n",
        "\n",
        "#     print(f\"Episode {e}, Total Reward: {total_reward}\") # Optionally keep print for monitoring\n",
        "\n",
        "# Re-run the training loop\n",
        "episodes = 500\n",
        "target_update_freq = 10\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action_idx = agent.act(state)\n",
        "\n",
        "        # Convert flat index to 3 product orders\n",
        "        orders = np.unravel_index(action_idx, (11, 11, 11))\n",
        "        orders = [o * 10 for o in orders]\n",
        "\n",
        "        next_state, reward, done, _ = env.step(orders)\n",
        "        agent.remember(state, action_idx, reward, next_state, done)\n",
        "        agent.replay()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if e % target_update_freq == 0:\n",
        "        agent.update_target_network()\n",
        "\n",
        "    print(f\"Episode {e}, Total Reward: {total_reward}\")"
      ],
      "id": "1a48eb03",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -2010.2499999999998\n",
            "Episode 1, Total Reward: -1883.9749999999997\n",
            "Episode 2, Total Reward: -1820.5999999999997\n",
            "Episode 3, Total Reward: -2115.7\n",
            "Episode 4, Total Reward: -1740.9750000000001\n",
            "Episode 5, Total Reward: -1954.4249999999995\n",
            "Episode 6, Total Reward: -1760.9750000000001\n",
            "Episode 7, Total Reward: -1869.4000000000003\n",
            "Episode 8, Total Reward: -1615.1500000000003\n",
            "Episode 9, Total Reward: -1614.6500000000008\n",
            "Episode 10, Total Reward: -1679.4750000000001\n",
            "Episode 11, Total Reward: -1678.6750000000002\n",
            "Episode 12, Total Reward: -1892.4499999999996\n",
            "Episode 13, Total Reward: -1254.7\n",
            "Episode 14, Total Reward: -1905.3250000000005\n",
            "Episode 15, Total Reward: -2059.7999999999997\n",
            "Episode 16, Total Reward: -1553.7\n",
            "Episode 17, Total Reward: -1757.0749999999998\n",
            "Episode 18, Total Reward: -2047.4999999999998\n",
            "Episode 19, Total Reward: -1462.8499999999995\n",
            "Episode 20, Total Reward: -1514.55\n",
            "Episode 21, Total Reward: -1516.8749999999998\n",
            "Episode 22, Total Reward: -2377.1\n",
            "Episode 23, Total Reward: -1482.3999999999999\n",
            "Episode 24, Total Reward: -2133.6000000000004\n",
            "Episode 25, Total Reward: -1487.7250000000004\n",
            "Episode 26, Total Reward: -2276.1999999999994\n",
            "Episode 27, Total Reward: -3203.3000000000015\n",
            "Episode 28, Total Reward: -1469.9999999999995\n",
            "Episode 29, Total Reward: -1917.2250000000001\n",
            "Episode 30, Total Reward: -1520.6000000000004\n",
            "Episode 31, Total Reward: -1683.9249999999997\n",
            "Episode 32, Total Reward: -1291.3250000000003\n",
            "Episode 33, Total Reward: -1789.2999999999997\n",
            "Episode 34, Total Reward: -1622.9\n",
            "Episode 35, Total Reward: -1480.2499999999995\n",
            "Episode 36, Total Reward: -1790.675\n",
            "Episode 37, Total Reward: -1408.55\n",
            "Episode 38, Total Reward: -1903.8\n",
            "Episode 39, Total Reward: -2043.6999999999998\n",
            "Episode 40, Total Reward: -1530.3249999999998\n",
            "Episode 41, Total Reward: -1565.225\n",
            "Episode 42, Total Reward: -1788.1499999999996\n",
            "Episode 43, Total Reward: -2175.675\n",
            "Episode 44, Total Reward: -1570.6999999999996\n",
            "Episode 45, Total Reward: -1360.325\n",
            "Episode 46, Total Reward: -1377.625\n",
            "Episode 47, Total Reward: -1652.25\n",
            "Episode 48, Total Reward: -1924.5500000000004\n",
            "Episode 49, Total Reward: -1272.4250000000002\n",
            "Episode 50, Total Reward: -1366.125\n",
            "Episode 51, Total Reward: -1651.1499999999999\n",
            "Episode 52, Total Reward: -1631.1999999999996\n",
            "Episode 53, Total Reward: -2060.525\n",
            "Episode 54, Total Reward: -1359.1249999999998\n",
            "Episode 55, Total Reward: -2227.3250000000007\n",
            "Episode 56, Total Reward: -1487.9250000000006\n",
            "Episode 57, Total Reward: -1135.4\n",
            "Episode 58, Total Reward: -1748.2\n",
            "Episode 59, Total Reward: -1618.2999999999997\n",
            "Episode 60, Total Reward: -1656.9999999999995\n",
            "Episode 61, Total Reward: -1373.825\n",
            "Episode 62, Total Reward: -1661.725\n",
            "Episode 63, Total Reward: -1754.1500000000003\n",
            "Episode 64, Total Reward: -1220.2000000000003\n",
            "Episode 65, Total Reward: -1380.575\n",
            "Episode 66, Total Reward: -1607.0249999999996\n",
            "Episode 67, Total Reward: -1324.35\n",
            "Episode 68, Total Reward: -1782.1999999999996\n",
            "Episode 69, Total Reward: -1791.7499999999998\n",
            "Episode 70, Total Reward: -1411.775\n",
            "Episode 71, Total Reward: -1732.6000000000004\n",
            "Episode 72, Total Reward: -2084.2000000000003\n",
            "Episode 73, Total Reward: -1958.1499999999999\n",
            "Episode 74, Total Reward: -2012.9500000000003\n",
            "Episode 75, Total Reward: -1589.8249999999996\n",
            "Episode 76, Total Reward: -1491.225\n",
            "Episode 77, Total Reward: -2130.5\n",
            "Episode 78, Total Reward: -2134.6499999999996\n",
            "Episode 79, Total Reward: -2968.3999999999996\n",
            "Episode 80, Total Reward: -1636.375\n",
            "Episode 81, Total Reward: -2116.8499999999995\n",
            "Episode 82, Total Reward: -1563.1249999999998\n",
            "Episode 83, Total Reward: -1649.8500000000001\n",
            "Episode 84, Total Reward: -1354.9\n",
            "Episode 85, Total Reward: -1329.325\n",
            "Episode 86, Total Reward: -1454.5\n",
            "Episode 87, Total Reward: -1184.0\n",
            "Episode 88, Total Reward: -1732.3750000000005\n",
            "Episode 89, Total Reward: -1814.0499999999997\n",
            "Episode 90, Total Reward: -1695.3749999999998\n",
            "Episode 91, Total Reward: -1671.8999999999996\n",
            "Episode 92, Total Reward: -1378.675\n",
            "Episode 93, Total Reward: -1248.1249999999995\n",
            "Episode 94, Total Reward: -1743.475\n",
            "Episode 95, Total Reward: -1185.1249999999998\n",
            "Episode 96, Total Reward: -1594.425\n",
            "Episode 97, Total Reward: -1811.5749999999998\n",
            "Episode 98, Total Reward: -1587.4750000000001\n",
            "Episode 99, Total Reward: -1286.6249999999998\n",
            "Episode 100, Total Reward: -1200.05\n",
            "Episode 101, Total Reward: -1216.05\n",
            "Episode 102, Total Reward: -2042.8999999999999\n",
            "Episode 103, Total Reward: -1352.625\n",
            "Episode 104, Total Reward: -1282.95\n",
            "Episode 105, Total Reward: -1485.6749999999997\n",
            "Episode 106, Total Reward: -1964.825\n",
            "Episode 107, Total Reward: -1381.4999999999998\n",
            "Episode 108, Total Reward: -1351.075\n",
            "Episode 109, Total Reward: -1550.375\n",
            "Episode 110, Total Reward: -1488.725\n",
            "Episode 111, Total Reward: -2052.375\n",
            "Episode 112, Total Reward: -1587.6000000000004\n",
            "Episode 113, Total Reward: -1533.6000000000001\n",
            "Episode 114, Total Reward: -1880.4250000000004\n",
            "Episode 115, Total Reward: -1859.7250000000004\n",
            "Episode 116, Total Reward: -1545.0000000000005\n",
            "Episode 117, Total Reward: -1689.2999999999995\n",
            "Episode 118, Total Reward: -1388.5500000000004\n",
            "Episode 119, Total Reward: -2120.125\n",
            "Episode 120, Total Reward: -1589.6250000000002\n",
            "Episode 121, Total Reward: -1488.0000000000002\n",
            "Episode 122, Total Reward: -1804.8250000000003\n",
            "Episode 123, Total Reward: -1470.3750000000007\n",
            "Episode 124, Total Reward: -1324.6250000000005\n",
            "Episode 125, Total Reward: -1597.4750000000001\n",
            "Episode 126, Total Reward: -1314.575\n",
            "Episode 127, Total Reward: -1745.1000000000004\n",
            "Episode 128, Total Reward: -1517.95\n",
            "Episode 129, Total Reward: -1421.9499999999998\n",
            "Episode 130, Total Reward: -1474.2499999999998\n",
            "Episode 131, Total Reward: -1514.5000000000002\n",
            "Episode 132, Total Reward: -1193.5250000000003\n",
            "Episode 133, Total Reward: -1461.4\n",
            "Episode 134, Total Reward: -1844.9999999999995\n",
            "Episode 135, Total Reward: -1648.5749999999996\n",
            "Episode 136, Total Reward: -1322.95\n",
            "Episode 137, Total Reward: -2215.3\n",
            "Episode 138, Total Reward: -1493.25\n",
            "Episode 139, Total Reward: -1411.2249999999997\n",
            "Episode 140, Total Reward: -1473.125\n",
            "Episode 141, Total Reward: -1665.8500000000001\n",
            "Episode 142, Total Reward: -1406.3749999999993\n",
            "Episode 143, Total Reward: -1694.0249999999999\n",
            "Episode 144, Total Reward: -3829.175\n",
            "Episode 145, Total Reward: -1409.4499999999998\n",
            "Episode 146, Total Reward: -1407.6\n",
            "Episode 147, Total Reward: -1143.225\n",
            "Episode 148, Total Reward: -1508.2\n",
            "Episode 149, Total Reward: -1873.1499999999999\n",
            "Episode 150, Total Reward: -1686.2749999999999\n",
            "Episode 151, Total Reward: -1704.7999999999995\n",
            "Episode 152, Total Reward: -1986.4999999999995\n",
            "Episode 153, Total Reward: -3387.600000000001\n",
            "Episode 154, Total Reward: -1310.7499999999998\n",
            "Episode 155, Total Reward: -2124.2250000000004\n",
            "Episode 156, Total Reward: -2006.2250000000004\n",
            "Episode 157, Total Reward: -1734.1499999999999\n",
            "Episode 158, Total Reward: -2089.575\n",
            "Episode 159, Total Reward: -1876.5749999999994\n",
            "Episode 160, Total Reward: -1727.1500000000005\n",
            "Episode 161, Total Reward: -1325.7499999999998\n",
            "Episode 162, Total Reward: -1587.2250000000001\n",
            "Episode 163, Total Reward: -1908.0250000000003\n",
            "Episode 164, Total Reward: -1589.375\n",
            "Episode 165, Total Reward: -1898.4500000000003\n",
            "Episode 166, Total Reward: -1803.1750000000002\n",
            "Episode 167, Total Reward: -1644.8999999999999\n",
            "Episode 168, Total Reward: -1645.9750000000006\n",
            "Episode 169, Total Reward: -1719.125\n",
            "Episode 170, Total Reward: -1635.7\n",
            "Episode 171, Total Reward: -2200.7250000000004\n",
            "Episode 172, Total Reward: -1673.7499999999998\n",
            "Episode 173, Total Reward: -2069.3250000000003\n",
            "Episode 174, Total Reward: -1428.225\n",
            "Episode 175, Total Reward: -1986.5500000000004\n",
            "Episode 176, Total Reward: -1433.5250000000003\n",
            "Episode 177, Total Reward: -1543.1249999999998\n",
            "Episode 178, Total Reward: -2073.075\n",
            "Episode 179, Total Reward: -1370.0000000000002\n",
            "Episode 180, Total Reward: -1700.45\n",
            "Episode 181, Total Reward: -2207.225\n",
            "Episode 182, Total Reward: -1776.3250000000003\n",
            "Episode 183, Total Reward: -1860.8250000000003\n",
            "Episode 184, Total Reward: -2182.1499999999996\n",
            "Episode 185, Total Reward: -1658.7250000000004\n",
            "Episode 186, Total Reward: -1830.2749999999996\n",
            "Episode 187, Total Reward: -1763.4749999999992\n",
            "Episode 188, Total Reward: -1567.1\n",
            "Episode 189, Total Reward: -1507.7750000000005\n",
            "Episode 190, Total Reward: -2023.1499999999996\n",
            "Episode 191, Total Reward: -1570.0999999999997\n",
            "Episode 192, Total Reward: -1287.1999999999998\n",
            "Episode 193, Total Reward: -1767.2\n",
            "Episode 194, Total Reward: -1451.8250000000005\n",
            "Episode 195, Total Reward: -1817.3999999999999\n",
            "Episode 196, Total Reward: -1465.05\n",
            "Episode 197, Total Reward: -1523.8749999999995\n",
            "Episode 198, Total Reward: -1626.4250000000002\n",
            "Episode 199, Total Reward: -1424.6750000000002\n",
            "Episode 200, Total Reward: -1559.4999999999998\n",
            "Episode 201, Total Reward: -1469.8000000000002\n",
            "Episode 202, Total Reward: -1191.9499999999996\n",
            "Episode 203, Total Reward: -1642.0\n",
            "Episode 204, Total Reward: -1845.8250000000005\n",
            "Episode 205, Total Reward: -2003.0749999999987\n",
            "Episode 206, Total Reward: -1591.4749999999995\n",
            "Episode 207, Total Reward: -1246.4499999999998\n",
            "Episode 208, Total Reward: -1410.8499999999997\n",
            "Episode 209, Total Reward: -1201.6499999999996\n",
            "Episode 210, Total Reward: -1396.1249999999998\n",
            "Episode 211, Total Reward: -1320.6249999999998\n",
            "Episode 212, Total Reward: -1518.8249999999996\n",
            "Episode 213, Total Reward: -1594.2500000000002\n",
            "Episode 214, Total Reward: -1634.5500000000004\n",
            "Episode 215, Total Reward: -1364.8499999999997\n",
            "Episode 216, Total Reward: -1366.1749999999995\n",
            "Episode 217, Total Reward: -1394.775\n",
            "Episode 218, Total Reward: -1478.9250000000004\n",
            "Episode 219, Total Reward: -1355.6000000000004\n",
            "Episode 220, Total Reward: -1689.75\n",
            "Episode 221, Total Reward: -2020.5500000000006\n",
            "Episode 222, Total Reward: -1899.2500000000002\n",
            "Episode 223, Total Reward: -1325.675\n",
            "Episode 224, Total Reward: -1963.2499999999993\n",
            "Episode 225, Total Reward: -1507.2250000000004\n",
            "Episode 226, Total Reward: -1391.45\n",
            "Episode 227, Total Reward: -1710.3999999999999\n",
            "Episode 228, Total Reward: -2435.0\n",
            "Episode 229, Total Reward: -1609.525\n",
            "Episode 230, Total Reward: -1253.1500000000005\n",
            "Episode 231, Total Reward: -1669.8500000000004\n",
            "Episode 232, Total Reward: -1341.1500000000003\n",
            "Episode 233, Total Reward: -1911.7750000000003\n",
            "Episode 234, Total Reward: -2878.5999999999995\n",
            "Episode 235, Total Reward: -1624.475\n",
            "Episode 236, Total Reward: -1534.7249999999997\n",
            "Episode 237, Total Reward: -2157.1499999999996\n",
            "Episode 238, Total Reward: -1580.1249999999998\n",
            "Episode 239, Total Reward: -1761.8749999999998\n",
            "Episode 240, Total Reward: -2005.2749999999999\n",
            "Episode 241, Total Reward: -1954.0000000000005\n",
            "Episode 242, Total Reward: -1783.1999999999996\n",
            "Episode 243, Total Reward: -2002.9749999999997\n",
            "Episode 244, Total Reward: -1431.9999999999998\n",
            "Episode 245, Total Reward: -1897.4750000000006\n",
            "Episode 246, Total Reward: -2128.9750000000004\n",
            "Episode 247, Total Reward: -1720.2750000000003\n",
            "Episode 248, Total Reward: -1931.3000000000002\n",
            "Episode 249, Total Reward: -1649.7250000000006\n",
            "Episode 250, Total Reward: -2153.875\n",
            "Episode 251, Total Reward: -1588.5999999999997\n",
            "Episode 252, Total Reward: -1627.8750000000002\n",
            "Episode 253, Total Reward: -1942.725\n",
            "Episode 254, Total Reward: -1817.3999999999999\n",
            "Episode 255, Total Reward: -2021.875\n",
            "Episode 256, Total Reward: -1665.7000000000005\n",
            "Episode 257, Total Reward: -1664.1749999999995\n",
            "Episode 258, Total Reward: -1604.65\n",
            "Episode 259, Total Reward: -1586.1250000000002\n",
            "Episode 260, Total Reward: -1722.2250000000004\n",
            "Episode 261, Total Reward: -1725.4749999999992\n",
            "Episode 262, Total Reward: -1637.6\n",
            "Episode 263, Total Reward: -1918.8500000000001\n",
            "Episode 264, Total Reward: -1505.8500000000001\n",
            "Episode 265, Total Reward: -1816.6499999999999\n",
            "Episode 266, Total Reward: -2052.4999999999995\n",
            "Episode 267, Total Reward: -1737.15\n",
            "Episode 268, Total Reward: -1550.65\n",
            "Episode 269, Total Reward: -1598.8999999999999\n",
            "Episode 270, Total Reward: -1313.0000000000002\n",
            "Episode 271, Total Reward: -1531.2250000000004\n",
            "Episode 272, Total Reward: -1685.55\n",
            "Episode 273, Total Reward: -1423.75\n",
            "Episode 274, Total Reward: -1182.7999999999997\n",
            "Episode 275, Total Reward: -1810.8999999999994\n",
            "Episode 276, Total Reward: -1628.9250000000002\n",
            "Episode 277, Total Reward: -1774.3500000000004\n",
            "Episode 278, Total Reward: -1799.275\n",
            "Episode 279, Total Reward: -1532.6500000000003\n",
            "Episode 280, Total Reward: -1306.55\n",
            "Episode 281, Total Reward: -1437.9000000000003\n",
            "Episode 282, Total Reward: -1481.6250000000002\n",
            "Episode 283, Total Reward: -1214.975\n",
            "Episode 284, Total Reward: -1474.75\n",
            "Episode 285, Total Reward: -1590.5\n",
            "Episode 286, Total Reward: -1403.2250000000001\n",
            "Episode 287, Total Reward: -1842.4499999999996\n",
            "Episode 288, Total Reward: -1564.8249999999998\n",
            "Episode 289, Total Reward: -1412.1999999999994\n",
            "Episode 290, Total Reward: -1398.0249999999996\n",
            "Episode 291, Total Reward: -1351.525\n",
            "Episode 292, Total Reward: -1854.8\n",
            "Episode 293, Total Reward: -1821.4000000000003\n",
            "Episode 294, Total Reward: -1426.85\n",
            "Episode 295, Total Reward: -1385.6499999999999\n",
            "Episode 296, Total Reward: -1387.9500000000003\n",
            "Episode 297, Total Reward: -1345.2499999999995\n",
            "Episode 298, Total Reward: -2054.6250000000005\n",
            "Episode 299, Total Reward: -2032.4\n",
            "Episode 300, Total Reward: -2492.874999999999\n",
            "Episode 301, Total Reward: -2350.3250000000003\n",
            "Episode 302, Total Reward: -1378.6999999999996\n",
            "Episode 303, Total Reward: -1588.4249999999997\n",
            "Episode 304, Total Reward: -1851.6750000000009\n",
            "Episode 305, Total Reward: -2809.7999999999993\n",
            "Episode 306, Total Reward: -1852.3999999999996\n",
            "Episode 307, Total Reward: -1914.1749999999997\n",
            "Episode 308, Total Reward: -1962.7499999999993\n",
            "Episode 309, Total Reward: -1632.4000000000005\n",
            "Episode 310, Total Reward: -1644.6749999999997\n",
            "Episode 311, Total Reward: -1380.0249999999999\n",
            "Episode 312, Total Reward: -1032.2499999999998\n",
            "Episode 313, Total Reward: -1385.6249999999998\n",
            "Episode 314, Total Reward: -1259.7\n",
            "Episode 315, Total Reward: -1413.375\n",
            "Episode 316, Total Reward: -1392.8499999999997\n",
            "Episode 317, Total Reward: -1244.8250000000003\n",
            "Episode 318, Total Reward: -1684.8250000000003\n",
            "Episode 319, Total Reward: -1760.7\n",
            "Episode 320, Total Reward: -1629.3500000000006\n",
            "Episode 321, Total Reward: -1060.0500000000002\n",
            "Episode 322, Total Reward: -1717.65\n",
            "Episode 323, Total Reward: -1222.8249999999998\n",
            "Episode 324, Total Reward: -1746.475\n",
            "Episode 325, Total Reward: -1440.5999999999997\n",
            "Episode 326, Total Reward: -1356.825\n",
            "Episode 327, Total Reward: -1241.6750000000002\n",
            "Episode 328, Total Reward: -1180.4750000000001\n",
            "Episode 329, Total Reward: -1595.6750000000002\n",
            "Episode 330, Total Reward: -1439.5000000000002\n",
            "Episode 331, Total Reward: -1500.275\n",
            "Episode 332, Total Reward: -1449.6000000000001\n",
            "Episode 333, Total Reward: -1577.2249999999997\n",
            "Episode 334, Total Reward: -1661.5000000000002\n",
            "Episode 335, Total Reward: -1487.45\n",
            "Episode 336, Total Reward: -1445.65\n",
            "Episode 337, Total Reward: -1238.05\n",
            "Episode 338, Total Reward: -1878.6499999999996\n",
            "Episode 339, Total Reward: -1110.7999999999997\n",
            "Episode 340, Total Reward: -1495.900000000001\n",
            "Episode 341, Total Reward: -1377.1500000000005\n",
            "Episode 342, Total Reward: -1146.7000000000003\n",
            "Episode 343, Total Reward: -1407.8250000000003\n",
            "Episode 344, Total Reward: -1357.7499999999998\n",
            "Episode 345, Total Reward: -1423.8500000000004\n",
            "Episode 346, Total Reward: -1642.7749999999996\n",
            "Episode 347, Total Reward: -1471.1500000000005\n",
            "Episode 348, Total Reward: -1451.85\n",
            "Episode 349, Total Reward: -1606.1750000000004\n",
            "Episode 350, Total Reward: -1285.2749999999999\n",
            "Episode 351, Total Reward: -1451.7999999999993\n",
            "Episode 352, Total Reward: -1577.05\n",
            "Episode 353, Total Reward: -1374.55\n",
            "Episode 354, Total Reward: -1060.3499999999997\n",
            "Episode 355, Total Reward: -1086.425\n",
            "Episode 356, Total Reward: -1465.4250000000002\n",
            "Episode 357, Total Reward: -1260.6750000000004\n",
            "Episode 358, Total Reward: -1798.6249999999998\n",
            "Episode 359, Total Reward: -1301.9249999999997\n",
            "Episode 360, Total Reward: -1640.2\n",
            "Episode 361, Total Reward: -1318.0249999999999\n",
            "Episode 362, Total Reward: -1637.2999999999997\n",
            "Episode 363, Total Reward: -1360.1000000000001\n",
            "Episode 364, Total Reward: -1439.225\n",
            "Episode 365, Total Reward: -1245.9750000000001\n",
            "Episode 366, Total Reward: -1286.625\n",
            "Episode 367, Total Reward: -1368.475\n",
            "Episode 368, Total Reward: -1413.0249999999994\n",
            "Episode 369, Total Reward: -2013.5750000000003\n",
            "Episode 370, Total Reward: -1308.375\n",
            "Episode 371, Total Reward: -1437.2999999999997\n",
            "Episode 372, Total Reward: -1336.1499999999999\n",
            "Episode 373, Total Reward: -1272.1000000000001\n",
            "Episode 374, Total Reward: -1323.8000000000002\n",
            "Episode 375, Total Reward: -1488.05\n",
            "Episode 376, Total Reward: -1730.075\n",
            "Episode 377, Total Reward: -1662.4749999999995\n",
            "Episode 378, Total Reward: -1463.925\n",
            "Episode 379, Total Reward: -1530.0\n",
            "Episode 380, Total Reward: -1636.1750000000004\n",
            "Episode 381, Total Reward: -1268.1999999999998\n",
            "Episode 382, Total Reward: -1196.3\n",
            "Episode 383, Total Reward: -1487.7999999999997\n",
            "Episode 384, Total Reward: -1445.8250000000003\n",
            "Episode 385, Total Reward: -1323.3750000000002\n",
            "Episode 386, Total Reward: -1403.0249999999999\n",
            "Episode 387, Total Reward: -1310.375\n",
            "Episode 388, Total Reward: -1736.9\n",
            "Episode 389, Total Reward: -1127.2749999999999\n",
            "Episode 390, Total Reward: -1439.1250000000005\n",
            "Episode 391, Total Reward: -1318.6999999999996\n",
            "Episode 392, Total Reward: -1283.725\n",
            "Episode 393, Total Reward: -1276.9499999999996\n",
            "Episode 394, Total Reward: -1484.6499999999996\n",
            "Episode 395, Total Reward: -1545.35\n",
            "Episode 396, Total Reward: -1537.6500000000003\n",
            "Episode 397, Total Reward: -1944.825\n",
            "Episode 398, Total Reward: -1914.9499999999998\n",
            "Episode 399, Total Reward: -1564.7999999999997\n",
            "Episode 400, Total Reward: -1640.7250000000001\n",
            "Episode 401, Total Reward: -1495.375\n",
            "Episode 402, Total Reward: -1372.7250000000001\n",
            "Episode 403, Total Reward: -1290.1499999999996\n",
            "Episode 404, Total Reward: -1499.8\n",
            "Episode 405, Total Reward: -1466.6\n",
            "Episode 406, Total Reward: -1500.7000000000007\n",
            "Episode 407, Total Reward: -1171.0999999999995\n",
            "Episode 408, Total Reward: -1295.8000000000002\n",
            "Episode 409, Total Reward: -1655.4749999999997\n",
            "Episode 410, Total Reward: -1176.4249999999993\n",
            "Episode 411, Total Reward: -1269.1499999999999\n",
            "Episode 412, Total Reward: -1174.1000000000004\n",
            "Episode 413, Total Reward: -1471.1749999999997\n",
            "Episode 414, Total Reward: -1324.3999999999996\n",
            "Episode 415, Total Reward: -1325.525\n",
            "Episode 416, Total Reward: -1721.5750000000003\n",
            "Episode 417, Total Reward: -1067.0000000000002\n",
            "Episode 418, Total Reward: -1395.9249999999997\n",
            "Episode 419, Total Reward: -1640.9250000000002\n",
            "Episode 420, Total Reward: -1358.4000000000005\n",
            "Episode 421, Total Reward: -1425.8999999999999\n",
            "Episode 422, Total Reward: -1283.85\n",
            "Episode 423, Total Reward: -2135.525\n",
            "Episode 424, Total Reward: -2027.375\n",
            "Episode 425, Total Reward: -1585.05\n",
            "Episode 426, Total Reward: -1821.225\n",
            "Episode 427, Total Reward: -1749.9750000000001\n",
            "Episode 428, Total Reward: -1355.5249999999999\n",
            "Episode 429, Total Reward: -1299.8249999999998\n",
            "Episode 430, Total Reward: -1884.6499999999992\n",
            "Episode 431, Total Reward: -1620.9749999999997\n",
            "Episode 432, Total Reward: -1799.7500000000011\n",
            "Episode 433, Total Reward: -2347.05\n",
            "Episode 434, Total Reward: -1612.6500000000003\n",
            "Episode 435, Total Reward: -2298.975000000001\n",
            "Episode 436, Total Reward: -7248.349999999999\n",
            "Episode 437, Total Reward: -1630.4750000000001\n",
            "Episode 438, Total Reward: -2356.575\n",
            "Episode 439, Total Reward: -2255.6499999999996\n",
            "Episode 440, Total Reward: -1672.0249999999996\n",
            "Episode 441, Total Reward: -1428.75\n",
            "Episode 442, Total Reward: -1219.8500000000001\n",
            "Episode 443, Total Reward: -1498.2250000000004\n",
            "Episode 444, Total Reward: -1344.3000000000002\n",
            "Episode 445, Total Reward: -1609.4000000000003\n",
            "Episode 446, Total Reward: -1941.2750000000003\n",
            "Episode 447, Total Reward: -1577.2500000000002\n",
            "Episode 448, Total Reward: -1610.1000000000001\n",
            "Episode 449, Total Reward: -1847.3000000000002\n",
            "Episode 450, Total Reward: -1791.1500000000003\n",
            "Episode 451, Total Reward: -1789.075\n",
            "Episode 452, Total Reward: -1192.4249999999997\n",
            "Episode 453, Total Reward: -1493.6499999999992\n",
            "Episode 454, Total Reward: -2539.3499999999995\n",
            "Episode 455, Total Reward: -1947.5500000000004\n",
            "Episode 456, Total Reward: -1570.7500000000002\n",
            "Episode 457, Total Reward: -2282.425000000001\n",
            "Episode 458, Total Reward: -1790.4250000000002\n",
            "Episode 459, Total Reward: -4116.525\n",
            "Episode 460, Total Reward: -2093.6249999999995\n",
            "Episode 461, Total Reward: -1259.725\n",
            "Episode 462, Total Reward: -1425.9249999999993\n",
            "Episode 463, Total Reward: -1427.2250000000001\n",
            "Episode 464, Total Reward: -1465.7999999999997\n",
            "Episode 465, Total Reward: -1530.625\n",
            "Episode 466, Total Reward: -2026.1999999999996\n",
            "Episode 467, Total Reward: -1712.3750000000005\n",
            "Episode 468, Total Reward: -1511.85\n",
            "Episode 469, Total Reward: -1275.3500000000004\n",
            "Episode 470, Total Reward: -1202.3999999999999\n",
            "Episode 471, Total Reward: -1424.9750000000001\n",
            "Episode 472, Total Reward: -1483.4750000000001\n",
            "Episode 473, Total Reward: -1360.2999999999997\n",
            "Episode 474, Total Reward: -1489.1499999999999\n",
            "Episode 475, Total Reward: -1488.8999999999999\n",
            "Episode 476, Total Reward: -1368.7499999999995\n",
            "Episode 477, Total Reward: -1226.4749999999997\n",
            "Episode 478, Total Reward: -1953.5499999999995\n",
            "Episode 479, Total Reward: -1461.5750000000003\n",
            "Episode 480, Total Reward: -2101.4999999999995\n",
            "Episode 481, Total Reward: -1345.8\n",
            "Episode 482, Total Reward: -1709.1999999999998\n",
            "Episode 483, Total Reward: -1338.1000000000004\n",
            "Episode 484, Total Reward: -1840.8249999999996\n",
            "Episode 485, Total Reward: -1294.5749999999998\n",
            "Episode 486, Total Reward: -1442.1999999999996\n",
            "Episode 487, Total Reward: -1447.5249999999999\n",
            "Episode 488, Total Reward: -1443.8500000000004\n",
            "Episode 489, Total Reward: -1245.0499999999997\n",
            "Episode 490, Total Reward: -1479.7000000000005\n",
            "Episode 491, Total Reward: -1362.2500000000002\n",
            "Episode 492, Total Reward: -1292.525\n",
            "Episode 493, Total Reward: -1500.125\n",
            "Episode 494, Total Reward: -1817.2000000000003\n",
            "Episode 495, Total Reward: -1335.0750000000005\n",
            "Episode 496, Total Reward: -1075.8000000000002\n",
            "Episode 497, Total Reward: -1338.3500000000004\n",
            "Episode 498, Total Reward: -1357.25\n",
            "Episode 499, Total Reward: -1734.2000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddd46c88"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SumTree:\n",
        "    \"\"\"\n",
        "    A SumTree data structure for efficient storage and sampling of priorities.\n",
        "    Used internally by PrioritizedReplayBuffer.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.data_pointer = 0\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        \"\"\"Add data and update the tree.\"\"\"\n",
        "        tree_index = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data\n",
        "        self.update(tree_index, priority)\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_index, priority):\n",
        "        \"\"\"Update the priority of a data point.\"\"\"\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "        while tree_index != 0:\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change\n",
        "\n",
        "    def get_leaf(self, v):\n",
        "        \"\"\"Retrieve a data point and its priority from the tree.\"\"\"\n",
        "        parent_index = 0\n",
        "        while True:\n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "            if left_child_index >= len(self.tree):\n",
        "                leaf_index = parent_index\n",
        "                break\n",
        "            else:\n",
        "                if v <= self.tree[left_child_index]:\n",
        "                    parent_index = left_child_index\n",
        "                else:\n",
        "                    v -= self.tree[left_child_index]\n",
        "                    parent_index = right_child_index\n",
        "        data_index = leaf_index - self.capacity + 1\n",
        "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
        "\n",
        "    @property\n",
        "    def total_priority(self):\n",
        "        \"\"\"Get the sum of all priorities.\"\"\"\n",
        "        return self.tree[0]\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"\n",
        "    Prioritized Experience Replay Buffer for DQN.\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size, alpha=0.6, seed=42):\n",
        "        self.sum_tree = SumTree(buffer_size)\n",
        "        self.alpha = alpha\n",
        "        self.buffer_size = buffer_size\n",
        "        self.experience = [None] * buffer_size  # Use a list to store experiences\n",
        "        self.current_size = 0 # To track the actual number of experiences stored\n",
        "        self.random_state = np.random.RandomState(seed)\n",
        "        self.max_priority = 1.0 # Initial max priority for new experiences\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adds an experience to the buffer with maximum priority.\"\"\"\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.sum_tree.add(self.max_priority, self.current_size) # Store index in SumTree\n",
        "        self.experience[self.current_size] = experience # Store experience in list\n",
        "        self.current_size = (self.current_size + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        \"\"\"Samples a batch of experiences based on priorities.\"\"\"\n",
        "        minibatch = []\n",
        "        indices = []\n",
        "        weights = []\n",
        "        total_priority = self.sum_tree.total_priority\n",
        "        segment = total_priority / batch_size\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            v = self.random_state.uniform(a, b)\n",
        "            tree_index, priority, data_index = self.sum_tree.get_leaf(v) # data_index is the index in the experience list\n",
        "\n",
        "            # Calculate importance sampling weight\n",
        "            sampling_probability = priority / total_priority\n",
        "            weight = (self.buffer_size * sampling_probability) ** -beta\n",
        "            weights.append(weight)\n",
        "            indices.append(tree_index) # Store the tree index for priority updates\n",
        "            minibatch.append(self.experience[data_index]) # Retrieve experience from list\n",
        "\n",
        "        # Normalize weights\n",
        "        max_weight = max(weights) if weights else 1.0\n",
        "        weights = [w / max_weight for w in weights]\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        return (torch.FloatTensor(states),\n",
        "                torch.LongTensor(actions),\n",
        "                torch.FloatTensor(rewards),\n",
        "                torch.FloatTensor(next_states),\n",
        "                torch.FloatTensor(dones),\n",
        "                torch.FloatTensor(weights),\n",
        "                indices)\n",
        "\n",
        "    def update_priorities(self, tree_indices, td_errors):\n",
        "        \"\"\"Updates the priorities of sampled experiences based on TD errors.\"\"\"\n",
        "        for tree_index, td_error in zip(tree_indices, td_errors):\n",
        "            priority = abs(td_error) ** self.alpha\n",
        "            self.sum_tree.update(tree_index, priority)\n",
        "            self.max_priority = max(self.max_priority, priority) # Update max priority"
      ],
      "id": "ddd46c88",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f8dedfc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8dedfc0",
        "outputId": "795db38a-d7d7-4759-d86f-73fe3c8a0356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Reward over 500 episodes: -1397.03\n"
          ]
        }
      ],
      "source": [
        "# # Calculate and display the average reward per episode\n",
        "# episode_rewards = []\n",
        "\n",
        "# for e in range(episodes):\n",
        "#     state = env.reset()\n",
        "#     total_reward = 0\n",
        "#     done = False\n",
        "\n",
        "#     while not done:\n",
        "#         action_idx = agent.act(state)\n",
        "#         orders = np.unravel_index(action_idx, (11, 11, 11))\n",
        "#         orders = [o * 10 for o in orders]\n",
        "#         next_state, reward, done, _ = env.step(orders)\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "\n",
        "#     episode_rewards.append(total_reward)\n",
        "\n",
        "# average_reward = np.mean(episode_rewards)\n",
        "# print(f\"Average Reward over {episodes} episodes: {average_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9zDn6knNENm"
      },
      "id": "H9zDn6knNENm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7768a88a",
        "outputId": "a6334fab-bd9a-486e-e184-c3f83b25ec9c"
      },
      "source": [
        "# Assuming 'agent' is your trained DQNAgent instance\n",
        "model_path = 'dueling_dqn_policy_net.pth'\n",
        "torch.save(agent.qnetwork.state_dict(), model_path)\n",
        "\n",
        "print(f\"Policy network saved to {model_path}\")"
      ],
      "id": "7768a88a",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy network saved to dueling_dqn_policy_net.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------\n"
      ],
      "metadata": {
        "id": "i9mSEeXuOg1u"
      },
      "id": "i9mSEeXuOg1u"
    },
    {
      "cell_type": "code",
      "source": [
        "# rl agent\n",
        "\n",
        "#rl_agent.py\n",
        "#import gym\n",
        "import subprocess\n",
        "import sys\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\"])\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Get the current directory of submission.py\n",
        "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "# Path to the model file - Changed to the saved policy network filename\n",
        "model_path = os.path.join(CURRENT_DIR, \"dueling_dqn_policy_net.pth\")\n",
        "\n",
        "# # Load model content\n",
        "# with open(model_path, 'r') as f:\n",
        "#     model_data = f.read()\n",
        "\n",
        "# # Optionally, process the model data\n",
        "# print(\"Loaded model data:\", model_data)\n",
        "\n",
        "# Updated to DuelingQNetwork to match the trained model\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=128):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.value_stream = nn.Linear(hidden_size, 1)\n",
        "        self.advantage_stream = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values\n",
        "\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def flatten_state(self, state):\n",
        "\n",
        "      # idea is to convert the environment state into a flat numpy array that the neural network can process\n",
        "\n",
        "      if isinstance(state, dict):\n",
        "          return np.concatenate([np.array(v, dtype=np.float32) for v in state.values()])\n",
        "      return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def run_policy(self,state):\n",
        "        ''' policy execution function '''\n",
        "        state = self.flatten_state(state)\n",
        "        STATE_SIZE = len(state)\n",
        "        ACTION_SIZE = 11 ** 3  # 3 products, 11 discrete actions each\n",
        "\n",
        "        # Load the DuelingQNetwork model\n",
        "        policy_net = DuelingQNetwork(STATE_SIZE, ACTION_SIZE)\n",
        "        policy_net.load_state_dict(torch.load(model_path,map_location='cpu'))\n",
        "\n",
        "        # loading a pretrained DQN model\n",
        "\n",
        "\n",
        "        policy_net.eval()\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = policy_net(state_tensor)\n",
        "        action_idx = torch.argmax(q_values).item()\n",
        "\n",
        "        # Convert flat index → orders for 3 products\n",
        "        orders = np.unravel_index(action_idx, (11, 11, 11))\n",
        "        return [o * 10 for o in orders]  # since action space is {0,10,...,100}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ukd3MPQHOjve",
        "outputId": "37ac4a93-4ebb-41ef-b60d-f35280e83eed"
      },
      "id": "ukd3MPQHOjve",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-511897301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Get the current directory of submission.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mCURRENT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Path to the model file - Changed to the saved policy network filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
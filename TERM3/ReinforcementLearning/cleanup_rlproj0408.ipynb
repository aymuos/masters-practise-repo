{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3900a7e",
   "metadata": {},
   "source": [
    "Starting point \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70614e",
   "metadata": {},
   "source": [
    "Cleaned | D3QN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d2227b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple, Optional\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInventoryEnv\u001b[39;00m:\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    Inventory management environment for 3 products with volume constraints, lead times, and stochastic or deterministic demand.\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m        demand_lambda (List[float]): Poisson mean for training demand generation.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "class InventoryEnv:\n",
    "    \"\"\"\n",
    "    Inventory management environment for 3 products with volume constraints, lead times, and stochastic or deterministic demand.\n",
    "\n",
    "    This environment simulates:\n",
    "    - Warehouse inventory evolution with lead-time-based ordering.\n",
    "    - Daily customer demand and fulfillment.\n",
    "    - Cost computation due to holding, ordering, and stockouts.\n",
    "\n",
    "    Attributes:\n",
    "        volume_capacity (float): Max warehouse volume capacity.\n",
    "        initial_inventory (List[int]): Initial stock for each product.\n",
    "        product_volumes (List[float]): Volume per unit of each product.\n",
    "        holding_cost_per_volume (float): Cost per unit volume per day for storing inventory.\n",
    "        stockout_costs (List[float]): Penalty per unit of unfulfilled demand for each product.\n",
    "        ordering_costs (List[float]): Fixed cost per order placed for each product.\n",
    "        discard_costs (List[float]): Cost per unit discarded due to over-capacity.\n",
    "        lead_times (List[int]): Days before an order arrives for each product.\n",
    "        simulation_days (int): Episode length in days.\n",
    "        demand_sequences (Optional[List[List[int]]]): Predefined demand for evaluation.\n",
    "        demand_lambda (List[float]): Poisson mean for training demand generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        volume_capacity: float = 1000.0,\n",
    "        initial_inventory: List[int] = [100, 100, 100],\n",
    "        product_volumes: List[float] = [2.0, 3.0, 1.5],\n",
    "        holding_cost_per_volume: float = 5.0,  # Updated holding cost\n",
    "        stockout_costs: List[float] = [400.0, 500.0, 300.0],\n",
    "        ordering_costs: List[float] = [80.0, 200.0, 120.0],\n",
    "        discard_costs: List[float] = [200.0, 250.0, 150.0],  # New discard penalties\n",
    "        lead_times: List[int] = [3, 2, 1],\n",
    "        simulation_days: int = 50,\n",
    "        demand_sequences: Optional[List[List[int]]] = None,\n",
    "        demand_lambda: List[float] = [30, 25, 35],\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        self.volume_capacity = volume_capacity\n",
    "        self.initial_inventory = initial_inventory[:]\n",
    "        self.product_volumes = product_volumes\n",
    "        self.holding_cost_per_volume = holding_cost_per_volume\n",
    "        self.stockout_costs = stockout_costs\n",
    "        self.ordering_costs = ordering_costs\n",
    "        self.discard_costs = discard_costs\n",
    "        self.lead_times = lead_times\n",
    "        self.simulation_days = simulation_days\n",
    "        self.demand_sequences = demand_sequences\n",
    "        self.demand_lambda = demand_lambda\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Reset environment to initial state for a new episode.\n",
    "        Returns the initial observation state.\n",
    "        \"\"\"\n",
    "        self.day = 0\n",
    "        self.inventory = self.initial_inventory[:] # Resets current inventory to initial inventory\n",
    "        self.pending_orders = [[] for _ in range(len(self.initial_inventory))]  # list of orders to be delivered (day_due, quantity)\n",
    "        return self._get_state() # Returns initial state of the environment\n",
    "\n",
    "    def step(self, action: List[int]) -> Tuple[List[int], float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Executes one simulation step.\n",
    "\n",
    "        Args:\n",
    "            action (List[int]): List of order quantities for each product. Each value must be in {0, 10, ..., 100}.\n",
    "\n",
    "        Returns:\n",
    "            state (List[int]): Updated state after taking the action.\n",
    "            reward (float): Scaled negative cost for the step.\n",
    "            done (bool): True if the episode is over.\n",
    "            info (dict): Additional information (cost breakdown, demand, fulfillment).\n",
    "        \"\"\"\n",
    "        assert all(a in range(0, 101, 10) for a in action), \"Actions must be in {0, 10, ..., 100}\" # Invalid actions are rejected\n",
    "\n",
    "        # 1. Receive due orders and add them to current inventory\n",
    "        for i in range(3):\n",
    "            arrivals = [qty for due, qty in self.pending_orders[i] if due == self.day]\n",
    "            self.inventory[i] += sum(arrivals)\n",
    "            self.pending_orders[i] = [(due, qty) for due, qty in self.pending_orders[i] if due > self.day]\n",
    "\n",
    "        # 2. Place new orders and add them to pending orders\n",
    "        order_cost = 0\n",
    "        for i in range(3):\n",
    "            if action[i] > 0:\n",
    "                order_cost += self.ordering_costs[i]\n",
    "                self.pending_orders[i].append((self.day + self.lead_times[i], action[i]))\n",
    "\n",
    "        # 3. Generate demand if not provided\n",
    "        if self.demand_sequences:\n",
    "            demand = self.demand_sequences[self.day]\n",
    "        else:\n",
    "            demand = [self.random_state.poisson(lam) for lam in self.demand_lambda]\n",
    "\n",
    "        # 4. Enforce volume capacity and compute discards\n",
    "        total_volume = sum(self.inventory[i] * self.product_volumes[i] for i in range(3))\n",
    "        discarded = [0, 0, 0]\n",
    "        if total_volume > self.volume_capacity:\n",
    "            overflow = total_volume - self.volume_capacity\n",
    "            # discard from highest-volume items first\n",
    "            for i in sorted(range(3), key=lambda j: self.product_volumes[j], reverse=True):\n",
    "                max_remove = int(overflow // self.product_volumes[i])\n",
    "                remove_qty = min(max_remove, self.inventory[i])\n",
    "                discarded[i] = remove_qty\n",
    "                self.inventory[i] -= remove_qty\n",
    "                overflow -= remove_qty * self.product_volumes[i]\n",
    "                if overflow <= 0:\n",
    "                    break\n",
    "\n",
    "        # 5. Fulfill demand and compute stockouts\n",
    "        fulfilled = [min(self.inventory[i], demand[i]) for i in range(3)]\n",
    "        unfulfilled = [demand[i] - fulfilled[i] for i in range(3)]\n",
    "        self.inventory = [self.inventory[i] - fulfilled[i] for i in range(3)]\n",
    "\n",
    "        # 6. Compute costs and reward\n",
    "        holding_cost = sum(self.inventory[i] * self.product_volumes[i] * self.holding_cost_per_volume for i in range(3))\n",
    "        stockout_cost = sum(unfulfilled[i] * self.stockout_costs[i] for i in range(3))\n",
    "        discard_cost = sum(discarded[i] * self.discard_costs[i] for i in range(3))\n",
    "        total_cost = holding_cost + stockout_cost + order_cost + discard_cost\n",
    "        reward = - total_cost / 100.0  # scaled for stability\n",
    "\n",
    "        # 7. Update state\n",
    "        self.day += 1\n",
    "        done = self.day >= self.simulation_days # True if episode ends\n",
    "        info = {\n",
    "            \"day\": self.day,\n",
    "            \"inventory\": self.inventory[:],\n",
    "            \"fulfilled\": fulfilled,\n",
    "            \"unfulfilled\": unfulfilled,\n",
    "            \"order_cost\": order_cost,\n",
    "            \"holding_cost\": holding_cost,\n",
    "            \"stockout_cost\": stockout_cost,\n",
    "            \"discard_cost\": discard_cost,\n",
    "            \"total_cost\": total_cost\n",
    "        }\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "    def _get_state(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Constructs the state vector including inventory levels and outstanding orders.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: State representation with 7 variables\n",
    "        \"\"\"\n",
    "        outstanding_orders = [sum(qty for _, qty in self.pending_orders[i]) for i in range(3)]\n",
    "        return self.inventory + outstanding_orders + [self.day]\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.value_stream = nn.Linear(hidden_size, 1)\n",
    "        self.advantage_stream = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, lr=1e-3,\n",
    "                 batch_size=64, buffer_size=50000, epsilon_start=1.0,\n",
    "                 epsilon_end=0.05, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.qnetwork = DuelingQNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network = DuelingQNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=lr)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.qnetwork(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        q_values = self.qnetwork(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Double DQN update\n",
    "        next_actions = self.qnetwork(next_states).max(1)[1].unsqueeze(1)\n",
    "        next_q_values = self.target_network(next_states).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "        targets = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.qnetwork.state_dict())\n",
    "\n",
    "\n",
    "env = InventoryEnv()\n",
    "state = env.reset()\n",
    "state_size = len(state)\n",
    "action_size = 11 ** 3  # since 3 products, each has 11 discrete order options\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "episodes = 500  # Reduced episodes for faster execution\n",
    "target_update_freq = 10\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action_idx = agent.act(state)\n",
    "\n",
    "        # Convert flat index to 3 product orders\n",
    "        orders = np.unravel_index(action_idx, (11, 11, 11))\n",
    "        orders = [o * 10 for o in orders]\n",
    "\n",
    "        next_state, reward, done, _ = env.step(orders)\n",
    "        agent.remember(state, action_idx, reward, next_state, done)\n",
    "        agent.replay()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if e % target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Episode {e}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dedfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the average reward per episode\n",
    "episode_rewards = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action_idx = agent.act(state)\n",
    "        orders = np.unravel_index(action_idx, (11, 11, 11))\n",
    "        orders = [o * 10 for o in orders]\n",
    "        next_state, reward, done, _ = env.step(orders)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "average_reward = np.mean(episode_rewards)\n",
    "print(f\"Average Reward over {episodes} episodes: {average_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
